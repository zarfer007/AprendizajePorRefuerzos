{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL Lab 2 part 2 EM FZ simple model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zarfer007/AprendizajePorRefuerzos/blob/master/testingGit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SytTR8K1oII-"
      },
      "source": [
        "# RL 2 LAB I.1\n",
        "\n",
        "  - Morbidoni, Emilio\n",
        "  - Zarzosa, Fernando\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD-FSSuKoIJF"
      },
      "source": [
        "# Interfaz básica stable-baselines\n",
        "\n",
        "### Instalación de Stable-baselines\n",
        "\n",
        "  - Desde Windows, además, instalar: \n",
        "* Microsoft Visual C++ desde https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
        "* PyType, mediante `conda install -c conda-forge pytype`\n",
        "\n",
        "  - Desde Linux o Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLaFYpurBU7P"
      },
      "source": [
        "#!pip install sphinxcontrib-spelling # if problems with sphinx\n",
        "#!pip install datascience"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ttqOLol_oIJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc762cd-cfe6-4cd3-9065-1c899e7d2111"
      },
      "source": [
        "#@title Instalación (no modificar)\n",
        "!pip install stable-baselines3[extra,tests,docs]>=0.11.0a4 && pip install sb3-contrib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinxcontrib-spelling 7.1.0 has requirement Sphinx>=3.0.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytest-cov 2.11.1 has requirement pytest>=4.6, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytest-forked 1.3.0 has requirement pytest>=3.10, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytest-xdist 2.2.1 has requirement pytest>=6.0.0, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "Collecting sb3-contrib\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/ce/e9809debd089278cc0b38f6f1fd21cf3442ebebb7e19c6335f79ee3bb9fe/sb3_contrib-1.0-py3-none-any.whl\n",
            "Requirement already satisfied: stable-baselines3[docs,tests]>=1.0 in /usr/local/lib/python3.7/dist-packages (from sb3-contrib) (1.0)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.1.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.8.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.2.2)\n",
            "Requirement already satisfied: sphinx; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.8.5)\n",
            "Collecting sphinxcontrib.spelling; extra == \"docs\"\n",
            "  Using cached https://files.pythonhosted.org/packages/f6/62/796d8ae02732c162f8d53406f520c9f3c886a9ab24de4ef6995404c2b1d8/sphinxcontrib_spelling-7.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: sphinx-autobuild; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2021.3.14)\n",
            "Requirement already satisfied: sphinx-rtd-theme; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.5.1)\n",
            "Requirement already satisfied: sphinx-autodoc-typehints; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.11.1)\n",
            "Requirement already satisfied: isort>=5.0; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (5.7.0)\n",
            "Requirement already satisfied: flake8>=3.8; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.9.0)\n",
            "Requirement already satisfied: pytest-cov; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.11.1)\n",
            "Requirement already satisfied: pytype; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2021.3.10)\n",
            "Requirement already satisfied: pytest-xdist; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.2.1)\n",
            "Requirement already satisfied: pytest; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.6.4)\n",
            "Requirement already satisfied: pytest-env; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.6.2)\n",
            "Requirement already satisfied: black; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (20.8b1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.7.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.4.7)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.6.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.23.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.9.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (20.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.15.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.16)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.7.2)\n",
            "Requirement already satisfied: PyEnchant>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.2.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.4.4)\n",
            "Requirement already satisfied: livereload in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.6.3)\n",
            "Requirement already satisfied: pyflakes<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.3.0)\n",
            "Requirement already satisfied: pycodestyle<2.8.0,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.7.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.6.1)\n",
            "Requirement already satisfied: coverage>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pytest-cov; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (5.5)\n",
            "Requirement already satisfied: pyyaml>=3.11 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.13)\n",
            "Requirement already satisfied: importlab>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.6.1)\n",
            "Requirement already satisfied: typed-ast in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.4.2)\n",
            "Requirement already satisfied: ninja>=1.10.0.post2 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.10.0.post2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (20.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.10.2)\n",
            "Requirement already satisfied: execnet>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytest-xdist; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.8.0)\n",
            "Requirement already satisfied: pytest-forked in /usr/local/lib/python3.7/dist-packages (from pytest-xdist; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (8.7.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.10.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.4.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.4.3)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2021.3.17)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (7.1.2)\n",
            "Requirement already satisfied: pathspec<1,>=0.6 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.24.3)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.4.1)\n",
            "Requirement already satisfied: tornado; python_version > \"2.7\" in /usr/local/lib/python3.7/dist-packages (from livereload->sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (5.1.1)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.7/dist-packages (from importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.5)\n",
            "Requirement already satisfied: apipkg>=1.4 in /usr/local/lib/python3.7/dist-packages (from execnet>=1.1->pytest-xdist; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2->importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (4.4.2)\n",
            "\u001b[31mERROR: sphinxcontrib-spelling 7.1.0 has requirement Sphinx>=3.0.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sb3-contrib, sphinxcontrib.spelling\n",
            "Successfully installed sb3-contrib-1.0 sphinxcontrib.spelling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUUkScRxoIJQ"
      },
      "source": [
        "# RL-baselines3-zoo\n",
        "\n",
        "  - Instalación de RLBaselinesZoo\n",
        "\n",
        "Desde Google Colab\n",
        "\n",
        "  - ---recursive allows to train agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM_pM0mIoIJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f144d9c0-ca2a-4436-d57e-44702d282501"
      },
      "source": [
        "#@title Instalación de RLBaselinesZoo (no modificar)\n",
        "\n",
        "#if IN_COLAB:\n",
        "!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "!cd rl-baselines3-zoo/\n",
        "!apt-get install swig cmake ffmpeg\n",
        "!pip install -r /content/rl-baselines3-zoo/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines3-zoo'...\n",
            "remote: Enumerating objects: 275, done.\u001b[K\n",
            "remote: Counting objects: 100% (275/275), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 2161 (delta 18), reused 257 (delta 12), pack-reused 1886\u001b[K\n",
            "Receiving objects: 100% (2161/2161), 1.00 MiB | 14.09 MiB/s, done.\n",
            "Resolving deltas: 100% (1363/1363), done.\n",
            "Submodule 'rl-trained-agents' (https://github.com/DLR-RM/rl-trained-agents) registered for path 'rl-trained-agents'\n",
            "Cloning into '/content/rl-baselines3-zoo/rl-trained-agents'...\n",
            "remote: Enumerating objects: 1, done.        \n",
            "remote: Counting objects: 100% (1/1), done.        \n",
            "remote: Total 1416 (delta 0), reused 0 (delta 0), pack-reused 1415\n",
            "Receiving objects: 100% (1416/1416), 977.06 MiB | 32.34 MiB/s, done.\n",
            "Resolving deltas: 100% (220/220), done.\n",
            "Submodule path 'rl-trained-agents': checked out 'd81fcd61cef4599564c859297ea68bacf677db6b'\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (1,300 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: stable-baselines3[docs,extra,tests]>=1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.0)\n",
            "Collecting box2d-py==2.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 4.6MB/s \n",
            "\u001b[?25hCollecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/9c/7b76db10cdaa69c840b211fe21ce6f31fb80b611b198fe18a64ddb8f374e/pybullet-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (88.7MB)\n",
            "\u001b[K     |████████████████████████████████| 88.7MB 76kB/s \n",
            "\u001b[?25hCollecting gym-minigrid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/d2/e2b0fc791c9d22c70ea48df762133d7ad930ea09e1bfa95a24a1c86ddf18/gym_minigrid-1.0.2-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hCollecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.0MB/s \n",
            "\u001b[?25hCollecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/67/ed0af7c66bcfb9a9a56fbafcca7d848452d78433208b59b003741879cc69/optuna-2.6.0-py3-none-any.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 40.4MB/s \n",
            "\u001b[?25hCollecting pytablewriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r /content/rl-baselines3-zoo/requirements.txt (line 8)) (0.11.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sb3-contrib>=1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/rl-baselines3-zoo/requirements.txt (line 10)) (1.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.5.1)\n",
            "Collecting sphinxcontrib.spelling; extra == \"docs\"\n",
            "  Using cached https://files.pythonhosted.org/packages/f6/62/796d8ae02732c162f8d53406f520c9f3c886a9ab24de4ef6995404c2b1d8/sphinxcontrib_spelling-7.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: sphinx; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.8.5)\n",
            "Requirement already satisfied: sphinx-autodoc-typehints; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: sphinx-autobuild; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2021.3.14)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.2.6)\n",
            "Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (5.4.8)\n",
            "Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (4.1.2.30)\n",
            "Requirement already satisfied: tensorboard>=2.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (7.0.0)\n",
            "Requirement already satisfied: flake8>=3.8; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.9.0)\n",
            "Requirement already satisfied: pytest; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.6.4)\n",
            "Requirement already satisfied: pytest-xdist; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: pytest-cov; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.11.1)\n",
            "Requirement already satisfied: isort>=5.0; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (5.7.0)\n",
            "Requirement already satisfied: black; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (20.8b1)\n",
            "Requirement already satisfied: pytype; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2021.3.10)\n",
            "Requirement already satisfied: pytest-env; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.6.2)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r /content/rl-baselines3-zoo/requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r /content/rl-baselines3-zoo/requirements.txt (line 5)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r /content/rl-baselines3-zoo/requirements.txt (line 5)) (1.4.1)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading https://files.pythonhosted.org/packages/01/1f/43b01223a0366171f474320c6e966c39a11587287f098a5f09809b45e05f/cmaes-0.8.2-py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (20.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (4.41.1)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/d6/7d9acb68a77acd140be7fececb7f2701b2a29d2da9c54184cb8f93509590/cliff-3.7.0-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (1.3.23)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/39/0230290df0519d528d8d0ffdfd900150ed24e0076d13b1f19e279444aab1/colorlog-4.7.2-py2.py3-none-any.whl\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/29/ed5c134aba874053859ba3e8d4705b4a5c1b66156deabc26cbe643e83f2e/alembic-1.5.7-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 36.3MB/s \n",
            "\u001b[?25hCollecting msgfy<1,>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl\n",
            "Collecting pathvalidate<3,>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/97/d1/096e837c64c21e1ac046fa6333f06979e1139d0e7b2c46da63d9484956e6/pathvalidate-2.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r /content/rl-baselines3-zoo/requirements.txt (line 7)) (54.0.0)\n",
            "Collecting typepy[datetime]<2,>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/bf/2d4cb8e7896bb20f213358dd66fcbab9f61e140b0046a9651df4d4f7dac8/typepy-1.1.4-py3-none-any.whl\n",
            "Collecting mbstrdecoder<2,>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/f6/0e6bb50c3c6380a4982c87d80e70b2f6e366523a57a0c58594aea472206d/mbstrdecoder-1.0.1-py3-none-any.whl\n",
            "Collecting tcolorpy<1,>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/02/51/bbb0cc7f30771c285c354634bf83653a2871d58c6923bd29bfddeb9c9cb1/tcolorpy-0.0.8-py3-none-any.whl\n",
            "Collecting DataProperty<2,>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/2d/e5413965af992f4e489b6f5eebf52db9c17953c772962d1223d434b05cef/DataProperty-0.50.0-py3-none-any.whl\n",
            "Collecting tabledata<2,>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/df/b2/264d9707502f0259a3eb82ec48064df98b1735d5a5f315b6a1d7105263f4/tabledata-1.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.7.2)\n",
            "Requirement already satisfied: PyEnchant>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.9.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.7.12)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.2.4)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.16)\n",
            "Requirement already satisfied: livereload in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: pyflakes<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: pycodestyle<2.8.0,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.7.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (8.7.0)\n",
            "Requirement already satisfied: execnet>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytest-xdist; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: pytest-forked in /usr/local/lib/python3.7/dist-packages (from pytest-xdist; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: coverage>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pytest-cov; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (5.5)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: typed-ast>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: pathspec<1,>=0.6 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2021.3.17)\n",
            "Requirement already satisfied: ninja>=1.10.0.post2 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.10.0.post2)\n",
            "Requirement already satisfied: importlab>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.6.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/8b/15061b32332bb35ea2a2f6263d0f616779d576e82739ec8e7fcf3c94abf5/cmd2-1.5.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 37.1MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (2.1.0)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.5MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: chardet<5,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->-r /content/rl-baselines3-zoo/requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.1.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: tornado; python_version > \"2.7\" in /usr/local/lib/python3.7/dist-packages (from livereload->sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (5.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: apipkg>=1.4 in /usr/local/lib/python3.7/dist-packages (from execnet>=1.1->pytest-xdist; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.5)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.7/dist-packages (from importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2->importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (4.4.2)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11107 sha256=07b3148205b83d6453d0d568cd1a8dd57abccc2cd050f99b58e81d1d67413778\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n",
            "Successfully built pyperclip\n",
            "\u001b[31mERROR: sphinxcontrib-spelling 7.1.0 has requirement Sphinx>=3.0.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: box2d-py, pybullet, gym-minigrid, pyyaml, pyaml, scikit-optimize, cmaes, pbr, stevedore, pyperclip, cmd2, cliff, colorlog, Mako, python-editor, alembic, optuna, msgfy, pathvalidate, mbstrdecoder, typepy, tcolorpy, DataProperty, tabledata, pytablewriter, sphinxcontrib.spelling\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed DataProperty-0.50.0 Mako-1.1.4 alembic-1.5.7 box2d-py-2.3.8 cliff-3.7.0 cmaes-0.8.2 cmd2-1.5.0 colorlog-4.7.2 gym-minigrid-1.0.2 mbstrdecoder-1.0.1 msgfy-0.1.0 optuna-2.6.0 pathvalidate-2.3.2 pbr-5.5.1 pyaml-20.4.0 pybullet-3.1.0 pyperclip-1.8.2 pytablewriter-0.58.0 python-editor-1.0.4 pyyaml-5.4.1 scikit-optimize-0.8.1 sphinxcontrib.spelling stevedore-3.3.0 tabledata-1.1.3 tcolorpy-0.0.8 typepy-1.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzBEoyPzoIJR"
      },
      "source": [
        "Desde Linux, ejecutando\n",
        "\n",
        "    git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "    cd rl-baselines3-zoo/\n",
        "    sudo apt-get install swig cmake ffmpeg\n",
        "    pip install -r requirements.txt\n",
        "\n",
        "Instalación desde Nabucodonosor\n",
        "\n",
        "    wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh\n",
        "    chmod 755 Anaconda3-2020.11-Linux-x86_64.sh\n",
        "    ./Anaconda3-2020.11-Linux-x86_64.sh\n",
        "    conda create --name rl\n",
        "    conda activate rl\n",
        "    conda config --add channels conda-forge\n",
        "    conda install jupyter atari_py swig\n",
        "    pip install stable-baselines3[extra,tests,docs]>=0.11.0a4\n",
        "    pip install sb3-contrib "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Morjd1dFoIJH"
      },
      "source": [
        "## Ejecución de un algoritmo de RL\n",
        "  - Importaciones/inicializaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG7i44kqoIJH"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "#from gym.envs.registration import register\n",
        "\n",
        "from stable_baselines3 import DQN, PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFLZieLNoIJO"
      },
      "source": [
        "Para usar un entorno compatible por esta librería, el mismo tiene que heredar de *gym.Env*. Vemos un ejemplo (crédito: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6HeKF2wRCcD"
      },
      "source": [
        "# Lab 2\n",
        "\n",
        "1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros. \n",
        "\n",
        "    Algunas ideas:\n",
        "\n",
        "    * Transformar GoLeftEnv en una grilla 2D, añadir paredes / trampas / agua.\n",
        "    * Crear un entorno que juegue a algún juego como el ta-te-ti.\n",
        "    * Crea un entorno totalmente nuevo que sea de tu interés!\n",
        "\n",
        "2. Entrena agentes en entornos más complejos con stable-baselines/rl-baselines-zoo. Tener en cuenta:\n",
        "\n",
        "    * Google Colab tiene una limitante en cuanto a cantidad de recursos de CPU/GPU (incluido un \"rendimiento decreciente silencioso\"), lo cuál reduce la capacidad de entrenar distintos entornos.\n",
        "    * Si el entorno no está implementado en stable-baselines, debe hacerse un wrapper a mano, lo que puede ser sencillo o puede llevar algo más de trabajo, teniendo que tocar código subyacente de la librería. \n",
        "\n",
        "\\* pueden ser usando stable-baselines/rl-baselines-zoo o bien utilizando algún otro algoritmo (incluso tabular)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QuB3twMgjjg"
      },
      "source": [
        "# 2.2 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6TCyu_UoIJR"
      },
      "source": [
        "## Ejecución\n",
        "\n",
        "Los agentes pueden ser llamados desde la consola mediante comandos como\n",
        "\n",
        "`python train.py --algo algo_name --env env_id`\n",
        "\n",
        "Los cuales pueden ser llamados usando"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSfIitK7oIJR"
      },
      "source": [
        "\n",
        "También es posible grabar un video! Ver https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#record-a-video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke2LldyZmi36"
      },
      "source": [
        "# MountainCarContinuous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yk12ZnNmkKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d786df-8d07-4078-c755-59ebf284e3cb"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg freeglut3-dev xvfb\n",
        "!pip install stable-baselines[mpi] --upgrade\n",
        "!pip install pybullet\n",
        "!pip install box2d box2d-kengz pyyaml pytablewriter optuna scikit-optimize\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connected to cloud.r-pro\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n",
            "Collecting stable-baselines[mpi]\n",
            "  Using cached https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->stable-baselines[mpi]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.1\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.7/dist-packages (2.3.10)\n",
            "Requirement already satisfied: box2d-kengz in /usr/local/lib/python3.7/dist-packages (2.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (5.4.1)\n",
            "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.7/dist-packages (0.58.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (1.0.1)\n",
            "Requirement already satisfied: typepy[datetime]<2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (1.1.4)\n",
            "Requirement already satisfied: tabledata<2,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (1.1.3)\n",
            "Requirement already satisfied: pathvalidate<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (2.3.2)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (54.0.0)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (0.0.8)\n",
            "Requirement already satisfied: msgfy<1,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (0.1.0)\n",
            "Requirement already satisfied: DataProperty<2,>=0.50.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (0.50.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (4.7.2)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (20.9)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.3.23)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.5.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2018.9; extra == \"datetime\" in /usr/local/lib/python3.7/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2018.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\" in /usr/local/lib/python3.7/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (1.5.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.1.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.5.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.4)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\"->typepy[datetime]<2,>=1.1.1->pytablewriter) (1.15.0)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2>=1.0.0->cliff->optuna) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zm4i3bd8ZdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2805d6eb-797f-4a36-d2b6-e902a2f53b3b"
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines-zoo'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 1843 (delta 0), reused 1 (delta 0), pack-reused 1840\u001b[K\n",
            "Receiving objects: 100% (1843/1843), 375.67 MiB | 33.32 MiB/s, done.\n",
            "Resolving deltas: 100% (1091/1091), done.\n",
            "Checking out files: 100% (333/333), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi1LIM6FQ-aJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897baa60-009f-40b4-dc42-25ad0d6b92c8"
      },
      "source": [
        "cd rl-baselines-zoo/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rl-baselines-zoo/rl-baselines-zoo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKvv66bsI0--"
      },
      "source": [
        "### Source:\n",
        "https://gym.openai.com/envs/MountainCarContinuous-v0/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk2E1RoOJD2U",
        "outputId": "0950babf-48d9-4c70-99c8-33b6f3fc3e8b"
      },
      "source": [
        "tensorflow_version # tensorflow version for gym is 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unknown TensorFlow version: # tensorflow version for gym is 1.x\n",
            "Currently selected TF version: 1.x\n",
            "Available versions:\n",
            " * 1.x\n",
            " * 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feS7q6Kr8emj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07da02e2-4767-4dc2-bbeb-d8ba42d398b0"
      },
      "source": [
        "# Baseline Model\n",
        "!python train.py --algo ppo2 --env MountainCarContinuous-v0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MountainCarContinuous-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.94),\n",
            "             ('n_envs', 16),\n",
            "             ('n_steps', 256),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 8),\n",
            "             ('noptepochs', 4),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MountainCarContinuous-v0_1\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00063773367 |\n",
            "| clipfrac           | 0.0014648438  |\n",
            "| explained_variance | -0.00818      |\n",
            "| fps                | 4562          |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 1.4159632     |\n",
            "| policy_loss        | -0.0009888856 |\n",
            "| serial_timesteps   | 256           |\n",
            "| time_elapsed       | 2.12e-05      |\n",
            "| total_timesteps    | 4096          |\n",
            "| value_loss         | 0.63798976    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00024230995 |\n",
            "| clipfrac           | 0.00012207031 |\n",
            "| explained_variance | -0.0423       |\n",
            "| fps                | 9277          |\n",
            "| n_updates          | 2             |\n",
            "| policy_entropy     | 1.4080168     |\n",
            "| policy_loss        | -0.0016784458 |\n",
            "| serial_timesteps   | 512           |\n",
            "| time_elapsed       | 0.898         |\n",
            "| total_timesteps    | 8192          |\n",
            "| value_loss         | 0.10600683    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 999.00 +/- 0.00\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0003233038 |\n",
            "| clipfrac           | 0.0006713867 |\n",
            "| ep_len_mean        | 724          |\n",
            "| ep_reward_mean     | 63.2         |\n",
            "| explained_variance | 0.00723      |\n",
            "| fps                | 755          |\n",
            "| n_updates          | 3            |\n",
            "| policy_entropy     | 1.3991096    |\n",
            "| policy_loss        | -0.001596961 |\n",
            "| serial_timesteps   | 768          |\n",
            "| time_elapsed       | 1.34         |\n",
            "| total_timesteps    | 12288        |\n",
            "| value_loss         | 0.18878943   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00011489005 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 982           |\n",
            "| ep_reward_mean     | -44.1         |\n",
            "| explained_variance | -0.0196       |\n",
            "| fps                | 8960          |\n",
            "| n_updates          | 4             |\n",
            "| policy_entropy     | 1.3932282     |\n",
            "| policy_loss        | -0.0005603612 |\n",
            "| serial_timesteps   | 1024          |\n",
            "| time_elapsed       | 6.77          |\n",
            "| total_timesteps    | 16384         |\n",
            "| value_loss         | 0.05970907    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-0.14 +/- 0.10\n",
            "Episode length: 999.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00014312149 |\n",
            "| clipfrac           | 6.1035156e-05 |\n",
            "| ep_len_mean        | 982           |\n",
            "| ep_reward_mean     | -44.1         |\n",
            "| explained_variance | 0.259         |\n",
            "| fps                | 766           |\n",
            "| n_updates          | 5             |\n",
            "| policy_entropy     | 1.3842179     |\n",
            "| policy_loss        | -0.0017475053 |\n",
            "| serial_timesteps   | 1280          |\n",
            "| time_elapsed       | 7.22          |\n",
            "| total_timesteps    | 20480         |\n",
            "| value_loss         | 0.041302748   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 7.780737e-05  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 982           |\n",
            "| ep_reward_mean     | -44.1         |\n",
            "| explained_variance | 0.286         |\n",
            "| fps                | 9208          |\n",
            "| n_updates          | 6             |\n",
            "| policy_entropy     | 1.3735335     |\n",
            "| policy_loss        | -0.0014714969 |\n",
            "| serial_timesteps   | 1536          |\n",
            "| time_elapsed       | 12.6          |\n",
            "| total_timesteps    | 24576         |\n",
            "| value_loss         | 0.038392603   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00038899435  |\n",
            "| clipfrac           | 0.00012207031  |\n",
            "| ep_len_mean        | 952            |\n",
            "| ep_reward_mean     | -32.9          |\n",
            "| explained_variance | 0.082          |\n",
            "| fps                | 9207           |\n",
            "| n_updates          | 7              |\n",
            "| policy_entropy     | 1.365754       |\n",
            "| policy_loss        | -0.00055942487 |\n",
            "| serial_timesteps   | 1792           |\n",
            "| time_elapsed       | 13             |\n",
            "| total_timesteps    | 28672          |\n",
            "| value_loss         | 0.23562996     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-0.49 +/- 0.28\n",
            "Episode length: 999.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00080637005 |\n",
            "| clipfrac           | 0.0034179688  |\n",
            "| ep_len_mean        | 956           |\n",
            "| ep_reward_mean     | -29.8         |\n",
            "| explained_variance | 0.178         |\n",
            "| fps                | 771           |\n",
            "| n_updates          | 8             |\n",
            "| policy_entropy     | 1.3631157     |\n",
            "| policy_loss        | -0.0013422171 |\n",
            "| serial_timesteps   | 2048          |\n",
            "| time_elapsed       | 13.5          |\n",
            "| total_timesteps    | 32768         |\n",
            "| value_loss         | 0.2717248     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00014840695  |\n",
            "| clipfrac           | 6.1035156e-05  |\n",
            "| ep_len_mean        | 956            |\n",
            "| ep_reward_mean     | -29.8          |\n",
            "| explained_variance | 0.444          |\n",
            "| fps                | 9167           |\n",
            "| n_updates          | 9              |\n",
            "| policy_entropy     | 1.3589132      |\n",
            "| policy_loss        | -0.00080389733 |\n",
            "| serial_timesteps   | 2304           |\n",
            "| time_elapsed       | 18.8           |\n",
            "| total_timesteps    | 36864          |\n",
            "| value_loss         | 0.0070542847   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=98.73 +/- 0.15\n",
            "Episode length: 562.00 +/- 125.55\n",
            "New best mean reward!\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0003834102   |\n",
            "| clipfrac           | 0.00030517578  |\n",
            "| ep_len_mean        | 906            |\n",
            "| ep_reward_mean     | -14.3          |\n",
            "| explained_variance | 0.202          |\n",
            "| fps                | 1278           |\n",
            "| n_updates          | 10             |\n",
            "| policy_entropy     | 1.3530284      |\n",
            "| policy_loss        | -0.00020252068 |\n",
            "| serial_timesteps   | 2560           |\n",
            "| time_elapsed       | 19.2           |\n",
            "| total_timesteps    | 40960          |\n",
            "| value_loss         | 0.63613397     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00015615112  |\n",
            "| clipfrac           | 6.1035156e-05  |\n",
            "| ep_len_mean        | 860            |\n",
            "| ep_reward_mean     | 1.21           |\n",
            "| explained_variance | 0.262          |\n",
            "| fps                | 9194           |\n",
            "| n_updates          | 11             |\n",
            "| policy_entropy     | 1.3513309      |\n",
            "| policy_loss        | -0.00021627017 |\n",
            "| serial_timesteps   | 2816           |\n",
            "| time_elapsed       | 22.4           |\n",
            "| total_timesteps    | 45056          |\n",
            "| value_loss         | 0.8352934      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001277488   |\n",
            "| clipfrac           | 0.010864258   |\n",
            "| ep_len_mean        | 805           |\n",
            "| ep_reward_mean     | 13            |\n",
            "| explained_variance | 0.279         |\n",
            "| fps                | 9123          |\n",
            "| n_updates          | 12            |\n",
            "| policy_entropy     | 1.3500423     |\n",
            "| policy_loss        | -0.0015151295 |\n",
            "| serial_timesteps   | 3072          |\n",
            "| time_elapsed       | 22.9          |\n",
            "| total_timesteps    | 49152         |\n",
            "| value_loss         | 0.8931359     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=98.10 +/- 0.20\n",
            "Episode length: 285.80 +/- 57.12\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00026775082  |\n",
            "| clipfrac           | 0.0004272461   |\n",
            "| ep_len_mean        | 738            |\n",
            "| ep_reward_mean     | 24.6           |\n",
            "| explained_variance | 0.31           |\n",
            "| fps                | 2202           |\n",
            "| n_updates          | 13             |\n",
            "| policy_entropy     | 1.3494128      |\n",
            "| policy_loss        | -0.00037004612 |\n",
            "| serial_timesteps   | 3328           |\n",
            "| time_elapsed       | 23.3           |\n",
            "| total_timesteps    | 53248          |\n",
            "| value_loss         | 1.1325355      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00096868444 |\n",
            "| clipfrac           | 0.006958008   |\n",
            "| ep_len_mean        | 694           |\n",
            "| ep_reward_mean     | 31.8          |\n",
            "| explained_variance | 0.321         |\n",
            "| fps                | 9017          |\n",
            "| n_updates          | 14            |\n",
            "| policy_entropy     | 1.3485408     |\n",
            "| policy_loss        | -0.0009966518 |\n",
            "| serial_timesteps   | 3584          |\n",
            "| time_elapsed       | 25.2          |\n",
            "| total_timesteps    | 57344         |\n",
            "| value_loss         | 0.92920935    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=97.36 +/- 0.50\n",
            "Episode length: 358.80 +/- 88.15\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0001269721   |\n",
            "| clipfrac           | 0.00012207031  |\n",
            "| ep_len_mean        | 653            |\n",
            "| ep_reward_mean     | 37.9           |\n",
            "| explained_variance | 0.337          |\n",
            "| fps                | 1856           |\n",
            "| n_updates          | 15             |\n",
            "| policy_entropy     | 1.3468248      |\n",
            "| policy_loss        | -5.1903844e-05 |\n",
            "| serial_timesteps   | 3840           |\n",
            "| time_elapsed       | 25.6           |\n",
            "| total_timesteps    | 61440          |\n",
            "| value_loss         | 0.90399534     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.8822865e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 566           |\n",
            "| ep_reward_mean     | 51.5          |\n",
            "| explained_variance | 0.354         |\n",
            "| fps                | 8933          |\n",
            "| n_updates          | 16            |\n",
            "| policy_entropy     | 1.3460795     |\n",
            "| policy_loss        | 0.00014231703 |\n",
            "| serial_timesteps   | 4096          |\n",
            "| time_elapsed       | 27.8          |\n",
            "| total_timesteps    | 65536         |\n",
            "| value_loss         | 1.4555706     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007535221  |\n",
            "| clipfrac           | 0.004272461   |\n",
            "| ep_len_mean        | 478           |\n",
            "| ep_reward_mean     | 66            |\n",
            "| explained_variance | 0.38          |\n",
            "| fps                | 9195          |\n",
            "| n_updates          | 17            |\n",
            "| policy_entropy     | 1.3458685     |\n",
            "| policy_loss        | -0.0011037786 |\n",
            "| serial_timesteps   | 4352          |\n",
            "| time_elapsed       | 28.3          |\n",
            "| total_timesteps    | 69632         |\n",
            "| value_loss         | 1.0541351     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=96.59 +/- 0.48\n",
            "Episode length: 246.80 +/- 55.62\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016596606  |\n",
            "| clipfrac           | 0.016967773   |\n",
            "| ep_len_mean        | 372           |\n",
            "| ep_reward_mean     | 81.3          |\n",
            "| explained_variance | 0.382         |\n",
            "| fps                | 2454          |\n",
            "| n_updates          | 18            |\n",
            "| policy_entropy     | 1.3445667     |\n",
            "| policy_loss        | -0.0026447587 |\n",
            "| serial_timesteps   | 4608          |\n",
            "| time_elapsed       | 28.7          |\n",
            "| total_timesteps    | 73728         |\n",
            "| value_loss         | 1.400949      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00010847484 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 312           |\n",
            "| ep_reward_mean     | 84.1          |\n",
            "| explained_variance | 0.398         |\n",
            "| fps                | 8964          |\n",
            "| n_updates          | 19            |\n",
            "| policy_entropy     | 1.3427347     |\n",
            "| policy_loss        | 0.00021173581 |\n",
            "| serial_timesteps   | 4864          |\n",
            "| time_elapsed       | 30.4          |\n",
            "| total_timesteps    | 77824         |\n",
            "| value_loss         | 1.3197522     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=95.78 +/- 0.52\n",
            "Episode length: 168.20 +/- 27.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00035537797  |\n",
            "| clipfrac           | 0.00012207031  |\n",
            "| ep_len_mean        | 284            |\n",
            "| ep_reward_mean     | 85.4           |\n",
            "| explained_variance | 0.418          |\n",
            "| fps                | 3235           |\n",
            "| n_updates          | 20             |\n",
            "| policy_entropy     | 1.3413336      |\n",
            "| policy_loss        | -0.00029151462 |\n",
            "| serial_timesteps   | 5120           |\n",
            "| time_elapsed       | 30.9           |\n",
            "| total_timesteps    | 81920          |\n",
            "| value_loss         | 1.1617074      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013768872  |\n",
            "| clipfrac           | 0.009033203   |\n",
            "| ep_len_mean        | 256           |\n",
            "| ep_reward_mean     | 86.5          |\n",
            "| explained_variance | 0.441         |\n",
            "| fps                | 9147          |\n",
            "| n_updates          | 21            |\n",
            "| policy_entropy     | 1.3403745     |\n",
            "| policy_loss        | -0.0017986721 |\n",
            "| serial_timesteps   | 5376          |\n",
            "| time_elapsed       | 32.1          |\n",
            "| total_timesteps    | 86016         |\n",
            "| value_loss         | 1.2866377     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=95.09 +/- 0.62\n",
            "Episode length: 172.60 +/- 65.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00088767963 |\n",
            "| clipfrac           | 0.0041503906  |\n",
            "| ep_len_mean        | 240           |\n",
            "| ep_reward_mean     | 87.2          |\n",
            "| explained_variance | 0.447         |\n",
            "| fps                | 3178          |\n",
            "| n_updates          | 22            |\n",
            "| policy_entropy     | 1.3390881     |\n",
            "| policy_loss        | -0.0017364244 |\n",
            "| serial_timesteps   | 5632          |\n",
            "| time_elapsed       | 32.6          |\n",
            "| total_timesteps    | 90112         |\n",
            "| value_loss         | 1.3449855     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.234688e-05   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 225            |\n",
            "| ep_reward_mean     | 87.8           |\n",
            "| explained_variance | 0.481          |\n",
            "| fps                | 9135           |\n",
            "| n_updates          | 23             |\n",
            "| policy_entropy     | 1.3370923      |\n",
            "| policy_loss        | -5.6136094e-05 |\n",
            "| serial_timesteps   | 5888           |\n",
            "| time_elapsed       | 33.9           |\n",
            "| total_timesteps    | 94208          |\n",
            "| value_loss         | 1.3086979      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0001582752   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 220            |\n",
            "| ep_reward_mean     | 87.9           |\n",
            "| explained_variance | 0.492          |\n",
            "| fps                | 9032           |\n",
            "| n_updates          | 24             |\n",
            "| policy_entropy     | 1.3352418      |\n",
            "| policy_loss        | -0.00024079271 |\n",
            "| serial_timesteps   | 6144           |\n",
            "| time_elapsed       | 34.3           |\n",
            "| total_timesteps    | 98304          |\n",
            "| value_loss         | 0.8511597      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=94.60 +/- 0.83\n",
            "Episode length: 166.00 +/- 40.70\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00029419118  |\n",
            "| clipfrac           | 0.00036621094  |\n",
            "| ep_len_mean        | 216            |\n",
            "| ep_reward_mean     | 88             |\n",
            "| explained_variance | 0.508          |\n",
            "| fps                | 3262           |\n",
            "| n_updates          | 25             |\n",
            "| policy_entropy     | 1.3333396      |\n",
            "| policy_loss        | -0.00034853638 |\n",
            "| serial_timesteps   | 6400           |\n",
            "| time_elapsed       | 34.8           |\n",
            "| total_timesteps    | 102400         |\n",
            "| value_loss         | 1.2811728      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013951753  |\n",
            "| clipfrac           | 0.010131836   |\n",
            "| ep_len_mean        | 209           |\n",
            "| ep_reward_mean     | 88.3          |\n",
            "| explained_variance | 0.523         |\n",
            "| fps                | 9056          |\n",
            "| n_updates          | 26            |\n",
            "| policy_entropy     | 1.3321781     |\n",
            "| policy_loss        | -0.0019389394 |\n",
            "| serial_timesteps   | 6656          |\n",
            "| time_elapsed       | 36            |\n",
            "| total_timesteps    | 106496        |\n",
            "| value_loss         | 1.1145148     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=93.13 +/- 0.58\n",
            "Episode length: 167.80 +/- 39.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0009684791  |\n",
            "| clipfrac           | 0.006286621   |\n",
            "| ep_len_mean        | 209           |\n",
            "| ep_reward_mean     | 88.3          |\n",
            "| explained_variance | 0.553         |\n",
            "| fps                | 3248          |\n",
            "| n_updates          | 27            |\n",
            "| policy_entropy     | 1.3297985     |\n",
            "| policy_loss        | -0.0009089707 |\n",
            "| serial_timesteps   | 6912          |\n",
            "| time_elapsed       | 36.5          |\n",
            "| total_timesteps    | 110592        |\n",
            "| value_loss         | 0.991483      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00022482661  |\n",
            "| clipfrac           | 0.00012207031  |\n",
            "| ep_len_mean        | 206            |\n",
            "| ep_reward_mean     | 88.3           |\n",
            "| explained_variance | 0.534          |\n",
            "| fps                | 9191           |\n",
            "| n_updates          | 28             |\n",
            "| policy_entropy     | 1.3277144      |\n",
            "| policy_loss        | -0.00025423497 |\n",
            "| serial_timesteps   | 7168           |\n",
            "| time_elapsed       | 37.7           |\n",
            "| total_timesteps    | 114688         |\n",
            "| value_loss         | 1.1957119      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00064609305  |\n",
            "| clipfrac           | 0.0012207031   |\n",
            "| ep_len_mean        | 196            |\n",
            "| ep_reward_mean     | 88.7           |\n",
            "| explained_variance | 0.578          |\n",
            "| fps                | 9099           |\n",
            "| n_updates          | 29             |\n",
            "| policy_entropy     | 1.326226       |\n",
            "| policy_loss        | -0.00033477746 |\n",
            "| serial_timesteps   | 7424           |\n",
            "| time_elapsed       | 38.2           |\n",
            "| total_timesteps    | 118784         |\n",
            "| value_loss         | 1.1974596      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=91.89 +/- 0.88\n",
            "Episode length: 153.60 +/- 25.22\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014385049  |\n",
            "| clipfrac           | 0.011413574   |\n",
            "| ep_len_mean        | 184           |\n",
            "| ep_reward_mean     | 89.1          |\n",
            "| explained_variance | 0.554         |\n",
            "| fps                | 3404          |\n",
            "| n_updates          | 30            |\n",
            "| policy_entropy     | 1.3257326     |\n",
            "| policy_loss        | -0.0013982106 |\n",
            "| serial_timesteps   | 7680          |\n",
            "| time_elapsed       | 38.6          |\n",
            "| total_timesteps    | 122880        |\n",
            "| value_loss         | 1.1093516     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013921428  |\n",
            "| clipfrac           | 0.008728027   |\n",
            "| ep_len_mean        | 171           |\n",
            "| ep_reward_mean     | 89.7          |\n",
            "| explained_variance | 0.562         |\n",
            "| fps                | 9098          |\n",
            "| n_updates          | 31            |\n",
            "| policy_entropy     | 1.3243852     |\n",
            "| policy_loss        | -0.0010323271 |\n",
            "| serial_timesteps   | 7936          |\n",
            "| time_elapsed       | 39.8          |\n",
            "| total_timesteps    | 126976        |\n",
            "| value_loss         | 1.1970359     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=92.53 +/- 0.57\n",
            "Episode length: 125.00 +/- 5.33\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00027550213  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 166            |\n",
            "| ep_reward_mean     | 89.9           |\n",
            "| explained_variance | 0.61           |\n",
            "| fps                | 3817           |\n",
            "| n_updates          | 32             |\n",
            "| policy_entropy     | 1.3224286      |\n",
            "| policy_loss        | -0.00010743344 |\n",
            "| serial_timesteps   | 8192           |\n",
            "| time_elapsed       | 40.3           |\n",
            "| total_timesteps    | 131072         |\n",
            "| value_loss         | 0.87541014     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0012554618 |\n",
            "| clipfrac           | 0.007873535  |\n",
            "| ep_len_mean        | 159          |\n",
            "| ep_reward_mean     | 90.3         |\n",
            "| explained_variance | 0.596        |\n",
            "| fps                | 9232         |\n",
            "| n_updates          | 33           |\n",
            "| policy_entropy     | 1.3207898    |\n",
            "| policy_loss        | -0.001183795 |\n",
            "| serial_timesteps   | 8448         |\n",
            "| time_elapsed       | 41.4         |\n",
            "| total_timesteps    | 135168       |\n",
            "| value_loss         | 1.0733858    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0012545856  |\n",
            "| clipfrac           | 0.0040893555  |\n",
            "| ep_len_mean        | 161           |\n",
            "| ep_reward_mean     | 90            |\n",
            "| explained_variance | 0.627         |\n",
            "| fps                | 9133          |\n",
            "| n_updates          | 34            |\n",
            "| policy_entropy     | 1.3193638     |\n",
            "| policy_loss        | -0.0013008668 |\n",
            "| serial_timesteps   | 8704          |\n",
            "| time_elapsed       | 41.8          |\n",
            "| total_timesteps    | 139264        |\n",
            "| value_loss         | 1.0090857     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=91.26 +/- 1.38\n",
            "Episode length: 161.40 +/- 44.01\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0005931215  |\n",
            "| clipfrac           | 0.0020751953  |\n",
            "| ep_len_mean        | 159           |\n",
            "| ep_reward_mean     | 90.1          |\n",
            "| explained_variance | 0.66          |\n",
            "| fps                | 3293          |\n",
            "| n_updates          | 35            |\n",
            "| policy_entropy     | 1.316557      |\n",
            "| policy_loss        | -0.0004459348 |\n",
            "| serial_timesteps   | 8960          |\n",
            "| time_elapsed       | 42.3          |\n",
            "| total_timesteps    | 143360        |\n",
            "| value_loss         | 0.944732      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0012791159  |\n",
            "| clipfrac           | 0.009460449   |\n",
            "| ep_len_mean        | 157           |\n",
            "| ep_reward_mean     | 90.1          |\n",
            "| explained_variance | 0.655         |\n",
            "| fps                | 9152          |\n",
            "| n_updates          | 36            |\n",
            "| policy_entropy     | 1.3153255     |\n",
            "| policy_loss        | -0.0012792717 |\n",
            "| serial_timesteps   | 9216          |\n",
            "| time_elapsed       | 43.5          |\n",
            "| total_timesteps    | 147456        |\n",
            "| value_loss         | 0.8744713     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=91.46 +/- 1.83\n",
            "Episode length: 146.00 +/- 42.30\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017354793  |\n",
            "| clipfrac           | 0.017089844   |\n",
            "| ep_len_mean        | 154           |\n",
            "| ep_reward_mean     | 90.3          |\n",
            "| explained_variance | 0.66          |\n",
            "| fps                | 3458          |\n",
            "| n_updates          | 37            |\n",
            "| policy_entropy     | 1.314505      |\n",
            "| policy_loss        | -0.0018566981 |\n",
            "| serial_timesteps   | 9472          |\n",
            "| time_elapsed       | 44            |\n",
            "| total_timesteps    | 151552        |\n",
            "| value_loss         | 0.7961665     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0010204143   |\n",
            "| clipfrac           | 0.0022583008   |\n",
            "| ep_len_mean        | 149            |\n",
            "| ep_reward_mean     | 90.5           |\n",
            "| explained_variance | 0.697          |\n",
            "| fps                | 9037           |\n",
            "| n_updates          | 38             |\n",
            "| policy_entropy     | 1.3138628      |\n",
            "| policy_loss        | -0.00085733266 |\n",
            "| serial_timesteps   | 9728           |\n",
            "| time_elapsed       | 45.1           |\n",
            "| total_timesteps    | 155648         |\n",
            "| value_loss         | 0.80199033     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000732605   |\n",
            "| clipfrac           | 0.0026245117  |\n",
            "| ep_len_mean        | 146           |\n",
            "| ep_reward_mean     | 90.6          |\n",
            "| explained_variance | 0.69          |\n",
            "| fps                | 9318          |\n",
            "| n_updates          | 39            |\n",
            "| policy_entropy     | 1.3130946     |\n",
            "| policy_loss        | -0.0007523696 |\n",
            "| serial_timesteps   | 9984          |\n",
            "| time_elapsed       | 45.6          |\n",
            "| total_timesteps    | 159744        |\n",
            "| value_loss         | 0.7425685     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=91.45 +/- 0.65\n",
            "Episode length: 122.00 +/- 7.01\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00078791135  |\n",
            "| clipfrac           | 0.0013427734   |\n",
            "| ep_len_mean        | 146            |\n",
            "| ep_reward_mean     | 90.5           |\n",
            "| explained_variance | 0.714          |\n",
            "| fps                | 3836           |\n",
            "| n_updates          | 40             |\n",
            "| policy_entropy     | 1.3115542      |\n",
            "| policy_loss        | -0.00039434282 |\n",
            "| serial_timesteps   | 10240          |\n",
            "| time_elapsed       | 46             |\n",
            "| total_timesteps    | 163840         |\n",
            "| value_loss         | 0.7103055      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014954954  |\n",
            "| clipfrac           | 0.0099487305  |\n",
            "| ep_len_mean        | 148           |\n",
            "| ep_reward_mean     | 90.5          |\n",
            "| explained_variance | 0.721         |\n",
            "| fps                | 9075          |\n",
            "| n_updates          | 41            |\n",
            "| policy_entropy     | 1.310074      |\n",
            "| policy_loss        | -0.0012003073 |\n",
            "| serial_timesteps   | 10496         |\n",
            "| time_elapsed       | 47.1          |\n",
            "| total_timesteps    | 167936        |\n",
            "| value_loss         | 0.59745306    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=91.38 +/- 0.37\n",
            "Episode length: 132.80 +/- 23.61\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00052760215  |\n",
            "| clipfrac           | 0.0021972656   |\n",
            "| ep_len_mean        | 152            |\n",
            "| ep_reward_mean     | 90.3           |\n",
            "| explained_variance | 0.706          |\n",
            "| fps                | 3661           |\n",
            "| n_updates          | 42             |\n",
            "| policy_entropy     | 1.3083035      |\n",
            "| policy_loss        | -0.00039695576 |\n",
            "| serial_timesteps   | 10752          |\n",
            "| time_elapsed       | 47.6           |\n",
            "| total_timesteps    | 172032         |\n",
            "| value_loss         | 0.6525627      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002343637   |\n",
            "| clipfrac           | 0.020080566   |\n",
            "| ep_len_mean        | 153           |\n",
            "| ep_reward_mean     | 90.2          |\n",
            "| explained_variance | 0.763         |\n",
            "| fps                | 9267          |\n",
            "| n_updates          | 43            |\n",
            "| policy_entropy     | 1.3070252     |\n",
            "| policy_loss        | -0.0022955183 |\n",
            "| serial_timesteps   | 11008         |\n",
            "| time_elapsed       | 48.7          |\n",
            "| total_timesteps    | 176128        |\n",
            "| value_loss         | 0.6126123     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=91.00 +/- 0.38\n",
            "Episode length: 139.00 +/- 23.09\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00092276244 |\n",
            "| clipfrac           | 0.0043945312  |\n",
            "| ep_len_mean        | 150           |\n",
            "| ep_reward_mean     | 90.3          |\n",
            "| explained_variance | 0.778         |\n",
            "| fps                | 3599          |\n",
            "| n_updates          | 44            |\n",
            "| policy_entropy     | 1.3044404     |\n",
            "| policy_loss        | -0.0006239482 |\n",
            "| serial_timesteps   | 11264         |\n",
            "| time_elapsed       | 49.1          |\n",
            "| total_timesteps    | 180224        |\n",
            "| value_loss         | 0.5265067     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0009445493   |\n",
            "| clipfrac           | 0.0060424805   |\n",
            "| ep_len_mean        | 145            |\n",
            "| ep_reward_mean     | 90.4           |\n",
            "| explained_variance | 0.801          |\n",
            "| fps                | 9083           |\n",
            "| n_updates          | 45             |\n",
            "| policy_entropy     | 1.303377       |\n",
            "| policy_loss        | -0.00055867864 |\n",
            "| serial_timesteps   | 11520          |\n",
            "| time_elapsed       | 50.3           |\n",
            "| total_timesteps    | 184320         |\n",
            "| value_loss         | 0.5156635      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00041096765 |\n",
            "| clipfrac           | 0.0006713867  |\n",
            "| ep_len_mean        | 140           |\n",
            "| ep_reward_mean     | 90.6          |\n",
            "| explained_variance | 0.805         |\n",
            "| fps                | 8881          |\n",
            "| n_updates          | 46            |\n",
            "| policy_entropy     | 1.302605      |\n",
            "| policy_loss        | -0.00026799   |\n",
            "| serial_timesteps   | 11776         |\n",
            "| time_elapsed       | 50.7          |\n",
            "| total_timesteps    | 188416        |\n",
            "| value_loss         | 0.49115652    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=90.67 +/- 0.63\n",
            "Episode length: 128.00 +/- 16.52\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0005736157   |\n",
            "| clipfrac           | 0.0008544922   |\n",
            "| ep_len_mean        | 137            |\n",
            "| ep_reward_mean     | 90.6           |\n",
            "| explained_variance | 0.847          |\n",
            "| fps                | 3720           |\n",
            "| n_updates          | 47             |\n",
            "| policy_entropy     | 1.3015773      |\n",
            "| policy_loss        | -0.00034689653 |\n",
            "| serial_timesteps   | 12032          |\n",
            "| time_elapsed       | 51.2           |\n",
            "| total_timesteps    | 192512         |\n",
            "| value_loss         | 0.44819415     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020203863  |\n",
            "| clipfrac           | 0.014404297   |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | 90.7          |\n",
            "| explained_variance | 0.857         |\n",
            "| fps                | 9124          |\n",
            "| n_updates          | 48            |\n",
            "| policy_entropy     | 1.2999446     |\n",
            "| policy_loss        | -0.0014070398 |\n",
            "| serial_timesteps   | 12288         |\n",
            "| time_elapsed       | 52.3          |\n",
            "| total_timesteps    | 196608        |\n",
            "| value_loss         | 0.36292106    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=90.18 +/- 0.08\n",
            "Episode length: 119.60 +/- 3.07\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014598289  |\n",
            "| clipfrac           | 0.014404297   |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | 90.5          |\n",
            "| explained_variance | 0.876         |\n",
            "| fps                | 3946          |\n",
            "| n_updates          | 49            |\n",
            "| policy_entropy     | 1.2980978     |\n",
            "| policy_loss        | -0.0014909082 |\n",
            "| serial_timesteps   | 12544         |\n",
            "| time_elapsed       | 52.7          |\n",
            "| total_timesteps    | 200704        |\n",
            "| value_loss         | 0.35852858    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010870256  |\n",
            "| clipfrac           | 0.007873535   |\n",
            "| ep_len_mean        | 131           |\n",
            "| ep_reward_mean     | 90.7          |\n",
            "| explained_variance | 0.879         |\n",
            "| fps                | 8938          |\n",
            "| n_updates          | 50            |\n",
            "| policy_entropy     | 1.2989712     |\n",
            "| policy_loss        | -0.0006958501 |\n",
            "| serial_timesteps   | 12800         |\n",
            "| time_elapsed       | 53.8          |\n",
            "| total_timesteps    | 204800        |\n",
            "| value_loss         | 0.34317034    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011052471  |\n",
            "| clipfrac           | 0.0071411133  |\n",
            "| ep_len_mean        | 127           |\n",
            "| ep_reward_mean     | 90.9          |\n",
            "| explained_variance | 0.85          |\n",
            "| fps                | 9150          |\n",
            "| n_updates          | 51            |\n",
            "| policy_entropy     | 1.2988592     |\n",
            "| policy_loss        | -0.0012025639 |\n",
            "| serial_timesteps   | 13056         |\n",
            "| time_elapsed       | 54.2          |\n",
            "| total_timesteps    | 208896        |\n",
            "| value_loss         | 0.4104728     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=90.46 +/- 0.18\n",
            "Episode length: 135.40 +/- 27.46\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019317223  |\n",
            "| clipfrac           | 0.016174316   |\n",
            "| ep_len_mean        | 125           |\n",
            "| ep_reward_mean     | 90.9          |\n",
            "| explained_variance | 0.897         |\n",
            "| fps                | 3639          |\n",
            "| n_updates          | 52            |\n",
            "| policy_entropy     | 1.2982085     |\n",
            "| policy_loss        | -0.0016681437 |\n",
            "| serial_timesteps   | 13312         |\n",
            "| time_elapsed       | 54.7          |\n",
            "| total_timesteps    | 212992        |\n",
            "| value_loss         | 0.29610124    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00046486993 |\n",
            "| clipfrac           | 0.001159668   |\n",
            "| ep_len_mean        | 122           |\n",
            "| ep_reward_mean     | 91.2          |\n",
            "| explained_variance | 0.868         |\n",
            "| fps                | 9157          |\n",
            "| n_updates          | 53            |\n",
            "| policy_entropy     | 1.296438      |\n",
            "| policy_loss        | -0.0005712336 |\n",
            "| serial_timesteps   | 13568         |\n",
            "| time_elapsed       | 55.8          |\n",
            "| total_timesteps    | 217088        |\n",
            "| value_loss         | 0.34948385    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=91.62 +/- 1.36\n",
            "Episode length: 112.00 +/- 1.67\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0021498266 |\n",
            "| clipfrac           | 0.020202637  |\n",
            "| ep_len_mean        | 120          |\n",
            "| ep_reward_mean     | 91.3         |\n",
            "| explained_variance | 0.858        |\n",
            "| fps                | 4097         |\n",
            "| n_updates          | 54           |\n",
            "| policy_entropy     | 1.2923168    |\n",
            "| policy_loss        | -0.001398867 |\n",
            "| serial_timesteps   | 13824        |\n",
            "| time_elapsed       | 56.2         |\n",
            "| total_timesteps    | 221184       |\n",
            "| value_loss         | 0.36559907   |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00020889383  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 119            |\n",
            "| ep_reward_mean     | 91.4           |\n",
            "| explained_variance | 0.882          |\n",
            "| fps                | 8887           |\n",
            "| n_updates          | 55             |\n",
            "| policy_entropy     | 1.2883958      |\n",
            "| policy_loss        | -0.00027590455 |\n",
            "| serial_timesteps   | 14080          |\n",
            "| time_elapsed       | 57.2           |\n",
            "| total_timesteps    | 225280         |\n",
            "| value_loss         | 0.3038763      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0031217833  |\n",
            "| clipfrac           | 0.034179688   |\n",
            "| ep_len_mean        | 114           |\n",
            "| ep_reward_mean     | 91.6          |\n",
            "| explained_variance | 0.911         |\n",
            "| fps                | 9172          |\n",
            "| n_updates          | 56            |\n",
            "| policy_entropy     | 1.2867789     |\n",
            "| policy_loss        | -0.0027657344 |\n",
            "| serial_timesteps   | 14336         |\n",
            "| time_elapsed       | 57.7          |\n",
            "| total_timesteps    | 229376        |\n",
            "| value_loss         | 0.24845307    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=92.05 +/- 1.37\n",
            "Episode length: 101.40 +/- 13.17\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010592624  |\n",
            "| clipfrac           | 0.0068359375  |\n",
            "| ep_len_mean        | 109           |\n",
            "| ep_reward_mean     | 92            |\n",
            "| explained_variance | 0.901         |\n",
            "| fps                | 4243          |\n",
            "| n_updates          | 57            |\n",
            "| policy_entropy     | 1.2847216     |\n",
            "| policy_loss        | -0.0010236755 |\n",
            "| serial_timesteps   | 14592         |\n",
            "| time_elapsed       | 58.1          |\n",
            "| total_timesteps    | 233472        |\n",
            "| value_loss         | 0.23346923    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007369584  |\n",
            "| clipfrac           | 0.0045166016  |\n",
            "| ep_len_mean        | 106           |\n",
            "| ep_reward_mean     | 92.1          |\n",
            "| explained_variance | 0.909         |\n",
            "| fps                | 8568          |\n",
            "| n_updates          | 58            |\n",
            "| policy_entropy     | 1.2818439     |\n",
            "| policy_loss        | -0.0009168299 |\n",
            "| serial_timesteps   | 14848         |\n",
            "| time_elapsed       | 59.1          |\n",
            "| total_timesteps    | 237568        |\n",
            "| value_loss         | 0.23323779    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=93.46 +/- 0.31\n",
            "Episode length: 86.20 +/- 11.91\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0035313356 |\n",
            "| clipfrac           | 0.037109375  |\n",
            "| ep_len_mean        | 103          |\n",
            "| ep_reward_mean     | 92.3         |\n",
            "| explained_variance | 0.921        |\n",
            "| fps                | 4646         |\n",
            "| n_updates          | 59           |\n",
            "| policy_entropy     | 1.2782278    |\n",
            "| policy_loss        | -0.002642808 |\n",
            "| serial_timesteps   | 15104        |\n",
            "| time_elapsed       | 59.6         |\n",
            "| total_timesteps    | 241664       |\n",
            "| value_loss         | 0.1853612    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0025692754 |\n",
            "| clipfrac           | 0.02130127   |\n",
            "| ep_len_mean        | 95.3         |\n",
            "| ep_reward_mean     | 92.7         |\n",
            "| explained_variance | 0.902        |\n",
            "| fps                | 9259         |\n",
            "| n_updates          | 60           |\n",
            "| policy_entropy     | 1.2753567    |\n",
            "| policy_loss        | -0.00263962  |\n",
            "| serial_timesteps   | 15360        |\n",
            "| time_elapsed       | 60.5         |\n",
            "| total_timesteps    | 245760       |\n",
            "| value_loss         | 0.22634219   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016481538  |\n",
            "| clipfrac           | 0.0072021484  |\n",
            "| ep_len_mean        | 93.7          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.935         |\n",
            "| fps                | 8993          |\n",
            "| n_updates          | 61            |\n",
            "| policy_entropy     | 1.272197      |\n",
            "| policy_loss        | -0.0011460261 |\n",
            "| serial_timesteps   | 15616         |\n",
            "| time_elapsed       | 60.9          |\n",
            "| total_timesteps    | 249856        |\n",
            "| value_loss         | 0.14550707    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=92.07 +/- 0.92\n",
            "Episode length: 96.20 +/- 17.41\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0027855346  |\n",
            "| clipfrac           | 0.027893066   |\n",
            "| ep_len_mean        | 94            |\n",
            "| ep_reward_mean     | 92.7          |\n",
            "| explained_variance | 0.923         |\n",
            "| fps                | 4331          |\n",
            "| n_updates          | 62            |\n",
            "| policy_entropy     | 1.2710639     |\n",
            "| policy_loss        | -0.0021899808 |\n",
            "| serial_timesteps   | 15872         |\n",
            "| time_elapsed       | 61.4          |\n",
            "| total_timesteps    | 253952        |\n",
            "| value_loss         | 0.17355551    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020051207  |\n",
            "| clipfrac           | 0.015197754   |\n",
            "| ep_len_mean        | 91.6          |\n",
            "| ep_reward_mean     | 92.6          |\n",
            "| explained_variance | 0.95          |\n",
            "| fps                | 8925          |\n",
            "| n_updates          | 63            |\n",
            "| policy_entropy     | 1.2693342     |\n",
            "| policy_loss        | -0.0011152201 |\n",
            "| serial_timesteps   | 16128         |\n",
            "| time_elapsed       | 62.3          |\n",
            "| total_timesteps    | 258048        |\n",
            "| value_loss         | 0.105210625   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=92.96 +/- 0.05\n",
            "Episode length: 81.80 +/- 1.94\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0039767576  |\n",
            "| clipfrac           | 0.03967285    |\n",
            "| ep_len_mean        | 89.5          |\n",
            "| ep_reward_mean     | 92.7          |\n",
            "| explained_variance | 0.958         |\n",
            "| fps                | 4770          |\n",
            "| n_updates          | 64            |\n",
            "| policy_entropy     | 1.2652084     |\n",
            "| policy_loss        | -0.0028530678 |\n",
            "| serial_timesteps   | 16384         |\n",
            "| time_elapsed       | 62.8          |\n",
            "| total_timesteps    | 262144        |\n",
            "| value_loss         | 0.08784507    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017880815  |\n",
            "| clipfrac           | 0.0146484375  |\n",
            "| ep_len_mean        | 86.6          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.962         |\n",
            "| fps                | 8796          |\n",
            "| n_updates          | 65            |\n",
            "| policy_entropy     | 1.2613373     |\n",
            "| policy_loss        | -0.0021385686 |\n",
            "| serial_timesteps   | 16640         |\n",
            "| time_elapsed       | 63.6          |\n",
            "| total_timesteps    | 266240        |\n",
            "| value_loss         | 0.07482786    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=92.74 +/- 0.08\n",
            "Episode length: 79.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0024177853  |\n",
            "| clipfrac           | 0.021118164   |\n",
            "| ep_len_mean        | 83.8          |\n",
            "| ep_reward_mean     | 92.9          |\n",
            "| explained_variance | 0.978         |\n",
            "| fps                | 4784          |\n",
            "| n_updates          | 66            |\n",
            "| policy_entropy     | 1.2595257     |\n",
            "| policy_loss        | -0.0022521242 |\n",
            "| serial_timesteps   | 16896         |\n",
            "| time_elapsed       | 64.1          |\n",
            "| total_timesteps    | 270336        |\n",
            "| value_loss         | 0.042821314   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016304394  |\n",
            "| clipfrac           | 0.010925293   |\n",
            "| ep_len_mean        | 83.6          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.962         |\n",
            "| fps                | 9141          |\n",
            "| n_updates          | 67            |\n",
            "| policy_entropy     | 1.2556022     |\n",
            "| policy_loss        | -0.0007689992 |\n",
            "| serial_timesteps   | 17152         |\n",
            "| time_elapsed       | 65            |\n",
            "| total_timesteps    | 274432        |\n",
            "| value_loss         | 0.07079701    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00093613303 |\n",
            "| clipfrac           | 0.004333496   |\n",
            "| ep_len_mean        | 84.7          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.957         |\n",
            "| fps                | 9058          |\n",
            "| n_updates          | 68            |\n",
            "| policy_entropy     | 1.2530931     |\n",
            "| policy_loss        | -0.0010429104 |\n",
            "| serial_timesteps   | 17408         |\n",
            "| time_elapsed       | 65.4          |\n",
            "| total_timesteps    | 278528        |\n",
            "| value_loss         | 0.07533977    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=92.64 +/- 0.16\n",
            "Episode length: 79.40 +/- 1.50\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0024895533   |\n",
            "| clipfrac           | 0.016174316    |\n",
            "| ep_len_mean        | 83.2           |\n",
            "| ep_reward_mean     | 92.9           |\n",
            "| explained_variance | 0.965          |\n",
            "| fps                | 4801           |\n",
            "| n_updates          | 69             |\n",
            "| policy_entropy     | 1.2524825      |\n",
            "| policy_loss        | -0.00096313556 |\n",
            "| serial_timesteps   | 17664          |\n",
            "| time_elapsed       | 65.9           |\n",
            "| total_timesteps    | 282624         |\n",
            "| value_loss         | 0.060762584    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010619557  |\n",
            "| clipfrac           | 0.008911133   |\n",
            "| ep_len_mean        | 81.5          |\n",
            "| ep_reward_mean     | 92.9          |\n",
            "| explained_variance | 0.979         |\n",
            "| fps                | 8804          |\n",
            "| n_updates          | 70            |\n",
            "| policy_entropy     | 1.2504092     |\n",
            "| policy_loss        | -0.0012954067 |\n",
            "| serial_timesteps   | 17920         |\n",
            "| time_elapsed       | 66.7          |\n",
            "| total_timesteps    | 286720        |\n",
            "| value_loss         | 0.036569845   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=92.73 +/- 0.12\n",
            "Episode length: 77.40 +/- 1.02\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0023094993   |\n",
            "| clipfrac           | 0.017578125    |\n",
            "| ep_len_mean        | 81.4           |\n",
            "| ep_reward_mean     | 92.8           |\n",
            "| explained_variance | 0.974          |\n",
            "| fps                | 4902           |\n",
            "| n_updates          | 71             |\n",
            "| policy_entropy     | 1.2466652      |\n",
            "| policy_loss        | -0.00069468934 |\n",
            "| serial_timesteps   | 18176          |\n",
            "| time_elapsed       | 67.2           |\n",
            "| total_timesteps    | 290816         |\n",
            "| value_loss         | 0.046455227    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021658724  |\n",
            "| clipfrac           | 0.021606445   |\n",
            "| ep_len_mean        | 81.3          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 8885          |\n",
            "| n_updates          | 72            |\n",
            "| policy_entropy     | 1.2408988     |\n",
            "| policy_loss        | -0.0018771308 |\n",
            "| serial_timesteps   | 18432         |\n",
            "| time_elapsed       | 68            |\n",
            "| total_timesteps    | 294912        |\n",
            "| value_loss         | 0.01836875    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0029891892 |\n",
            "| clipfrac           | 0.028076172  |\n",
            "| ep_len_mean        | 79.8         |\n",
            "| ep_reward_mean     | 92.9         |\n",
            "| explained_variance | 0.984        |\n",
            "| fps                | 9051         |\n",
            "| n_updates          | 73           |\n",
            "| policy_entropy     | 1.2362334    |\n",
            "| policy_loss        | -0.001988478 |\n",
            "| serial_timesteps   | 18688        |\n",
            "| time_elapsed       | 68.5         |\n",
            "| total_timesteps    | 299008       |\n",
            "| value_loss         | 0.026266124  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=92.67 +/- 0.16\n",
            "Episode length: 78.20 +/- 1.60\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018653179  |\n",
            "| clipfrac           | 0.0119018555  |\n",
            "| ep_len_mean        | 79.1          |\n",
            "| ep_reward_mean     | 92.9          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 4737          |\n",
            "| n_updates          | 74            |\n",
            "| policy_entropy     | 1.2340086     |\n",
            "| policy_loss        | -0.0014214541 |\n",
            "| serial_timesteps   | 18944         |\n",
            "| time_elapsed       | 68.9          |\n",
            "| total_timesteps    | 303104        |\n",
            "| value_loss         | 0.020808853   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020707042  |\n",
            "| clipfrac           | 0.017089844   |\n",
            "| ep_len_mean        | 78.9          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.981         |\n",
            "| fps                | 9161          |\n",
            "| n_updates          | 75            |\n",
            "| policy_entropy     | 1.2310703     |\n",
            "| policy_loss        | -0.0018013336 |\n",
            "| serial_timesteps   | 19200         |\n",
            "| time_elapsed       | 69.8          |\n",
            "| total_timesteps    | 307200        |\n",
            "| value_loss         | 0.030380182   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=92.84 +/- 0.06\n",
            "Episode length: 76.40 +/- 0.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0024610725 |\n",
            "| clipfrac           | 0.021362305  |\n",
            "| ep_len_mean        | 79.4         |\n",
            "| ep_reward_mean     | 93           |\n",
            "| explained_variance | 0.984        |\n",
            "| fps                | 4865         |\n",
            "| n_updates          | 76           |\n",
            "| policy_entropy     | 1.2263135    |\n",
            "| policy_loss        | -0.002018512 |\n",
            "| serial_timesteps   | 19456        |\n",
            "| time_elapsed       | 70.2         |\n",
            "| total_timesteps    | 311296       |\n",
            "| value_loss         | 0.024690459  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021503903  |\n",
            "| clipfrac           | 0.018981934   |\n",
            "| ep_len_mean        | 78.2          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 8608          |\n",
            "| n_updates          | 77            |\n",
            "| policy_entropy     | 1.2193449     |\n",
            "| policy_loss        | -0.0015503656 |\n",
            "| serial_timesteps   | 19712         |\n",
            "| time_elapsed       | 71.1          |\n",
            "| total_timesteps    | 315392        |\n",
            "| value_loss         | 0.014034519   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020209407  |\n",
            "| clipfrac           | 0.0154418945  |\n",
            "| ep_len_mean        | 77.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.989         |\n",
            "| fps                | 9064          |\n",
            "| n_updates          | 78            |\n",
            "| policy_entropy     | 1.2130351     |\n",
            "| policy_loss        | -0.0011855318 |\n",
            "| serial_timesteps   | 19968         |\n",
            "| time_elapsed       | 71.6          |\n",
            "| total_timesteps    | 319488        |\n",
            "| value_loss         | 0.017359262   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=92.84 +/- 0.16\n",
            "Episode length: 76.00 +/- 1.55\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0029826385  |\n",
            "| clipfrac           | 0.03112793    |\n",
            "| ep_len_mean        | 77.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 4948          |\n",
            "| n_updates          | 79            |\n",
            "| policy_entropy     | 1.2109988     |\n",
            "| policy_loss        | -0.0016414225 |\n",
            "| serial_timesteps   | 20224         |\n",
            "| time_elapsed       | 72            |\n",
            "| total_timesteps    | 323584        |\n",
            "| value_loss         | 0.026610067   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0028888439  |\n",
            "| clipfrac           | 0.028198242   |\n",
            "| ep_len_mean        | 76.2          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.992         |\n",
            "| fps                | 9025          |\n",
            "| n_updates          | 80            |\n",
            "| policy_entropy     | 1.2095045     |\n",
            "| policy_loss        | -0.0015884343 |\n",
            "| serial_timesteps   | 20480         |\n",
            "| time_elapsed       | 72.8          |\n",
            "| total_timesteps    | 327680        |\n",
            "| value_loss         | 0.012537284   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=93.00 +/- 0.06\n",
            "Episode length: 74.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0006790412  |\n",
            "| clipfrac           | 0.0017089844  |\n",
            "| ep_len_mean        | 76.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 4954          |\n",
            "| n_updates          | 81            |\n",
            "| policy_entropy     | 1.2062923     |\n",
            "| policy_loss        | -0.0005725756 |\n",
            "| serial_timesteps   | 20736         |\n",
            "| time_elapsed       | 73.3          |\n",
            "| total_timesteps    | 331776        |\n",
            "| value_loss         | 0.02687036    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0014634768 |\n",
            "| clipfrac           | 0.012329102  |\n",
            "| ep_len_mean        | 76.4         |\n",
            "| ep_reward_mean     | 93.1         |\n",
            "| explained_variance | 0.99         |\n",
            "| fps                | 8666         |\n",
            "| n_updates          | 82           |\n",
            "| policy_entropy     | 1.2058649    |\n",
            "| policy_loss        | -0.000986564 |\n",
            "| serial_timesteps   | 20992        |\n",
            "| time_elapsed       | 74.1         |\n",
            "| total_timesteps    | 335872       |\n",
            "| value_loss         | 0.015152198  |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0019563462   |\n",
            "| clipfrac           | 0.0068969727   |\n",
            "| ep_len_mean        | 76             |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.978          |\n",
            "| fps                | 8912           |\n",
            "| n_updates          | 83             |\n",
            "| policy_entropy     | 1.203012       |\n",
            "| policy_loss        | -0.00035793678 |\n",
            "| serial_timesteps   | 21248          |\n",
            "| time_elapsed       | 74.6           |\n",
            "| total_timesteps    | 339968         |\n",
            "| value_loss         | 0.03301808     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=92.83 +/- 0.17\n",
            "Episode length: 76.00 +/- 1.41\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017725084  |\n",
            "| clipfrac           | 0.0134887695  |\n",
            "| ep_len_mean        | 78.1          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.974         |\n",
            "| fps                | 4916          |\n",
            "| n_updates          | 84            |\n",
            "| policy_entropy     | 1.2002263     |\n",
            "| policy_loss        | -0.0016871826 |\n",
            "| serial_timesteps   | 21504         |\n",
            "| time_elapsed       | 75.1          |\n",
            "| total_timesteps    | 344064        |\n",
            "| value_loss         | 0.038713045   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026338326  |\n",
            "| clipfrac           | 0.029846191   |\n",
            "| ep_len_mean        | 78.1          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.989         |\n",
            "| fps                | 9067          |\n",
            "| n_updates          | 85            |\n",
            "| policy_entropy     | 1.1952573     |\n",
            "| policy_loss        | -0.0024997098 |\n",
            "| serial_timesteps   | 21760         |\n",
            "| time_elapsed       | 75.9          |\n",
            "| total_timesteps    | 348160        |\n",
            "| value_loss         | 0.013942654   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=92.97 +/- 0.14\n",
            "Episode length: 74.80 +/- 1.33\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0017003253   |\n",
            "| clipfrac           | 0.007751465    |\n",
            "| ep_len_mean        | 75.7           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.975          |\n",
            "| fps                | 4915           |\n",
            "| n_updates          | 86             |\n",
            "| policy_entropy     | 1.1874748      |\n",
            "| policy_loss        | -0.00084896607 |\n",
            "| serial_timesteps   | 22016          |\n",
            "| time_elapsed       | 76.3           |\n",
            "| total_timesteps    | 352256         |\n",
            "| value_loss         | 0.038954947    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020859437  |\n",
            "| clipfrac           | 0.020324707   |\n",
            "| ep_len_mean        | 76.6          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 8793          |\n",
            "| n_updates          | 87            |\n",
            "| policy_entropy     | 1.1827289     |\n",
            "| policy_loss        | -0.0019232596 |\n",
            "| serial_timesteps   | 22272         |\n",
            "| time_elapsed       | 77.2          |\n",
            "| total_timesteps    | 356352        |\n",
            "| value_loss         | 0.013185246   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=92.95 +/- 0.13\n",
            "Episode length: 74.40 +/- 1.20\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015575635  |\n",
            "| clipfrac           | 0.009765625   |\n",
            "| ep_len_mean        | 75            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 5012          |\n",
            "| n_updates          | 88            |\n",
            "| policy_entropy     | 1.1792734     |\n",
            "| policy_loss        | -0.0008385437 |\n",
            "| serial_timesteps   | 22528         |\n",
            "| time_elapsed       | 77.6          |\n",
            "| total_timesteps    | 360448        |\n",
            "| value_loss         | 0.013353261   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0028300183  |\n",
            "| clipfrac           | 0.018859863   |\n",
            "| ep_len_mean        | 75.7          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.974         |\n",
            "| fps                | 9051          |\n",
            "| n_updates          | 89            |\n",
            "| policy_entropy     | 1.1754652     |\n",
            "| policy_loss        | -0.0009416995 |\n",
            "| serial_timesteps   | 22784         |\n",
            "| time_elapsed       | 78.5          |\n",
            "| total_timesteps    | 364544        |\n",
            "| value_loss         | 0.037533928   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014494399  |\n",
            "| clipfrac           | 0.013244629   |\n",
            "| ep_len_mean        | 76.4          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.992         |\n",
            "| fps                | 9007          |\n",
            "| n_updates          | 90            |\n",
            "| policy_entropy     | 1.1726099     |\n",
            "| policy_loss        | -0.0007278698 |\n",
            "| serial_timesteps   | 23040         |\n",
            "| time_elapsed       | 78.9          |\n",
            "| total_timesteps    | 368640        |\n",
            "| value_loss         | 0.011642298   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=93.01 +/- 0.07\n",
            "Episode length: 73.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016613116  |\n",
            "| clipfrac           | 0.0115356445  |\n",
            "| ep_len_mean        | 74.2          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 4962          |\n",
            "| n_updates          | 91            |\n",
            "| policy_entropy     | 1.170605      |\n",
            "| policy_loss        | -0.0012608211 |\n",
            "| serial_timesteps   | 23296         |\n",
            "| time_elapsed       | 79.4          |\n",
            "| total_timesteps    | 372736        |\n",
            "| value_loss         | 0.009136288   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020665205  |\n",
            "| clipfrac           | 0.018981934   |\n",
            "| ep_len_mean        | 74            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 8847          |\n",
            "| n_updates          | 92            |\n",
            "| policy_entropy     | 1.1700994     |\n",
            "| policy_loss        | -0.0009201903 |\n",
            "| serial_timesteps   | 23552         |\n",
            "| time_elapsed       | 80.2          |\n",
            "| total_timesteps    | 376832        |\n",
            "| value_loss         | 0.017410014   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=93.10 +/- 0.08\n",
            "Episode length: 72.80 +/- 0.75\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0023564836 |\n",
            "| clipfrac           | 0.020019531  |\n",
            "| ep_len_mean        | 74.9         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.991        |\n",
            "| fps                | 4981         |\n",
            "| n_updates          | 93           |\n",
            "| policy_entropy     | 1.1694504    |\n",
            "| policy_loss        | -0.001153175 |\n",
            "| serial_timesteps   | 23808        |\n",
            "| time_elapsed       | 80.7         |\n",
            "| total_timesteps    | 380928       |\n",
            "| value_loss         | 0.012364595  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015069724  |\n",
            "| clipfrac           | 0.013061523   |\n",
            "| ep_len_mean        | 74            |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 8948          |\n",
            "| n_updates          | 94            |\n",
            "| policy_entropy     | 1.1672832     |\n",
            "| policy_loss        | -0.0008207856 |\n",
            "| serial_timesteps   | 24064         |\n",
            "| time_elapsed       | 81.5          |\n",
            "| total_timesteps    | 385024        |\n",
            "| value_loss         | 0.012967207   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013104427  |\n",
            "| clipfrac           | 0.0068969727  |\n",
            "| ep_len_mean        | 75.8          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.968         |\n",
            "| fps                | 8824          |\n",
            "| n_updates          | 95            |\n",
            "| policy_entropy     | 1.1651328     |\n",
            "| policy_loss        | -0.0006356531 |\n",
            "| serial_timesteps   | 24320         |\n",
            "| time_elapsed       | 81.9          |\n",
            "| total_timesteps    | 389120        |\n",
            "| value_loss         | 0.04498485    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=93.25 +/- 0.09\n",
            "Episode length: 74.40 +/- 1.62\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0018208643   |\n",
            "| clipfrac           | 0.0119018555   |\n",
            "| ep_len_mean        | 76.6           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.963          |\n",
            "| fps                | 4862           |\n",
            "| n_updates          | 96             |\n",
            "| policy_entropy     | 1.1633968      |\n",
            "| policy_loss        | -0.00055674004 |\n",
            "| serial_timesteps   | 24576          |\n",
            "| time_elapsed       | 82.4           |\n",
            "| total_timesteps    | 393216         |\n",
            "| value_loss         | 0.05265722     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0027500323  |\n",
            "| clipfrac           | 0.025512695   |\n",
            "| ep_len_mean        | 76.2          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.985         |\n",
            "| fps                | 8991          |\n",
            "| n_updates          | 97            |\n",
            "| policy_entropy     | 1.1597486     |\n",
            "| policy_loss        | -0.0009069642 |\n",
            "| serial_timesteps   | 24832         |\n",
            "| time_elapsed       | 83.2          |\n",
            "| total_timesteps    | 397312        |\n",
            "| value_loss         | 0.020903047   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=93.15 +/- 0.04\n",
            "Episode length: 72.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014502078  |\n",
            "| clipfrac           | 0.012084961   |\n",
            "| ep_len_mean        | 74.7          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 5026          |\n",
            "| n_updates          | 98            |\n",
            "| policy_entropy     | 1.1564305     |\n",
            "| policy_loss        | -0.0008661269 |\n",
            "| serial_timesteps   | 25088         |\n",
            "| time_elapsed       | 83.7          |\n",
            "| total_timesteps    | 401408        |\n",
            "| value_loss         | 0.012698836   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0030901337  |\n",
            "| clipfrac           | 0.03100586    |\n",
            "| ep_len_mean        | 75            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 8743          |\n",
            "| n_updates          | 99            |\n",
            "| policy_entropy     | 1.1542797     |\n",
            "| policy_loss        | -0.0014180403 |\n",
            "| serial_timesteps   | 25344         |\n",
            "| time_elapsed       | 84.5          |\n",
            "| total_timesteps    | 405504        |\n",
            "| value_loss         | 0.017618146   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001732287   |\n",
            "| clipfrac           | 0.013061523   |\n",
            "| ep_len_mean        | 75.1          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.972         |\n",
            "| fps                | 9079          |\n",
            "| n_updates          | 100           |\n",
            "| policy_entropy     | 1.1521333     |\n",
            "| policy_loss        | -0.0008473006 |\n",
            "| serial_timesteps   | 25600         |\n",
            "| time_elapsed       | 85            |\n",
            "| total_timesteps    | 409600        |\n",
            "| value_loss         | 0.03777656    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=93.18 +/- 0.08\n",
            "Episode length: 72.20 +/- 0.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0021111912 |\n",
            "| clipfrac           | 0.019042969  |\n",
            "| ep_len_mean        | 75           |\n",
            "| ep_reward_mean     | 93.1         |\n",
            "| explained_variance | 0.98         |\n",
            "| fps                | 4951         |\n",
            "| n_updates          | 101          |\n",
            "| policy_entropy     | 1.1501899    |\n",
            "| policy_loss        | -0.002828372 |\n",
            "| serial_timesteps   | 25856        |\n",
            "| time_elapsed       | 85.4         |\n",
            "| total_timesteps    | 413696       |\n",
            "| value_loss         | 0.028833274  |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0009241298   |\n",
            "| clipfrac           | 0.003540039    |\n",
            "| ep_len_mean        | 74.1           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.993          |\n",
            "| fps                | 8766           |\n",
            "| n_updates          | 102            |\n",
            "| policy_entropy     | 1.1492468      |\n",
            "| policy_loss        | -0.00071983825 |\n",
            "| serial_timesteps   | 26112          |\n",
            "| time_elapsed       | 86.3           |\n",
            "| total_timesteps    | 417792         |\n",
            "| value_loss         | 0.008843963    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=93.18 +/- 0.14\n",
            "Episode length: 73.40 +/- 1.20\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013545379  |\n",
            "| clipfrac           | 0.010253906   |\n",
            "| ep_len_mean        | 75            |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.983         |\n",
            "| fps                | 4936          |\n",
            "| n_updates          | 103           |\n",
            "| policy_entropy     | 1.1439459     |\n",
            "| policy_loss        | -0.0019320524 |\n",
            "| serial_timesteps   | 26368         |\n",
            "| time_elapsed       | 86.7          |\n",
            "| total_timesteps    | 421888        |\n",
            "| value_loss         | 0.025797496   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0010232922 |\n",
            "| clipfrac           | 0.0038452148 |\n",
            "| ep_len_mean        | 76.6         |\n",
            "| ep_reward_mean     | 93           |\n",
            "| explained_variance | 0.987        |\n",
            "| fps                | 8852         |\n",
            "| n_updates          | 104          |\n",
            "| policy_entropy     | 1.1399909    |\n",
            "| policy_loss        | -0.000779503 |\n",
            "| serial_timesteps   | 26624        |\n",
            "| time_elapsed       | 87.6         |\n",
            "| total_timesteps    | 425984       |\n",
            "| value_loss         | 0.017453305  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=93.14 +/- 0.02\n",
            "Episode length: 72.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0025570705  |\n",
            "| clipfrac           | 0.020202637   |\n",
            "| ep_len_mean        | 75.9          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.978         |\n",
            "| fps                | 4964          |\n",
            "| n_updates          | 105           |\n",
            "| policy_entropy     | 1.1386156     |\n",
            "| policy_loss        | -0.0022654831 |\n",
            "| serial_timesteps   | 26880         |\n",
            "| time_elapsed       | 88            |\n",
            "| total_timesteps    | 430080        |\n",
            "| value_loss         | 0.030228104   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0034666273  |\n",
            "| clipfrac           | 0.038879395   |\n",
            "| ep_len_mean        | 75.8          |\n",
            "| ep_reward_mean     | 92.9          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 8651          |\n",
            "| n_updates          | 106           |\n",
            "| policy_entropy     | 1.1393476     |\n",
            "| policy_loss        | -0.0024343668 |\n",
            "| serial_timesteps   | 27136         |\n",
            "| time_elapsed       | 88.8          |\n",
            "| total_timesteps    | 434176        |\n",
            "| value_loss         | 0.015937027   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0030093794 |\n",
            "| clipfrac           | 0.03149414   |\n",
            "| ep_len_mean        | 76.7         |\n",
            "| ep_reward_mean     | 92.9         |\n",
            "| explained_variance | 0.977        |\n",
            "| fps                | 9110         |\n",
            "| n_updates          | 107          |\n",
            "| policy_entropy     | 1.1389333    |\n",
            "| policy_loss        | -0.002684035 |\n",
            "| serial_timesteps   | 27392        |\n",
            "| time_elapsed       | 89.3         |\n",
            "| total_timesteps    | 438272       |\n",
            "| value_loss         | 0.033727136  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=93.11 +/- 0.10\n",
            "Episode length: 73.80 +/- 1.33\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020208664  |\n",
            "| clipfrac           | 0.015563965   |\n",
            "| ep_len_mean        | 76.2          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.981         |\n",
            "| fps                | 4887          |\n",
            "| n_updates          | 108           |\n",
            "| policy_entropy     | 1.1351308     |\n",
            "| policy_loss        | -0.0011906835 |\n",
            "| serial_timesteps   | 27648         |\n",
            "| time_elapsed       | 89.8          |\n",
            "| total_timesteps    | 442368        |\n",
            "| value_loss         | 0.026470173   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0007889287   |\n",
            "| clipfrac           | 0.0013427734   |\n",
            "| ep_len_mean        | 76             |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.98           |\n",
            "| fps                | 9092           |\n",
            "| n_updates          | 109            |\n",
            "| policy_entropy     | 1.1312487      |\n",
            "| policy_loss        | -0.00029410102 |\n",
            "| serial_timesteps   | 27904          |\n",
            "| time_elapsed       | 90.6           |\n",
            "| total_timesteps    | 446464         |\n",
            "| value_loss         | 0.029351162    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=92.98 +/- 0.07\n",
            "Episode length: 73.20 +/- 0.75\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0010898183   |\n",
            "| clipfrac           | 0.006591797    |\n",
            "| ep_len_mean        | 75.1           |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.993          |\n",
            "| fps                | 4920           |\n",
            "| n_updates          | 110            |\n",
            "| policy_entropy     | 1.1270221      |\n",
            "| policy_loss        | -0.00027365662 |\n",
            "| serial_timesteps   | 28160          |\n",
            "| time_elapsed       | 91.1           |\n",
            "| total_timesteps    | 450560         |\n",
            "| value_loss         | 0.0101409685   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010190652  |\n",
            "| clipfrac           | 0.008056641   |\n",
            "| ep_len_mean        | 73.6          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 8842          |\n",
            "| n_updates          | 111           |\n",
            "| policy_entropy     | 1.1237168     |\n",
            "| policy_loss        | -0.0010579274 |\n",
            "| serial_timesteps   | 28416         |\n",
            "| time_elapsed       | 91.9          |\n",
            "| total_timesteps    | 454656        |\n",
            "| value_loss         | 0.010715986   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021864467  |\n",
            "| clipfrac           | 0.023010254   |\n",
            "| ep_len_mean        | 73.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 9208          |\n",
            "| n_updates          | 112           |\n",
            "| policy_entropy     | 1.1247873     |\n",
            "| policy_loss        | -0.0011558882 |\n",
            "| serial_timesteps   | 28672         |\n",
            "| time_elapsed       | 92.4          |\n",
            "| total_timesteps    | 458752        |\n",
            "| value_loss         | 0.009876047   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=93.03 +/- 0.11\n",
            "Episode length: 72.80 +/- 0.98\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019526285  |\n",
            "| clipfrac           | 0.01739502    |\n",
            "| ep_len_mean        | 73            |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 4989          |\n",
            "| n_updates          | 113           |\n",
            "| policy_entropy     | 1.1238596     |\n",
            "| policy_loss        | -0.0014393808 |\n",
            "| serial_timesteps   | 28928         |\n",
            "| time_elapsed       | 92.8          |\n",
            "| total_timesteps    | 462848        |\n",
            "| value_loss         | 0.007000324   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00023582704  |\n",
            "| clipfrac           | 0.0009765625   |\n",
            "| ep_len_mean        | 72.3           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.994          |\n",
            "| fps                | 8866           |\n",
            "| n_updates          | 114            |\n",
            "| policy_entropy     | 1.1233605      |\n",
            "| policy_loss        | -0.00049778726 |\n",
            "| serial_timesteps   | 29184          |\n",
            "| time_elapsed       | 93.6           |\n",
            "| total_timesteps    | 466944         |\n",
            "| value_loss         | 0.008271157    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=93.15 +/- 0.10\n",
            "Episode length: 71.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018774949  |\n",
            "| clipfrac           | 0.01953125    |\n",
            "| ep_len_mean        | 72.3          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5026          |\n",
            "| n_updates          | 115           |\n",
            "| policy_entropy     | 1.1235694     |\n",
            "| policy_loss        | -0.0011730279 |\n",
            "| serial_timesteps   | 29440         |\n",
            "| time_elapsed       | 94.1          |\n",
            "| total_timesteps    | 471040        |\n",
            "| value_loss         | 0.006204659   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0009414187   |\n",
            "| clipfrac           | 0.0079956055   |\n",
            "| ep_len_mean        | 72.8           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.991          |\n",
            "| fps                | 8642           |\n",
            "| n_updates          | 116            |\n",
            "| policy_entropy     | 1.1223918      |\n",
            "| policy_loss        | -0.00041461198 |\n",
            "| serial_timesteps   | 29696          |\n",
            "| time_elapsed       | 94.9           |\n",
            "| total_timesteps    | 475136         |\n",
            "| value_loss         | 0.012251118    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0031949386   |\n",
            "| clipfrac           | 0.037719727    |\n",
            "| ep_len_mean        | 73             |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.98           |\n",
            "| fps                | 9068           |\n",
            "| n_updates          | 117            |\n",
            "| policy_entropy     | 1.1233569      |\n",
            "| policy_loss        | -0.00052883907 |\n",
            "| serial_timesteps   | 29952          |\n",
            "| time_elapsed       | 95.4           |\n",
            "| total_timesteps    | 479232         |\n",
            "| value_loss         | 0.025589671    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=93.19 +/- 0.10\n",
            "Episode length: 71.40 +/- 0.80\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0006788093   |\n",
            "| clipfrac           | 0.0037231445   |\n",
            "| ep_len_mean        | 73.2           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.993          |\n",
            "| fps                | 4936           |\n",
            "| n_updates          | 118            |\n",
            "| policy_entropy     | 1.1234741      |\n",
            "| policy_loss        | -0.00060355046 |\n",
            "| serial_timesteps   | 30208          |\n",
            "| time_elapsed       | 95.8           |\n",
            "| total_timesteps    | 483328         |\n",
            "| value_loss         | 0.007876844    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0013248831   |\n",
            "| clipfrac           | 0.0056152344   |\n",
            "| ep_len_mean        | 73.5           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.96           |\n",
            "| fps                | 9015           |\n",
            "| n_updates          | 119            |\n",
            "| policy_entropy     | 1.1227366      |\n",
            "| policy_loss        | -0.00045091903 |\n",
            "| serial_timesteps   | 30464          |\n",
            "| time_elapsed       | 96.7           |\n",
            "| total_timesteps    | 487424         |\n",
            "| value_loss         | 0.05379707     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=93.23 +/- 0.09\n",
            "Episode length: 70.80 +/- 0.75\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00079174095  |\n",
            "| clipfrac           | 0.0045776367   |\n",
            "| ep_len_mean        | 75             |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.959          |\n",
            "| fps                | 5180           |\n",
            "| n_updates          | 120            |\n",
            "| policy_entropy     | 1.1228034      |\n",
            "| policy_loss        | -0.00032618444 |\n",
            "| serial_timesteps   | 30720          |\n",
            "| time_elapsed       | 97.1           |\n",
            "| total_timesteps    | 491520         |\n",
            "| value_loss         | 0.05228583     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00054307043  |\n",
            "| clipfrac           | 0.0017700195   |\n",
            "| ep_len_mean        | 74.3           |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.988          |\n",
            "| fps                | 8542           |\n",
            "| n_updates          | 121            |\n",
            "| policy_entropy     | 1.1234343      |\n",
            "| policy_loss        | -0.00041724264 |\n",
            "| serial_timesteps   | 30976          |\n",
            "| time_elapsed       | 97.9           |\n",
            "| total_timesteps    | 495616         |\n",
            "| value_loss         | 0.016246792    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0010562054   |\n",
            "| clipfrac           | 0.0053710938   |\n",
            "| ep_len_mean        | 73.6           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.972          |\n",
            "| fps                | 8757           |\n",
            "| n_updates          | 122            |\n",
            "| policy_entropy     | 1.1248379      |\n",
            "| policy_loss        | -0.00029396548 |\n",
            "| serial_timesteps   | 31232          |\n",
            "| time_elapsed       | 98.4           |\n",
            "| total_timesteps    | 499712         |\n",
            "| value_loss         | 0.037000757    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=93.15 +/- 0.07\n",
            "Episode length: 71.40 +/- 0.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0026325267 |\n",
            "| clipfrac           | 0.027770996  |\n",
            "| ep_len_mean        | 73.8         |\n",
            "| ep_reward_mean     | 93           |\n",
            "| explained_variance | 0.994        |\n",
            "| fps                | 4987         |\n",
            "| n_updates          | 123          |\n",
            "| policy_entropy     | 1.1255587    |\n",
            "| policy_loss        | -0.001705773 |\n",
            "| serial_timesteps   | 31488        |\n",
            "| time_elapsed       | 98.9         |\n",
            "| total_timesteps    | 503808       |\n",
            "| value_loss         | 0.0067765196 |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0016580928   |\n",
            "| clipfrac           | 0.010192871    |\n",
            "| ep_len_mean        | 72             |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.989          |\n",
            "| fps                | 8965           |\n",
            "| n_updates          | 124            |\n",
            "| policy_entropy     | 1.1262232      |\n",
            "| policy_loss        | -0.00027016038 |\n",
            "| serial_timesteps   | 31744          |\n",
            "| time_elapsed       | 99.7           |\n",
            "| total_timesteps    | 507904         |\n",
            "| value_loss         | 0.015871467    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=93.21 +/- 0.08\n",
            "Episode length: 70.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013217429  |\n",
            "| clipfrac           | 0.0071411133  |\n",
            "| ep_len_mean        | 72.5          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 4990          |\n",
            "| n_updates          | 125           |\n",
            "| policy_entropy     | 1.1237668     |\n",
            "| policy_loss        | -0.0011571252 |\n",
            "| serial_timesteps   | 32000         |\n",
            "| time_elapsed       | 100           |\n",
            "| total_timesteps    | 512000        |\n",
            "| value_loss         | 0.008914845   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026993807  |\n",
            "| clipfrac           | 0.020019531   |\n",
            "| ep_len_mean        | 72.8          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.978         |\n",
            "| fps                | 8588          |\n",
            "| n_updates          | 126           |\n",
            "| policy_entropy     | 1.1219394     |\n",
            "| policy_loss        | -0.0009262915 |\n",
            "| serial_timesteps   | 32256         |\n",
            "| time_elapsed       | 101           |\n",
            "| total_timesteps    | 516096        |\n",
            "| value_loss         | 0.029637503   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=93.26 +/- 0.10\n",
            "Episode length: 71.00 +/- 0.89\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0008564173  |\n",
            "| clipfrac           | 0.004638672   |\n",
            "| ep_len_mean        | 74.4          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.98          |\n",
            "| fps                | 5105          |\n",
            "| n_updates          | 127           |\n",
            "| policy_entropy     | 1.1228205     |\n",
            "| policy_loss        | 0.00014508565 |\n",
            "| serial_timesteps   | 32512         |\n",
            "| time_elapsed       | 101           |\n",
            "| total_timesteps    | 520192        |\n",
            "| value_loss         | 0.025901519   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0014177558   |\n",
            "| clipfrac           | 0.012268066    |\n",
            "| ep_len_mean        | 74.5           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.98           |\n",
            "| fps                | 8903           |\n",
            "| n_updates          | 128            |\n",
            "| policy_entropy     | 1.1222221      |\n",
            "| policy_loss        | -0.00032599765 |\n",
            "| serial_timesteps   | 32768          |\n",
            "| time_elapsed       | 102            |\n",
            "| total_timesteps    | 524288         |\n",
            "| value_loss         | 0.028557349    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0004567027   |\n",
            "| clipfrac           | 0.0015258789   |\n",
            "| ep_len_mean        | 73.2           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.984          |\n",
            "| fps                | 9127           |\n",
            "| n_updates          | 129            |\n",
            "| policy_entropy     | 1.1172142      |\n",
            "| policy_loss        | -0.00025464344 |\n",
            "| serial_timesteps   | 33024          |\n",
            "| time_elapsed       | 103            |\n",
            "| total_timesteps    | 528384         |\n",
            "| value_loss         | 0.020502225    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=93.18 +/- 0.07\n",
            "Episode length: 71.00 +/- 0.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018474316  |\n",
            "| clipfrac           | 0.014282227   |\n",
            "| ep_len_mean        | 73.8          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.96          |\n",
            "| fps                | 4996          |\n",
            "| n_updates          | 130           |\n",
            "| policy_entropy     | 1.1133116     |\n",
            "| policy_loss        | -0.0024002937 |\n",
            "| serial_timesteps   | 33280         |\n",
            "| time_elapsed       | 103           |\n",
            "| total_timesteps    | 532480        |\n",
            "| value_loss         | 0.052122973   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0024711068  |\n",
            "| clipfrac           | 0.022827148   |\n",
            "| ep_len_mean        | 73.3          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.978         |\n",
            "| fps                | 8638          |\n",
            "| n_updates          | 131           |\n",
            "| policy_entropy     | 1.1113757     |\n",
            "| policy_loss        | -0.0027051622 |\n",
            "| serial_timesteps   | 33536         |\n",
            "| time_elapsed       | 104           |\n",
            "| total_timesteps    | 536576        |\n",
            "| value_loss         | 0.029162534   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=93.30 +/- 0.02\n",
            "Episode length: 70.60 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0023545835  |\n",
            "| clipfrac           | 0.02331543    |\n",
            "| ep_len_mean        | 72.1          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.994         |\n",
            "| fps                | 5081          |\n",
            "| n_updates          | 132           |\n",
            "| policy_entropy     | 1.1065309     |\n",
            "| policy_loss        | -0.0016371054 |\n",
            "| serial_timesteps   | 33792         |\n",
            "| time_elapsed       | 104           |\n",
            "| total_timesteps    | 540672        |\n",
            "| value_loss         | 0.0056633493  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.001157626    |\n",
            "| clipfrac           | 0.007751465    |\n",
            "| ep_len_mean        | 71.8           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.971          |\n",
            "| fps                | 9140           |\n",
            "| n_updates          | 133            |\n",
            "| policy_entropy     | 1.1012688      |\n",
            "| policy_loss        | -0.00065842154 |\n",
            "| serial_timesteps   | 34048          |\n",
            "| time_elapsed       | 105            |\n",
            "| total_timesteps    | 544768         |\n",
            "| value_loss         | 0.03948348     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011044679  |\n",
            "| clipfrac           | 0.005493164   |\n",
            "| ep_len_mean        | 73.7          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.99          |\n",
            "| fps                | 8932          |\n",
            "| n_updates          | 134           |\n",
            "| policy_entropy     | 1.0974734     |\n",
            "| policy_loss        | -0.0009964219 |\n",
            "| serial_timesteps   | 34304         |\n",
            "| time_elapsed       | 106           |\n",
            "| total_timesteps    | 548864        |\n",
            "| value_loss         | 0.01353194    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=93.21 +/- 0.05\n",
            "Episode length: 71.20 +/- 0.98\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0022885636  |\n",
            "| clipfrac           | 0.023071289   |\n",
            "| ep_len_mean        | 74.5          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 5016          |\n",
            "| n_updates          | 135           |\n",
            "| policy_entropy     | 1.0939764     |\n",
            "| policy_loss        | -0.0008722857 |\n",
            "| serial_timesteps   | 34560         |\n",
            "| time_elapsed       | 106           |\n",
            "| total_timesteps    | 552960        |\n",
            "| value_loss         | 0.023766777   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0026697898 |\n",
            "| clipfrac           | 0.024108887  |\n",
            "| ep_len_mean        | 73.1         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.993        |\n",
            "| fps                | 8702         |\n",
            "| n_updates          | 136          |\n",
            "| policy_entropy     | 1.0910286    |\n",
            "| policy_loss        | -0.0024013   |\n",
            "| serial_timesteps   | 34816        |\n",
            "| time_elapsed       | 107          |\n",
            "| total_timesteps    | 557056       |\n",
            "| value_loss         | 0.008335906  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=93.22 +/- 0.08\n",
            "Episode length: 71.20 +/- 1.17\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007967509  |\n",
            "| clipfrac           | 0.0036010742  |\n",
            "| ep_len_mean        | 71.8          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 4978          |\n",
            "| n_updates          | 137           |\n",
            "| policy_entropy     | 1.0899779     |\n",
            "| policy_loss        | -0.0008679172 |\n",
            "| serial_timesteps   | 35072         |\n",
            "| time_elapsed       | 107           |\n",
            "| total_timesteps    | 561152        |\n",
            "| value_loss         | 0.006207731   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002873818   |\n",
            "| clipfrac           | 0.025878906   |\n",
            "| ep_len_mean        | 71.4          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 9224          |\n",
            "| n_updates          | 138           |\n",
            "| policy_entropy     | 1.0887471     |\n",
            "| policy_loss        | -0.0008570291 |\n",
            "| serial_timesteps   | 35328         |\n",
            "| time_elapsed       | 108           |\n",
            "| total_timesteps    | 565248        |\n",
            "| value_loss         | 0.0059282607  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00060890673 |\n",
            "| clipfrac           | 0.003967285   |\n",
            "| ep_len_mean        | 72.4          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 8853          |\n",
            "| n_updates          | 139           |\n",
            "| policy_entropy     | 1.0855757     |\n",
            "| policy_loss        | -0.0005984674 |\n",
            "| serial_timesteps   | 35584         |\n",
            "| time_elapsed       | 109           |\n",
            "| total_timesteps    | 569344        |\n",
            "| value_loss         | 0.023671575   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=93.14 +/- 0.14\n",
            "Episode length: 71.40 +/- 1.20\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021829493  |\n",
            "| clipfrac           | 0.021240234   |\n",
            "| ep_len_mean        | 73.6          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 4965          |\n",
            "| n_updates          | 140           |\n",
            "| policy_entropy     | 1.0849055     |\n",
            "| policy_loss        | -0.0029825638 |\n",
            "| serial_timesteps   | 35840         |\n",
            "| time_elapsed       | 109           |\n",
            "| total_timesteps    | 573440        |\n",
            "| value_loss         | 0.006808467   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0022490008  |\n",
            "| clipfrac           | 0.013977051   |\n",
            "| ep_len_mean        | 71.7          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 8953          |\n",
            "| n_updates          | 141           |\n",
            "| policy_entropy     | 1.0847715     |\n",
            "| policy_loss        | -0.0008285123 |\n",
            "| serial_timesteps   | 36096         |\n",
            "| time_elapsed       | 110           |\n",
            "| total_timesteps    | 577536        |\n",
            "| value_loss         | 0.008546036   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=93.22 +/- 0.04\n",
            "Episode length: 71.00 +/- 0.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014091072  |\n",
            "| clipfrac           | 0.009765625   |\n",
            "| ep_len_mean        | 72            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 4974          |\n",
            "| n_updates          | 142           |\n",
            "| policy_entropy     | 1.0827029     |\n",
            "| policy_loss        | -0.0012377995 |\n",
            "| serial_timesteps   | 36352         |\n",
            "| time_elapsed       | 110           |\n",
            "| total_timesteps    | 581632        |\n",
            "| value_loss         | 0.0044287494  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021225775  |\n",
            "| clipfrac           | 0.016296387   |\n",
            "| ep_len_mean        | 71            |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 9025          |\n",
            "| n_updates          | 143           |\n",
            "| policy_entropy     | 1.0797361     |\n",
            "| policy_loss        | -0.0008131997 |\n",
            "| serial_timesteps   | 36608         |\n",
            "| time_elapsed       | 111           |\n",
            "| total_timesteps    | 585728        |\n",
            "| value_loss         | 0.0043751798  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00090155797  |\n",
            "| clipfrac           | 0.0018310547   |\n",
            "| ep_len_mean        | 71.2           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.997          |\n",
            "| fps                | 9082           |\n",
            "| n_updates          | 144            |\n",
            "| policy_entropy     | 1.0779752      |\n",
            "| policy_loss        | -0.00011144995 |\n",
            "| serial_timesteps   | 36864          |\n",
            "| time_elapsed       | 112            |\n",
            "| total_timesteps    | 589824         |\n",
            "| value_loss         | 0.0045161545   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=93.26 +/- 0.05\n",
            "Episode length: 70.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00049707183 |\n",
            "| clipfrac           | 0.0018920898  |\n",
            "| ep_len_mean        | 72            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 5006          |\n",
            "| n_updates          | 145           |\n",
            "| policy_entropy     | 1.0764785     |\n",
            "| policy_loss        | -0.0001547783 |\n",
            "| serial_timesteps   | 37120         |\n",
            "| time_elapsed       | 112           |\n",
            "| total_timesteps    | 593920        |\n",
            "| value_loss         | 0.011775521   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0014869735 |\n",
            "| clipfrac           | 0.012573242  |\n",
            "| ep_len_mean        | 72.7         |\n",
            "| ep_reward_mean     | 93.1         |\n",
            "| explained_variance | 0.992        |\n",
            "| fps                | 9027         |\n",
            "| n_updates          | 146          |\n",
            "| policy_entropy     | 1.075016     |\n",
            "| policy_loss        | -0.001427044 |\n",
            "| serial_timesteps   | 37376        |\n",
            "| time_elapsed       | 113          |\n",
            "| total_timesteps    | 598016       |\n",
            "| value_loss         | 0.0101920795 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=93.21 +/- 0.08\n",
            "Episode length: 70.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001363626   |\n",
            "| clipfrac           | 0.008911133   |\n",
            "| ep_len_mean        | 72.7          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5008          |\n",
            "| n_updates          | 147           |\n",
            "| policy_entropy     | 1.0751841     |\n",
            "| policy_loss        | -0.0006900954 |\n",
            "| serial_timesteps   | 37632         |\n",
            "| time_elapsed       | 113           |\n",
            "| total_timesteps    | 602112        |\n",
            "| value_loss         | 0.005081827   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0023804265 |\n",
            "| clipfrac           | 0.01928711   |\n",
            "| ep_len_mean        | 72.1         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.986        |\n",
            "| fps                | 8841         |\n",
            "| n_updates          | 148          |\n",
            "| policy_entropy     | 1.0729965    |\n",
            "| policy_loss        | -0.000698169 |\n",
            "| serial_timesteps   | 37888        |\n",
            "| time_elapsed       | 114          |\n",
            "| total_timesteps    | 606208       |\n",
            "| value_loss         | 0.018630914  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=93.19 +/- 0.08\n",
            "Episode length: 70.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000819333   |\n",
            "| clipfrac           | 0.0013427734  |\n",
            "| ep_len_mean        | 71.7          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5018          |\n",
            "| n_updates          | 149           |\n",
            "| policy_entropy     | 1.0718333     |\n",
            "| policy_loss        | -0.0004631989 |\n",
            "| serial_timesteps   | 38144         |\n",
            "| time_elapsed       | 115           |\n",
            "| total_timesteps    | 610304        |\n",
            "| value_loss         | 0.0058516148  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001401042   |\n",
            "| clipfrac           | 0.010681152   |\n",
            "| ep_len_mean        | 71.1          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 9164          |\n",
            "| n_updates          | 150           |\n",
            "| policy_entropy     | 1.0710053     |\n",
            "| policy_loss        | -0.0015404017 |\n",
            "| serial_timesteps   | 38400         |\n",
            "| time_elapsed       | 116           |\n",
            "| total_timesteps    | 614400        |\n",
            "| value_loss         | 0.009715884   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000862051   |\n",
            "| clipfrac           | 0.0032348633  |\n",
            "| ep_len_mean        | 71.2          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.994         |\n",
            "| fps                | 9038          |\n",
            "| n_updates          | 151           |\n",
            "| policy_entropy     | 1.0675416     |\n",
            "| policy_loss        | -0.0004491282 |\n",
            "| serial_timesteps   | 38656         |\n",
            "| time_elapsed       | 116           |\n",
            "| total_timesteps    | 618496        |\n",
            "| value_loss         | 0.0074911285  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=93.22 +/- 0.06\n",
            "Episode length: 70.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016859807  |\n",
            "| clipfrac           | 0.016540527   |\n",
            "| ep_len_mean        | 71.2          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.984         |\n",
            "| fps                | 4990          |\n",
            "| n_updates          | 152           |\n",
            "| policy_entropy     | 1.0655063     |\n",
            "| policy_loss        | -0.0028189428 |\n",
            "| serial_timesteps   | 38912         |\n",
            "| time_elapsed       | 116           |\n",
            "| total_timesteps    | 622592        |\n",
            "| value_loss         | 0.020260563   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0011001634   |\n",
            "| clipfrac           | 0.003112793    |\n",
            "| ep_len_mean        | 71.6           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.993          |\n",
            "| fps                | 9064           |\n",
            "| n_updates          | 153            |\n",
            "| policy_entropy     | 1.0649483      |\n",
            "| policy_loss        | -0.00046202887 |\n",
            "| serial_timesteps   | 39168          |\n",
            "| time_elapsed       | 117            |\n",
            "| total_timesteps    | 626688         |\n",
            "| value_loss         | 0.008296734    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=93.26 +/- 0.05\n",
            "Episode length: 70.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00043879115 |\n",
            "| clipfrac           | 0.00024414062 |\n",
            "| ep_len_mean        | 72.9          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.977         |\n",
            "| fps                | 4997          |\n",
            "| n_updates          | 154           |\n",
            "| policy_entropy     | 1.0611807     |\n",
            "| policy_loss        | 0.00011919158 |\n",
            "| serial_timesteps   | 39424         |\n",
            "| time_elapsed       | 118           |\n",
            "| total_timesteps    | 630784        |\n",
            "| value_loss         | 0.030867599   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026732632  |\n",
            "| clipfrac           | 0.01776123    |\n",
            "| ep_len_mean        | 73.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 8855          |\n",
            "| n_updates          | 155           |\n",
            "| policy_entropy     | 1.0583186     |\n",
            "| policy_loss        | -0.0009757672 |\n",
            "| serial_timesteps   | 39680         |\n",
            "| time_elapsed       | 119           |\n",
            "| total_timesteps    | 634880        |\n",
            "| value_loss         | 0.008613228   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00075702486  |\n",
            "| clipfrac           | 0.0036621094   |\n",
            "| ep_len_mean        | 72.8           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.973          |\n",
            "| fps                | 8956           |\n",
            "| n_updates          | 156            |\n",
            "| policy_entropy     | 1.0538877      |\n",
            "| policy_loss        | -0.00069806905 |\n",
            "| serial_timesteps   | 39936          |\n",
            "| time_elapsed       | 119            |\n",
            "| total_timesteps    | 638976         |\n",
            "| value_loss         | 0.034999155    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=93.23 +/- 0.09\n",
            "Episode length: 70.40 +/- 0.80\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00038077362  |\n",
            "| clipfrac           | 0.0017089844   |\n",
            "| ep_len_mean        | 73.3           |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.987          |\n",
            "| fps                | 5013           |\n",
            "| n_updates          | 157            |\n",
            "| policy_entropy     | 1.0520688      |\n",
            "| policy_loss        | -0.00082969526 |\n",
            "| serial_timesteps   | 40192          |\n",
            "| time_elapsed       | 119            |\n",
            "| total_timesteps    | 643072         |\n",
            "| value_loss         | 0.016334439    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0035696006  |\n",
            "| clipfrac           | 0.03729248    |\n",
            "| ep_len_mean        | 72.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 9027          |\n",
            "| n_updates          | 158           |\n",
            "| policy_entropy     | 1.047641      |\n",
            "| policy_loss        | -0.0015670377 |\n",
            "| serial_timesteps   | 40448         |\n",
            "| time_elapsed       | 120           |\n",
            "| total_timesteps    | 647168        |\n",
            "| value_loss         | 0.016632274   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=93.21 +/- 0.11\n",
            "Episode length: 70.80 +/- 0.75\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0017183356   |\n",
            "| clipfrac           | 0.011047363    |\n",
            "| ep_len_mean        | 72.4           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.985          |\n",
            "| fps                | 4950           |\n",
            "| n_updates          | 159            |\n",
            "| policy_entropy     | 1.0435436      |\n",
            "| policy_loss        | -0.00057278597 |\n",
            "| serial_timesteps   | 40704          |\n",
            "| time_elapsed       | 121            |\n",
            "| total_timesteps    | 651264         |\n",
            "| value_loss         | 0.019279083    |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.001664211  |\n",
            "| clipfrac           | 0.007446289  |\n",
            "| ep_len_mean        | 71           |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.994        |\n",
            "| fps                | 8964         |\n",
            "| n_updates          | 160          |\n",
            "| policy_entropy     | 1.04007      |\n",
            "| policy_loss        | -0.000687655 |\n",
            "| serial_timesteps   | 40960        |\n",
            "| time_elapsed       | 122          |\n",
            "| total_timesteps    | 655360       |\n",
            "| value_loss         | 0.007072909  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0036813496  |\n",
            "| clipfrac           | 0.04034424    |\n",
            "| ep_len_mean        | 70.8          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 8997          |\n",
            "| n_updates          | 161           |\n",
            "| policy_entropy     | 1.0373577     |\n",
            "| policy_loss        | -0.0022605604 |\n",
            "| serial_timesteps   | 41216         |\n",
            "| time_elapsed       | 122           |\n",
            "| total_timesteps    | 659456        |\n",
            "| value_loss         | 0.016868724   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=93.21 +/- 0.06\n",
            "Episode length: 70.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002886178   |\n",
            "| clipfrac           | 0.030029297   |\n",
            "| ep_len_mean        | 71.5          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 5039          |\n",
            "| n_updates          | 162           |\n",
            "| policy_entropy     | 1.0342388     |\n",
            "| policy_loss        | -0.0017321452 |\n",
            "| serial_timesteps   | 41472         |\n",
            "| time_elapsed       | 122           |\n",
            "| total_timesteps    | 663552        |\n",
            "| value_loss         | 0.006980278   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026118548  |\n",
            "| clipfrac           | 0.026367188   |\n",
            "| ep_len_mean        | 71.1          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 9219          |\n",
            "| n_updates          | 163           |\n",
            "| policy_entropy     | 1.0299901     |\n",
            "| policy_loss        | -0.0012522239 |\n",
            "| serial_timesteps   | 41728         |\n",
            "| time_elapsed       | 123           |\n",
            "| total_timesteps    | 667648        |\n",
            "| value_loss         | 0.003249707   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=93.21 +/- 0.05\n",
            "Episode length: 70.40 +/- 0.49\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0019114097   |\n",
            "| clipfrac           | 0.014404297    |\n",
            "| ep_len_mean        | 70.6           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.985          |\n",
            "| fps                | 5060           |\n",
            "| n_updates          | 164            |\n",
            "| policy_entropy     | 1.0280612      |\n",
            "| policy_loss        | -0.00028324808 |\n",
            "| serial_timesteps   | 41984          |\n",
            "| time_elapsed       | 124            |\n",
            "| total_timesteps    | 671744         |\n",
            "| value_loss         | 0.019245727    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019571814  |\n",
            "| clipfrac           | 0.020812988   |\n",
            "| ep_len_mean        | 71.4          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.994         |\n",
            "| fps                | 8905          |\n",
            "| n_updates          | 165           |\n",
            "| policy_entropy     | 1.0286758     |\n",
            "| policy_loss        | -0.0014461031 |\n",
            "| serial_timesteps   | 42240         |\n",
            "| time_elapsed       | 125           |\n",
            "| total_timesteps    | 675840        |\n",
            "| value_loss         | 0.006778305   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0022561492 |\n",
            "| clipfrac           | 0.021484375  |\n",
            "| ep_len_mean        | 70.7         |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.995        |\n",
            "| fps                | 8848         |\n",
            "| n_updates          | 166          |\n",
            "| policy_entropy     | 1.0281183    |\n",
            "| policy_loss        | -0.001391975 |\n",
            "| serial_timesteps   | 42496        |\n",
            "| time_elapsed       | 125          |\n",
            "| total_timesteps    | 679936       |\n",
            "| value_loss         | 0.0063530537 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=93.24 +/- 0.06\n",
            "Episode length: 70.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002084418   |\n",
            "| clipfrac           | 0.01928711    |\n",
            "| ep_len_mean        | 71.2          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5043          |\n",
            "| n_updates          | 167           |\n",
            "| policy_entropy     | 1.0281222     |\n",
            "| policy_loss        | -0.0007292931 |\n",
            "| serial_timesteps   | 42752         |\n",
            "| time_elapsed       | 125           |\n",
            "| total_timesteps    | 684032        |\n",
            "| value_loss         | 0.003978324   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0017106569   |\n",
            "| clipfrac           | 0.012634277    |\n",
            "| ep_len_mean        | 70.5           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.997          |\n",
            "| fps                | 8758           |\n",
            "| n_updates          | 168            |\n",
            "| policy_entropy     | 1.0270643      |\n",
            "| policy_loss        | -0.00063879654 |\n",
            "| serial_timesteps   | 43008          |\n",
            "| time_elapsed       | 126            |\n",
            "| total_timesteps    | 688128         |\n",
            "| value_loss         | 0.0036688556   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=93.29 +/- 0.06\n",
            "Episode length: 69.60 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007312311  |\n",
            "| clipfrac           | 0.00018310547 |\n",
            "| ep_len_mean        | 70.9          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5032          |\n",
            "| n_updates          | 169           |\n",
            "| policy_entropy     | 1.0257148     |\n",
            "| policy_loss        | 0.0003571429  |\n",
            "| serial_timesteps   | 43264         |\n",
            "| time_elapsed       | 127           |\n",
            "| total_timesteps    | 692224        |\n",
            "| value_loss         | 0.004610252   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020797388  |\n",
            "| clipfrac           | 0.01940918    |\n",
            "| ep_len_mean        | 71.3          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 8967          |\n",
            "| n_updates          | 170           |\n",
            "| policy_entropy     | 1.0258571     |\n",
            "| policy_loss        | -0.0015892468 |\n",
            "| serial_timesteps   | 43520         |\n",
            "| time_elapsed       | 128           |\n",
            "| total_timesteps    | 696320        |\n",
            "| value_loss         | 0.004893308   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=93.27 +/- 0.05\n",
            "Episode length: 69.60 +/- 0.49\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0032573952   |\n",
            "| clipfrac           | 0.029846191    |\n",
            "| ep_len_mean        | 71.2           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.984          |\n",
            "| fps                | 5057           |\n",
            "| n_updates          | 171            |\n",
            "| policy_entropy     | 1.0272143      |\n",
            "| policy_loss        | -0.00085250475 |\n",
            "| serial_timesteps   | 43776          |\n",
            "| time_elapsed       | 128            |\n",
            "| total_timesteps    | 700416         |\n",
            "| value_loss         | 0.018975172    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0017214849   |\n",
            "| clipfrac           | 0.01184082     |\n",
            "| ep_len_mean        | 71.7           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.982          |\n",
            "| fps                | 8793           |\n",
            "| n_updates          | 172            |\n",
            "| policy_entropy     | 1.0284258      |\n",
            "| policy_loss        | -0.00075348304 |\n",
            "| serial_timesteps   | 44032          |\n",
            "| time_elapsed       | 129            |\n",
            "| total_timesteps    | 704512         |\n",
            "| value_loss         | 0.023171067    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0022846253   |\n",
            "| clipfrac           | 0.018920898    |\n",
            "| ep_len_mean        | 73.2           |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.973          |\n",
            "| fps                | 8888           |\n",
            "| n_updates          | 173            |\n",
            "| policy_entropy     | 1.0271412      |\n",
            "| policy_loss        | -0.00034841255 |\n",
            "| serial_timesteps   | 44288          |\n",
            "| time_elapsed       | 129            |\n",
            "| total_timesteps    | 708608         |\n",
            "| value_loss         | 0.03280614     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=93.24 +/- 0.07\n",
            "Episode length: 70.00 +/- 0.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011266595  |\n",
            "| clipfrac           | 0.008117676   |\n",
            "| ep_len_mean        | 74.4          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.968         |\n",
            "| fps                | 5100          |\n",
            "| n_updates          | 174           |\n",
            "| policy_entropy     | 1.0247691     |\n",
            "| policy_loss        | -0.0005538147 |\n",
            "| serial_timesteps   | 44544         |\n",
            "| time_elapsed       | 130           |\n",
            "| total_timesteps    | 712704        |\n",
            "| value_loss         | 0.042497262   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002040951   |\n",
            "| clipfrac           | 0.013305664   |\n",
            "| ep_len_mean        | 73.6          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.992         |\n",
            "| fps                | 9004          |\n",
            "| n_updates          | 175           |\n",
            "| policy_entropy     | 1.0245847     |\n",
            "| policy_loss        | -0.0011798439 |\n",
            "| serial_timesteps   | 44800         |\n",
            "| time_elapsed       | 131           |\n",
            "| total_timesteps    | 716800        |\n",
            "| value_loss         | 0.007677209   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=93.25 +/- 0.07\n",
            "Episode length: 69.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000946492   |\n",
            "| clipfrac           | 0.003479004   |\n",
            "| ep_len_mean        | 70.8          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 5028          |\n",
            "| n_updates          | 176           |\n",
            "| policy_entropy     | 1.0234425     |\n",
            "| policy_loss        | -0.0007398287 |\n",
            "| serial_timesteps   | 45056         |\n",
            "| time_elapsed       | 131           |\n",
            "| total_timesteps    | 720896        |\n",
            "| value_loss         | 0.0048745675  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.001495343    |\n",
            "| clipfrac           | 0.010131836    |\n",
            "| ep_len_mean        | 71.4           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.988          |\n",
            "| fps                | 8666           |\n",
            "| n_updates          | 177            |\n",
            "| policy_entropy     | 1.0225705      |\n",
            "| policy_loss        | -0.00025057275 |\n",
            "| serial_timesteps   | 45312          |\n",
            "| time_elapsed       | 132            |\n",
            "| total_timesteps    | 724992         |\n",
            "| value_loss         | 0.0146764815   |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0017950466 |\n",
            "| clipfrac           | 0.019104004  |\n",
            "| ep_len_mean        | 71.2         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.997        |\n",
            "| fps                | 8704         |\n",
            "| n_updates          | 178          |\n",
            "| policy_entropy     | 1.0220932    |\n",
            "| policy_loss        | -0.001013648 |\n",
            "| serial_timesteps   | 45568        |\n",
            "| time_elapsed       | 132          |\n",
            "| total_timesteps    | 729088       |\n",
            "| value_loss         | 0.0034361356 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=93.30 +/- 0.06\n",
            "Episode length: 69.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017575936  |\n",
            "| clipfrac           | 0.016174316   |\n",
            "| ep_len_mean        | 71.5          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 5072          |\n",
            "| n_updates          | 179           |\n",
            "| policy_entropy     | 1.021483      |\n",
            "| policy_loss        | -0.0002941657 |\n",
            "| serial_timesteps   | 45824         |\n",
            "| time_elapsed       | 133           |\n",
            "| total_timesteps    | 733184        |\n",
            "| value_loss         | 0.016013589   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015923649  |\n",
            "| clipfrac           | 0.011230469   |\n",
            "| ep_len_mean        | 73            |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 9012          |\n",
            "| n_updates          | 180           |\n",
            "| policy_entropy     | 1.0176681     |\n",
            "| policy_loss        | -0.0005879683 |\n",
            "| serial_timesteps   | 46080         |\n",
            "| time_elapsed       | 134           |\n",
            "| total_timesteps    | 737280        |\n",
            "| value_loss         | 0.0055492767  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=93.34 +/- 0.06\n",
            "Episode length: 69.40 +/- 0.49\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0007181146   |\n",
            "| clipfrac           | 0.002746582    |\n",
            "| ep_len_mean        | 72.5           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.984          |\n",
            "| fps                | 5052           |\n",
            "| n_updates          | 181            |\n",
            "| policy_entropy     | 1.0160574      |\n",
            "| policy_loss        | -3.0937124e-05 |\n",
            "| serial_timesteps   | 46336          |\n",
            "| time_elapsed       | 134            |\n",
            "| total_timesteps    | 741376         |\n",
            "| value_loss         | 0.020603884    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011349499  |\n",
            "| clipfrac           | 0.0045166016  |\n",
            "| ep_len_mean        | 71.9          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 8814          |\n",
            "| n_updates          | 182           |\n",
            "| policy_entropy     | 1.013315      |\n",
            "| policy_loss        | -0.0003961437 |\n",
            "| serial_timesteps   | 46592         |\n",
            "| time_elapsed       | 135           |\n",
            "| total_timesteps    | 745472        |\n",
            "| value_loss         | 0.0056012357  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0008075772   |\n",
            "| clipfrac           | 0.002746582    |\n",
            "| ep_len_mean        | 72.1           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.99           |\n",
            "| fps                | 8765           |\n",
            "| n_updates          | 183            |\n",
            "| policy_entropy     | 1.0111918      |\n",
            "| policy_loss        | -4.9218972e-05 |\n",
            "| serial_timesteps   | 46848          |\n",
            "| time_elapsed       | 135            |\n",
            "| total_timesteps    | 749568         |\n",
            "| value_loss         | 0.012738828    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=93.26 +/- 0.08\n",
            "Episode length: 69.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001376541   |\n",
            "| clipfrac           | 0.010803223   |\n",
            "| ep_len_mean        | 70.8          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5098          |\n",
            "| n_updates          | 184           |\n",
            "| policy_entropy     | 1.0112281     |\n",
            "| policy_loss        | -0.0012957355 |\n",
            "| serial_timesteps   | 47104         |\n",
            "| time_elapsed       | 136           |\n",
            "| total_timesteps    | 753664        |\n",
            "| value_loss         | 0.0041761985  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002631586   |\n",
            "| clipfrac           | 0.027770996   |\n",
            "| ep_len_mean        | 70.5          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 9085          |\n",
            "| n_updates          | 185           |\n",
            "| policy_entropy     | 1.0094347     |\n",
            "| policy_loss        | -0.0020354479 |\n",
            "| serial_timesteps   | 47360         |\n",
            "| time_elapsed       | 137           |\n",
            "| total_timesteps    | 757760        |\n",
            "| value_loss         | 0.003911192   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=93.25 +/- 0.08\n",
            "Episode length: 69.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0041485974  |\n",
            "| clipfrac           | 0.044433594   |\n",
            "| ep_len_mean        | 70            |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5115          |\n",
            "| n_updates          | 186           |\n",
            "| policy_entropy     | 1.0061643     |\n",
            "| policy_loss        | -0.0024525293 |\n",
            "| serial_timesteps   | 47616         |\n",
            "| time_elapsed       | 137           |\n",
            "| total_timesteps    | 761856        |\n",
            "| value_loss         | 0.0031077827  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019265163  |\n",
            "| clipfrac           | 0.016540527   |\n",
            "| ep_len_mean        | 71.2          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.984         |\n",
            "| fps                | 8780          |\n",
            "| n_updates          | 187           |\n",
            "| policy_entropy     | 1.0018964     |\n",
            "| policy_loss        | -0.0018553918 |\n",
            "| serial_timesteps   | 47872         |\n",
            "| time_elapsed       | 138           |\n",
            "| total_timesteps    | 765952        |\n",
            "| value_loss         | 0.019214738   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=93.27 +/- 0.08\n",
            "Episode length: 69.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014669569  |\n",
            "| clipfrac           | 0.007446289   |\n",
            "| ep_len_mean        | 71            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 5123          |\n",
            "| n_updates          | 188           |\n",
            "| policy_entropy     | 1.0010583     |\n",
            "| policy_loss        | -0.0011816891 |\n",
            "| serial_timesteps   | 48128         |\n",
            "| time_elapsed       | 138           |\n",
            "| total_timesteps    | 770048        |\n",
            "| value_loss         | 0.0061999536  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00082365196 |\n",
            "| clipfrac           | 0.0031738281  |\n",
            "| ep_len_mean        | 70.5          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 9104          |\n",
            "| n_updates          | 189           |\n",
            "| policy_entropy     | 0.9983723     |\n",
            "| policy_loss        | -0.0005972707 |\n",
            "| serial_timesteps   | 48384         |\n",
            "| time_elapsed       | 139           |\n",
            "| total_timesteps    | 774144        |\n",
            "| value_loss         | 0.002601088   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014580752  |\n",
            "| clipfrac           | 0.008605957   |\n",
            "| ep_len_mean        | 70.1          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 7051          |\n",
            "| n_updates          | 190           |\n",
            "| policy_entropy     | 0.99626064    |\n",
            "| policy_loss        | -0.0015373583 |\n",
            "| serial_timesteps   | 48640         |\n",
            "| time_elapsed       | 140           |\n",
            "| total_timesteps    | 778240        |\n",
            "| value_loss         | 0.0026423098  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=93.31 +/- 0.04\n",
            "Episode length: 69.20 +/- 0.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0032745076 |\n",
            "| clipfrac           | 0.029968262  |\n",
            "| ep_len_mean        | 69.8         |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.999        |\n",
            "| fps                | 4019         |\n",
            "| n_updates          | 191          |\n",
            "| policy_entropy     | 0.99392927   |\n",
            "| policy_loss        | -0.002364782 |\n",
            "| serial_timesteps   | 48896        |\n",
            "| time_elapsed       | 140          |\n",
            "| total_timesteps    | 782336       |\n",
            "| value_loss         | 0.0015361821 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0035799008  |\n",
            "| clipfrac           | 0.037963867   |\n",
            "| ep_len_mean        | 69.2          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.992         |\n",
            "| fps                | 7777          |\n",
            "| n_updates          | 192           |\n",
            "| policy_entropy     | 0.99163914    |\n",
            "| policy_loss        | -0.0018031708 |\n",
            "| serial_timesteps   | 49152         |\n",
            "| time_elapsed       | 141           |\n",
            "| total_timesteps    | 786432        |\n",
            "| value_loss         | 0.008931938   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=93.33 +/- 0.12\n",
            "Episode length: 70.00 +/- 1.10\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0037696057  |\n",
            "| clipfrac           | 0.04498291    |\n",
            "| ep_len_mean        | 70.3          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 4018          |\n",
            "| n_updates          | 193           |\n",
            "| policy_entropy     | 0.9896802     |\n",
            "| policy_loss        | -0.0029581788 |\n",
            "| serial_timesteps   | 49408         |\n",
            "| time_elapsed       | 142           |\n",
            "| total_timesteps    | 790528        |\n",
            "| value_loss         | 0.005859166   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00078556564  |\n",
            "| clipfrac           | 0.0022583008   |\n",
            "| ep_len_mean        | 71.4           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.998          |\n",
            "| fps                | 7015           |\n",
            "| n_updates          | 194            |\n",
            "| policy_entropy     | 0.98692167     |\n",
            "| policy_loss        | -6.7113244e-05 |\n",
            "| serial_timesteps   | 49664          |\n",
            "| time_elapsed       | 143            |\n",
            "| total_timesteps    | 794624         |\n",
            "| value_loss         | 0.0028234953   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014252805  |\n",
            "| clipfrac           | 0.008178711   |\n",
            "| ep_len_mean        | 69.9          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 7062          |\n",
            "| n_updates          | 195           |\n",
            "| policy_entropy     | 0.985239      |\n",
            "| policy_loss        | -0.0011221194 |\n",
            "| serial_timesteps   | 49920         |\n",
            "| time_elapsed       | 143           |\n",
            "| total_timesteps    | 798720        |\n",
            "| value_loss         | 0.0016449802  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=93.31 +/- 0.04\n",
            "Episode length: 69.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0025442026  |\n",
            "| clipfrac           | 0.027770996   |\n",
            "| ep_len_mean        | 69.4          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 4018          |\n",
            "| n_updates          | 196           |\n",
            "| policy_entropy     | 0.98420787    |\n",
            "| policy_loss        | -0.0011928912 |\n",
            "| serial_timesteps   | 50176         |\n",
            "| time_elapsed       | 144           |\n",
            "| total_timesteps    | 802816        |\n",
            "| value_loss         | 0.015201605   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0023513439  |\n",
            "| clipfrac           | 0.021911621   |\n",
            "| ep_len_mean        | 70            |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 7262          |\n",
            "| n_updates          | 197           |\n",
            "| policy_entropy     | 0.98382545    |\n",
            "| policy_loss        | -0.0018125889 |\n",
            "| serial_timesteps   | 50432         |\n",
            "| time_elapsed       | 145           |\n",
            "| total_timesteps    | 806912        |\n",
            "| value_loss         | 0.0020176766  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=93.33 +/- 0.05\n",
            "Episode length: 69.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018545159  |\n",
            "| clipfrac           | 0.015686035   |\n",
            "| ep_len_mean        | 70.5          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 3796          |\n",
            "| n_updates          | 198           |\n",
            "| policy_entropy     | 0.98245347    |\n",
            "| policy_loss        | -0.0011562721 |\n",
            "| serial_timesteps   | 50688         |\n",
            "| time_elapsed       | 145           |\n",
            "| total_timesteps    | 811008        |\n",
            "| value_loss         | 0.019986618   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0027737184  |\n",
            "| clipfrac           | 0.029724121   |\n",
            "| ep_len_mean        | 70.6          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 6984          |\n",
            "| n_updates          | 199           |\n",
            "| policy_entropy     | 0.9848331     |\n",
            "| policy_loss        | -0.0016181681 |\n",
            "| serial_timesteps   | 50944         |\n",
            "| time_elapsed       | 147           |\n",
            "| total_timesteps    | 815104        |\n",
            "| value_loss         | 0.003573839   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0013749007   |\n",
            "| clipfrac           | 0.0119018555   |\n",
            "| ep_len_mean        | 69.5           |\n",
            "| ep_reward_mean     | 93.4           |\n",
            "| explained_variance | 0.998          |\n",
            "| fps                | 8884           |\n",
            "| n_updates          | 200            |\n",
            "| policy_entropy     | 0.9829637      |\n",
            "| policy_loss        | -0.00069565594 |\n",
            "| serial_timesteps   | 51200          |\n",
            "| time_elapsed       | 147            |\n",
            "| total_timesteps    | 819200         |\n",
            "| value_loss         | 0.002744375    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=93.32 +/- 0.05\n",
            "Episode length: 69.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002594882   |\n",
            "| clipfrac           | 0.03100586    |\n",
            "| ep_len_mean        | 70.5          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.98          |\n",
            "| fps                | 5057          |\n",
            "| n_updates          | 201           |\n",
            "| policy_entropy     | 0.9801421     |\n",
            "| policy_loss        | -0.0017637433 |\n",
            "| serial_timesteps   | 51456         |\n",
            "| time_elapsed       | 148           |\n",
            "| total_timesteps    | 823296        |\n",
            "| value_loss         | 0.023057692   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0030481233   |\n",
            "| clipfrac           | 0.027160645    |\n",
            "| ep_len_mean        | 71             |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.98           |\n",
            "| fps                | 7614           |\n",
            "| n_updates          | 202            |\n",
            "| policy_entropy     | 0.9773828      |\n",
            "| policy_loss        | -0.00046997724 |\n",
            "| serial_timesteps   | 51712          |\n",
            "| time_elapsed       | 148            |\n",
            "| total_timesteps    | 827392         |\n",
            "| value_loss         | 0.02435643     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=93.30 +/- 0.08\n",
            "Episode length: 69.60 +/- 0.49\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0013309259   |\n",
            "| clipfrac           | 0.009277344    |\n",
            "| ep_len_mean        | 70.2           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.999          |\n",
            "| fps                | 4230           |\n",
            "| n_updates          | 203            |\n",
            "| policy_entropy     | 0.9757458      |\n",
            "| policy_loss        | -0.00092064193 |\n",
            "| serial_timesteps   | 51968          |\n",
            "| time_elapsed       | 149            |\n",
            "| total_timesteps    | 831488         |\n",
            "| value_loss         | 0.0013955397   |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0023969093 |\n",
            "| clipfrac           | 0.024841309  |\n",
            "| ep_len_mean        | 69           |\n",
            "| ep_reward_mean     | 93.4         |\n",
            "| explained_variance | 0.998        |\n",
            "| fps                | 9129         |\n",
            "| n_updates          | 204          |\n",
            "| policy_entropy     | 0.97433984   |\n",
            "| policy_loss        | -0.002009152 |\n",
            "| serial_timesteps   | 52224        |\n",
            "| time_elapsed       | 150          |\n",
            "| total_timesteps    | 835584       |\n",
            "| value_loss         | 0.0016747448 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020616276  |\n",
            "| clipfrac           | 0.015075684   |\n",
            "| ep_len_mean        | 68.9          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 8749          |\n",
            "| n_updates          | 205           |\n",
            "| policy_entropy     | 0.96896845    |\n",
            "| policy_loss        | -0.0013049929 |\n",
            "| serial_timesteps   | 52480         |\n",
            "| time_elapsed       | 150           |\n",
            "| total_timesteps    | 839680        |\n",
            "| value_loss         | 0.004675874   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=93.41 +/- 0.06\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.003455069   |\n",
            "| clipfrac           | 0.041809082   |\n",
            "| ep_len_mean        | 68.7          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 5051          |\n",
            "| n_updates          | 206           |\n",
            "| policy_entropy     | 0.9645439     |\n",
            "| policy_loss        | -0.0039781774 |\n",
            "| serial_timesteps   | 52736         |\n",
            "| time_elapsed       | 151           |\n",
            "| total_timesteps    | 843776        |\n",
            "| value_loss         | 0.0010865644  |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0014268032 |\n",
            "| clipfrac           | 0.012573242  |\n",
            "| ep_len_mean        | 69.7         |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.951        |\n",
            "| fps                | 8871         |\n",
            "| n_updates          | 207          |\n",
            "| policy_entropy     | 0.96112055   |\n",
            "| policy_loss        | 0.0010459687 |\n",
            "| serial_timesteps   | 52992        |\n",
            "| time_elapsed       | 152          |\n",
            "| total_timesteps    | 847872       |\n",
            "| value_loss         | 0.05133278   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=93.44 +/- 0.05\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002455697   |\n",
            "| clipfrac           | 0.0154418945  |\n",
            "| ep_len_mean        | 72.3          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.98          |\n",
            "| fps                | 5099          |\n",
            "| n_updates          | 208           |\n",
            "| policy_entropy     | 0.95870864    |\n",
            "| policy_loss        | -0.0012330301 |\n",
            "| serial_timesteps   | 53248         |\n",
            "| time_elapsed       | 152           |\n",
            "| total_timesteps    | 851968        |\n",
            "| value_loss         | 0.022903906   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0017374377 |\n",
            "| clipfrac           | 0.011962891  |\n",
            "| ep_len_mean        | 70           |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.998        |\n",
            "| fps                | 8992         |\n",
            "| n_updates          | 209          |\n",
            "| policy_entropy     | 0.9555863    |\n",
            "| policy_loss        | -0.001311685 |\n",
            "| serial_timesteps   | 53504        |\n",
            "| time_elapsed       | 153          |\n",
            "| total_timesteps    | 856064       |\n",
            "| value_loss         | 0.0016555322 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=93.45 +/- 0.02\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018923223  |\n",
            "| clipfrac           | 0.013916016   |\n",
            "| ep_len_mean        | 68.6          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5020          |\n",
            "| n_updates          | 210           |\n",
            "| policy_entropy     | 0.95355844    |\n",
            "| policy_loss        | -0.0014007961 |\n",
            "| serial_timesteps   | 53760         |\n",
            "| time_elapsed       | 153           |\n",
            "| total_timesteps    | 860160        |\n",
            "| value_loss         | 0.0031152056  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0022482264  |\n",
            "| clipfrac           | 0.020690918   |\n",
            "| ep_len_mean        | 69.1          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 9065          |\n",
            "| n_updates          | 211           |\n",
            "| policy_entropy     | 0.9530529     |\n",
            "| policy_loss        | -0.0019914352 |\n",
            "| serial_timesteps   | 54016         |\n",
            "| time_elapsed       | 154           |\n",
            "| total_timesteps    | 864256        |\n",
            "| value_loss         | 0.0023346744  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017448405  |\n",
            "| clipfrac           | 0.016967773   |\n",
            "| ep_len_mean        | 69.3          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.984         |\n",
            "| fps                | 8952          |\n",
            "| n_updates          | 212           |\n",
            "| policy_entropy     | 0.95242083    |\n",
            "| policy_loss        | -0.0006345095 |\n",
            "| serial_timesteps   | 54272         |\n",
            "| time_elapsed       | 155           |\n",
            "| total_timesteps    | 868352        |\n",
            "| value_loss         | 0.016657604   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=93.47 +/- 0.11\n",
            "Episode length: 69.80 +/- 3.12\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0032295326  |\n",
            "| clipfrac           | 0.03857422    |\n",
            "| ep_len_mean        | 70.6          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.981         |\n",
            "| fps                | 4933          |\n",
            "| n_updates          | 213           |\n",
            "| policy_entropy     | 0.95250916    |\n",
            "| policy_loss        | -0.0015168063 |\n",
            "| serial_timesteps   | 54528         |\n",
            "| time_elapsed       | 155           |\n",
            "| total_timesteps    | 872448        |\n",
            "| value_loss         | 0.021882413   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0016593855   |\n",
            "| clipfrac           | 0.012207031    |\n",
            "| ep_len_mean        | 71.2           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.995          |\n",
            "| fps                | 8692           |\n",
            "| n_updates          | 214            |\n",
            "| policy_entropy     | 0.9513757      |\n",
            "| policy_loss        | -0.00068152655 |\n",
            "| serial_timesteps   | 54784          |\n",
            "| time_elapsed       | 156            |\n",
            "| total_timesteps    | 876544         |\n",
            "| value_loss         | 0.0053161313   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=93.40 +/- 0.06\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0024662353  |\n",
            "| clipfrac           | 0.02166748    |\n",
            "| ep_len_mean        | 69.9          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5032          |\n",
            "| n_updates          | 215           |\n",
            "| policy_entropy     | 0.9499858     |\n",
            "| policy_loss        | -0.0015288083 |\n",
            "| serial_timesteps   | 55040         |\n",
            "| time_elapsed       | 156           |\n",
            "| total_timesteps    | 880640        |\n",
            "| value_loss         | 0.0036770161  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0016009127   |\n",
            "| clipfrac           | 0.013366699    |\n",
            "| ep_len_mean        | 69             |\n",
            "| ep_reward_mean     | 93.4           |\n",
            "| explained_variance | 0.999          |\n",
            "| fps                | 8702           |\n",
            "| n_updates          | 216            |\n",
            "| policy_entropy     | 0.9509027      |\n",
            "| policy_loss        | -0.00072149443 |\n",
            "| serial_timesteps   | 55296          |\n",
            "| time_elapsed       | 157            |\n",
            "| total_timesteps    | 884736         |\n",
            "| value_loss         | 0.0013506272   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014850679  |\n",
            "| clipfrac           | 0.010437012   |\n",
            "| ep_len_mean        | 68.8          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 8871          |\n",
            "| n_updates          | 217           |\n",
            "| policy_entropy     | 0.9533832     |\n",
            "| policy_loss        | -0.0004890441 |\n",
            "| serial_timesteps   | 55552         |\n",
            "| time_elapsed       | 158           |\n",
            "| total_timesteps    | 888832        |\n",
            "| value_loss         | 0.002468859   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=93.41 +/- 0.06\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021185237  |\n",
            "| clipfrac           | 0.018676758   |\n",
            "| ep_len_mean        | 68.9          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 5113          |\n",
            "| n_updates          | 218           |\n",
            "| policy_entropy     | 0.95473623    |\n",
            "| policy_loss        | -0.0023115815 |\n",
            "| serial_timesteps   | 55808         |\n",
            "| time_elapsed       | 158           |\n",
            "| total_timesteps    | 892928        |\n",
            "| value_loss         | 0.0014367852  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018252275  |\n",
            "| clipfrac           | 0.013916016   |\n",
            "| ep_len_mean        | 68.4          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 8914          |\n",
            "| n_updates          | 219           |\n",
            "| policy_entropy     | 0.9562224     |\n",
            "| policy_loss        | -0.001644514  |\n",
            "| serial_timesteps   | 56064         |\n",
            "| time_elapsed       | 159           |\n",
            "| total_timesteps    | 897024        |\n",
            "| value_loss         | 0.00095795275 |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=93.45 +/- 0.01\n",
            "Episode length: 68.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017526112  |\n",
            "| clipfrac           | 0.016235352   |\n",
            "| ep_len_mean        | 68.8          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5126          |\n",
            "| n_updates          | 220           |\n",
            "| policy_entropy     | 0.9541        |\n",
            "| policy_loss        | -0.0013216339 |\n",
            "| serial_timesteps   | 56320         |\n",
            "| time_elapsed       | 159           |\n",
            "| total_timesteps    | 901120        |\n",
            "| value_loss         | 0.0035293028  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002246479   |\n",
            "| clipfrac           | 0.020996094   |\n",
            "| ep_len_mean        | 68.8          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 8752          |\n",
            "| n_updates          | 221           |\n",
            "| policy_entropy     | 0.95132506    |\n",
            "| policy_loss        | -0.0013472794 |\n",
            "| serial_timesteps   | 56576         |\n",
            "| time_elapsed       | 160           |\n",
            "| total_timesteps    | 905216        |\n",
            "| value_loss         | 0.0023436225  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0028805009  |\n",
            "| clipfrac           | 0.033203125   |\n",
            "| ep_len_mean        | 68.3          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 8967          |\n",
            "| n_updates          | 222           |\n",
            "| policy_entropy     | 0.9506912     |\n",
            "| policy_loss        | -0.0030356403 |\n",
            "| serial_timesteps   | 56832         |\n",
            "| time_elapsed       | 161           |\n",
            "| total_timesteps    | 909312        |\n",
            "| value_loss         | 0.0009974954  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=93.50 +/- 0.10\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0038252587  |\n",
            "| clipfrac           | 0.04888916    |\n",
            "| ep_len_mean        | 69            |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 5007          |\n",
            "| n_updates          | 223           |\n",
            "| policy_entropy     | 0.9487778     |\n",
            "| policy_loss        | -0.0013134595 |\n",
            "| serial_timesteps   | 57088         |\n",
            "| time_elapsed       | 161           |\n",
            "| total_timesteps    | 913408        |\n",
            "| value_loss         | 0.018009067   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0006939989 |\n",
            "| clipfrac           | 0.0014648438 |\n",
            "| ep_len_mean        | 71.5         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.954        |\n",
            "| fps                | 8763         |\n",
            "| n_updates          | 224          |\n",
            "| policy_entropy     | 0.9471356    |\n",
            "| policy_loss        | 0.0005213215 |\n",
            "| serial_timesteps   | 57344        |\n",
            "| time_elapsed       | 162          |\n",
            "| total_timesteps    | 917504       |\n",
            "| value_loss         | 0.048271757  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=93.44 +/- 0.01\n",
            "Episode length: 68.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014398084  |\n",
            "| clipfrac           | 0.0087890625  |\n",
            "| ep_len_mean        | 71.3          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.989         |\n",
            "| fps                | 5106          |\n",
            "| n_updates          | 225           |\n",
            "| policy_entropy     | 0.94743276    |\n",
            "| policy_loss        | -0.0010080175 |\n",
            "| serial_timesteps   | 57600         |\n",
            "| time_elapsed       | 162           |\n",
            "| total_timesteps    | 921600        |\n",
            "| value_loss         | 0.012370775   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026776649  |\n",
            "| clipfrac           | 0.023803711   |\n",
            "| ep_len_mean        | 69.2          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 9149          |\n",
            "| n_updates          | 226           |\n",
            "| policy_entropy     | 0.94678974    |\n",
            "| policy_loss        | -0.0008429637 |\n",
            "| serial_timesteps   | 57856         |\n",
            "| time_elapsed       | 163           |\n",
            "| total_timesteps    | 925696        |\n",
            "| value_loss         | 0.0017217437  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0016225511   |\n",
            "| clipfrac           | 0.012329102    |\n",
            "| ep_len_mean        | 68.3           |\n",
            "| ep_reward_mean     | 93.5           |\n",
            "| explained_variance | 0.996          |\n",
            "| fps                | 8839           |\n",
            "| n_updates          | 227            |\n",
            "| policy_entropy     | 0.94309163     |\n",
            "| policy_loss        | -0.00061969255 |\n",
            "| serial_timesteps   | 58112          |\n",
            "| time_elapsed       | 164            |\n",
            "| total_timesteps    | 929792         |\n",
            "| value_loss         | 0.0041111284   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=93.41 +/- 0.05\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0040247836  |\n",
            "| clipfrac           | 0.045715332   |\n",
            "| ep_len_mean        | 68.8          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.968         |\n",
            "| fps                | 5055          |\n",
            "| n_updates          | 228           |\n",
            "| policy_entropy     | 0.9406805     |\n",
            "| policy_loss        | -0.0018935627 |\n",
            "| serial_timesteps   | 58368         |\n",
            "| time_elapsed       | 164           |\n",
            "| total_timesteps    | 933888        |\n",
            "| value_loss         | 0.033924233   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019905309  |\n",
            "| clipfrac           | 0.016052246   |\n",
            "| ep_len_mean        | 72.3          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.941         |\n",
            "| fps                | 8814          |\n",
            "| n_updates          | 229           |\n",
            "| policy_entropy     | 0.9416084     |\n",
            "| policy_loss        | -0.0009846184 |\n",
            "| serial_timesteps   | 58624         |\n",
            "| time_elapsed       | 165           |\n",
            "| total_timesteps    | 937984        |\n",
            "| value_loss         | 0.06237421    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=93.49 +/- 0.08\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00473955    |\n",
            "| clipfrac           | 0.057800293   |\n",
            "| ep_len_mean        | 72.8          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.951         |\n",
            "| fps                | 5053          |\n",
            "| n_updates          | 230           |\n",
            "| policy_entropy     | 0.94092405    |\n",
            "| policy_loss        | -0.0005160108 |\n",
            "| serial_timesteps   | 58880         |\n",
            "| time_elapsed       | 165           |\n",
            "| total_timesteps    | 942080        |\n",
            "| value_loss         | 0.05444293    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026493922  |\n",
            "| clipfrac           | 0.024902344   |\n",
            "| ep_len_mean        | 71            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 8854          |\n",
            "| n_updates          | 231           |\n",
            "| policy_entropy     | 0.9391272     |\n",
            "| policy_loss        | -0.0010599977 |\n",
            "| serial_timesteps   | 59136         |\n",
            "| time_elapsed       | 166           |\n",
            "| total_timesteps    | 946176        |\n",
            "| value_loss         | 0.01139138    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=93.46 +/- 0.01\n",
            "Episode length: 68.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.001098187  |\n",
            "| clipfrac           | 0.0040283203 |\n",
            "| ep_len_mean        | 68.8         |\n",
            "| ep_reward_mean     | 93.5         |\n",
            "| explained_variance | 0.996        |\n",
            "| fps                | 5189         |\n",
            "| n_updates          | 232          |\n",
            "| policy_entropy     | 0.9382913    |\n",
            "| policy_loss        | 0.0005090714 |\n",
            "| serial_timesteps   | 59392        |\n",
            "| time_elapsed       | 167          |\n",
            "| total_timesteps    | 950272       |\n",
            "| value_loss         | 0.003047104  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0025581333  |\n",
            "| clipfrac           | 0.025146484   |\n",
            "| ep_len_mean        | 68.6          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 9072          |\n",
            "| n_updates          | 233           |\n",
            "| policy_entropy     | 0.9378085     |\n",
            "| policy_loss        | -0.0020752253 |\n",
            "| serial_timesteps   | 59648         |\n",
            "| time_elapsed       | 167           |\n",
            "| total_timesteps    | 954368        |\n",
            "| value_loss         | 0.0019291929  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00021695526 |\n",
            "| clipfrac           | 0.00030517578 |\n",
            "| ep_len_mean        | 69.3          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 8386          |\n",
            "| n_updates          | 234           |\n",
            "| policy_entropy     | 0.9394726     |\n",
            "| policy_loss        | 0.00015688455 |\n",
            "| serial_timesteps   | 59904         |\n",
            "| time_elapsed       | 168           |\n",
            "| total_timesteps    | 958464        |\n",
            "| value_loss         | 0.019921744   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=93.44 +/- 0.06\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010163028  |\n",
            "| clipfrac           | 0.0045776367  |\n",
            "| ep_len_mean        | 68.9          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 5033          |\n",
            "| n_updates          | 235           |\n",
            "| policy_entropy     | 0.937932      |\n",
            "| policy_loss        | -0.0015034092 |\n",
            "| serial_timesteps   | 60160         |\n",
            "| time_elapsed       | 168           |\n",
            "| total_timesteps    | 962560        |\n",
            "| value_loss         | 0.001668208   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014735422  |\n",
            "| clipfrac           | 0.01171875    |\n",
            "| ep_len_mean        | 68.2          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 8811          |\n",
            "| n_updates          | 236           |\n",
            "| policy_entropy     | 0.93435633    |\n",
            "| policy_loss        | -0.0021876078 |\n",
            "| serial_timesteps   | 60416         |\n",
            "| time_elapsed       | 169           |\n",
            "| total_timesteps    | 966656        |\n",
            "| value_loss         | 0.0015150595  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=93.43 +/- 0.04\n",
            "Episode length: 68.20 +/- 0.40\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0005156223   |\n",
            "| clipfrac           | 0.0018310547   |\n",
            "| ep_len_mean        | 69.2           |\n",
            "| ep_reward_mean     | 93.4           |\n",
            "| explained_variance | 0.981          |\n",
            "| fps                | 5171           |\n",
            "| n_updates          | 237            |\n",
            "| policy_entropy     | 0.9325632      |\n",
            "| policy_loss        | -1.8896098e-05 |\n",
            "| serial_timesteps   | 60672          |\n",
            "| time_elapsed       | 170            |\n",
            "| total_timesteps    | 970752         |\n",
            "| value_loss         | 0.019236406    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0026995216   |\n",
            "| clipfrac           | 0.030883789    |\n",
            "| ep_len_mean        | 69.6           |\n",
            "| ep_reward_mean     | 93.4           |\n",
            "| explained_variance | 0.984          |\n",
            "| fps                | 8694           |\n",
            "| n_updates          | 238            |\n",
            "| policy_entropy     | 0.93181        |\n",
            "| policy_loss        | -0.00050350634 |\n",
            "| serial_timesteps   | 60928          |\n",
            "| time_elapsed       | 170            |\n",
            "| total_timesteps    | 974848         |\n",
            "| value_loss         | 0.01767097     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.001814493  |\n",
            "| clipfrac           | 0.012207031  |\n",
            "| ep_len_mean        | 68.8         |\n",
            "| ep_reward_mean     | 93.4         |\n",
            "| explained_variance | 0.998        |\n",
            "| fps                | 8698         |\n",
            "| n_updates          | 239          |\n",
            "| policy_entropy     | 0.9295953    |\n",
            "| policy_loss        | -0.001416875 |\n",
            "| serial_timesteps   | 61184        |\n",
            "| time_elapsed       | 171          |\n",
            "| total_timesteps    | 978944       |\n",
            "| value_loss         | 0.0014367546 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=93.44 +/- 0.05\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002081736   |\n",
            "| clipfrac           | 0.018493652   |\n",
            "| ep_len_mean        | 68.1          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5055          |\n",
            "| n_updates          | 240           |\n",
            "| policy_entropy     | 0.9240358     |\n",
            "| policy_loss        | -0.0010181455 |\n",
            "| serial_timesteps   | 61440         |\n",
            "| time_elapsed       | 171           |\n",
            "| total_timesteps    | 983040        |\n",
            "| value_loss         | 0.0038031547  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0032422943  |\n",
            "| clipfrac           | 0.034484863   |\n",
            "| ep_len_mean        | 68            |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 8972          |\n",
            "| n_updates          | 241           |\n",
            "| policy_entropy     | 0.92075574    |\n",
            "| policy_loss        | -0.0028878702 |\n",
            "| serial_timesteps   | 61696         |\n",
            "| time_elapsed       | 172           |\n",
            "| total_timesteps    | 987136        |\n",
            "| value_loss         | 0.0008664542  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=93.46 +/- 0.01\n",
            "Episode length: 68.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0038118686 |\n",
            "| clipfrac           | 0.04260254   |\n",
            "| ep_len_mean        | 67.8         |\n",
            "| ep_reward_mean     | 93.5         |\n",
            "| explained_variance | 0.999        |\n",
            "| fps                | 5134         |\n",
            "| n_updates          | 242          |\n",
            "| policy_entropy     | 0.9194388    |\n",
            "| policy_loss        | -0.002865188 |\n",
            "| serial_timesteps   | 61952        |\n",
            "| time_elapsed       | 173          |\n",
            "| total_timesteps    | 991232       |\n",
            "| value_loss         | 0.000955056  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0035013508  |\n",
            "| clipfrac           | 0.035888672   |\n",
            "| ep_len_mean        | 67.7          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 8918          |\n",
            "| n_updates          | 243           |\n",
            "| policy_entropy     | 0.9188707     |\n",
            "| policy_loss        | -0.0024517637 |\n",
            "| serial_timesteps   | 62208         |\n",
            "| time_elapsed       | 173           |\n",
            "| total_timesteps    | 995328        |\n",
            "| value_loss         | 0.00085769873 |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0021224113 |\n",
            "| clipfrac           | 0.020080566  |\n",
            "| ep_len_mean        | 67.9         |\n",
            "| ep_reward_mean     | 93.5         |\n",
            "| explained_variance | 0.999        |\n",
            "| fps                | 8873         |\n",
            "| n_updates          | 244          |\n",
            "| policy_entropy     | 0.9146245    |\n",
            "| policy_loss        | -0.002179942 |\n",
            "| serial_timesteps   | 62464        |\n",
            "| time_elapsed       | 174          |\n",
            "| total_timesteps    | 999424       |\n",
            "| value_loss         | 0.0010709271 |\n",
            "-------------------------------------\n",
            "Saving to logs/ppo2/MountainCarContinuous-v0_1\n",
            "[77e9fbce07ac:03032] *** Process received signal ***\n",
            "[77e9fbce07ac:03032] Signal: Segmentation fault (11)\n",
            "[77e9fbce07ac:03032] Signal code: Address not mapped (1)\n",
            "[77e9fbce07ac:03032] Failing at address: 0x7f1d1e5a820d\n",
            "[77e9fbce07ac:03032] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f1d21253980]\n",
            "[77e9fbce07ac:03032] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f1d20e928a5]\n",
            "[77e9fbce07ac:03032] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f1d216fde44]\n",
            "[77e9fbce07ac:03032] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f1d20e93735]\n",
            "[77e9fbce07ac:03032] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f1d216fbcb3]\n",
            "[77e9fbce07ac:03032] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3OYGGy_QxBM",
        "outputId": "0866dc1c-0988-4a16-ea84-62ce66e915e1"
      },
      "source": [
        "# Overwritting the baseline timesteps model\n",
        "\n",
        "!python train.py --algo ppo2 --env MountainCarContinuous-v0 --n-timesteps 10000 #--save-freq 1000\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MountainCarContinuous-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.94),\n",
            "             ('n_envs', 16),\n",
            "             ('n_steps', 256),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 8),\n",
            "             ('noptepochs', 4),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=10000\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MountainCarContinuous-v0_2\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00072787184 |\n",
            "| clipfrac           | 0.0022583008  |\n",
            "| explained_variance | -0.0333       |\n",
            "| fps                | 6006          |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 1.4160135     |\n",
            "| policy_loss        | -0.0010549161 |\n",
            "| serial_timesteps   | 256           |\n",
            "| time_elapsed       | 2.22e-05      |\n",
            "| total_timesteps    | 4096          |\n",
            "| value_loss         | 0.5902689     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.000113212736 |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | -0.104         |\n",
            "| fps                | 9022           |\n",
            "| n_updates          | 2              |\n",
            "| policy_entropy     | 1.4079745      |\n",
            "| policy_loss        | -0.0010972274  |\n",
            "| serial_timesteps   | 512            |\n",
            "| time_elapsed       | 0.682          |\n",
            "| total_timesteps    | 8192           |\n",
            "| value_loss         | 0.1042344      |\n",
            "---------------------------------------\n",
            "Saving to logs/ppo2/MountainCarContinuous-v0_2\n",
            "[77e9fbce07ac:03075] *** Process received signal ***\n",
            "[77e9fbce07ac:03075] Signal: Segmentation fault (11)\n",
            "[77e9fbce07ac:03075] Signal code: Address not mapped (1)\n",
            "[77e9fbce07ac:03075] Failing at address: 0x7f21ab9b420d\n",
            "[77e9fbce07ac:03075] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f21ae65f980]\n",
            "[77e9fbce07ac:03075] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f21ae29e8a5]\n",
            "[77e9fbce07ac:03075] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f21aeb09e44]\n",
            "[77e9fbce07ac:03075] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f21ae29f735]\n",
            "[77e9fbce07ac:03075] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f21aeb07cb3]\n",
            "[77e9fbce07ac:03075] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5om6tse4YPBB"
      },
      "source": [
        "## Optimizing search of parameters with Optuna\n",
        "\n",
        "  - link of optuna: Used for changing hyperparameter (looking for the best so it optimizes a search)\n",
        "  - https://colab.research.google.com/github/optuna/optuna/blob/master/examples/quickstart.ipynb\n",
        "  - Research related to the algorithm Optuna (https://arxiv.org/pdf/1907.10902.pdf)\n",
        "  - That is why we add an hyperparameter optimizer search (up to the search of 10 parameters based on a n (=300, in our exercise) sample trial\n",
        "  - There are many selection of hyperparameters that can be chosen, but in terms of cost-effectiveness some could be skipped (or prunned)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_9Qyyalz-Xm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319e126c-2a7b-49da-e686-5a222eb12189"
      },
      "source": [
        "#%tensorboard --logdir=tensorboard\n",
        "!python train.py --algo ppo2 --env MountainCarContinuous-v0 -n 50000 -optimize --n-trials 300 --n-jobs 2 --sampler tpe --pruner median\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MountainCarContinuous-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.94),\n",
            "             ('n_envs', 16),\n",
            "             ('n_steps', 256),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 8),\n",
            "             ('noptepochs', 4),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=50000\n",
            "Normalizing input and reward\n",
            "Optimizing hyperparameters\n",
            "Sampler: tpe - Pruner: median\n",
            "\u001b[32m[I 2021-03-19 14:44:52,481]\u001b[0m A new study created in memory with name: no-name-35872e91-0846-4263-8b8e-ea341e796f03\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalizing input and reward\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:47:53,176]\u001b[0m Trial 0 finished with value: 2.479185968695674e-05 and parameters: {'batch_size': 64, 'n_steps': 2048, 'gamma': 0.99, 'lr': 3.9593049302166474e-05, 'ent_coef': 7.683809505828654e-06, 'cliprange': 0.1, 'noptepochs': 50, 'lambda': 0.9}. Best is trial 0 with value: 2.479185968695674e-05.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:48:46,101]\u001b[0m Trial 1 finished with value: 0.03835057467222214 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.0001957132697680036, 'ent_coef': 0.00014792911172195836, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 0 with value: 2.479185968695674e-05.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:51:36,591]\u001b[0m Trial 3 finished with value: 1.0547242709435523e-05 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.9, 'lr': 0.0017630805649951789, 'ent_coef': 0.00011755942144170166, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 0.98}. Best is trial 3 with value: 1.0547242709435523e-05.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:51:46,629]\u001b[0m Trial 2 finished with value: -98.11212158203125 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00040439804087355876, 'ent_coef': 5.3070689857745075e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:55:07,731]\u001b[0m Trial 4 finished with value: 99.8990478515625 and parameters: {'batch_size': 64, 'n_steps': 16, 'gamma': 0.9, 'lr': 0.19676640808550347, 'ent_coef': 6.78231367357659e-05, 'cliprange': 0.4, 'noptepochs': 50, 'lambda': 0.98}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:55:36,303]\u001b[0m Trial 5 finished with value: -97.49061584472656 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00019978716190919521, 'ent_coef': 0.029631005344805793, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:58:07,474]\u001b[0m Trial 6 finished with value: 9.76723458734341e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.999, 'lr': 0.36373600104557074, 'ent_coef': 0.0004260848102998029, 'cliprange': 0.1, 'noptepochs': 50, 'lambda': 0.95}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:58:20,786]\u001b[0m Trial 7 finished with value: 95.83981323242188 and parameters: {'batch_size': 256, 'n_steps': 32, 'gamma': 0.98, 'lr': 0.008036823471884707, 'ent_coef': 0.0026906441138624237, 'cliprange': 0.1, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:02:16,722]\u001b[0m Trial 8 finished with value: 0.12172162532806396 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.002701664157747044, 'ent_coef': 1.668574645875183e-08, 'cliprange': 0.1, 'noptepochs': 10, 'lambda': 0.9}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:02:25,865]\u001b[0m Trial 9 finished with value: 0.03436021879315376 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.01962178667848197, 'ent_coef': 1.1704551145623017e-07, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:03:36,763]\u001b[0m Trial 10 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:06:30,830]\u001b[0m Trial 11 finished with value: 0.0007682001451030374 and parameters: {'batch_size': 256, 'n_steps': 512, 'gamma': 0.995, 'lr': 1.2208229413789481e-05, 'ent_coef': 2.6808620874762475e-06, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 0.8}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:07:50,940]\u001b[0m Trial 12 finished with value: 0.0005766440299339592 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'lr': 1.8024964641995533e-05, 'ent_coef': 0.08869037731058829, 'cliprange': 0.3, 'noptepochs': 30, 'lambda': 0.95}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:10:35,897]\u001b[0m Trial 13 finished with value: 0.04034343734383583 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00012726599674660344, 'ent_coef': 0.08421016257647383, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:11:55,841]\u001b[0m Trial 14 finished with value: 0.744507908821106 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.995, 'lr': 0.0001671345227695574, 'ent_coef': 0.09503632408427945, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:13:52,888]\u001b[0m Trial 16 finished with value: -95.39118957519531 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.0005993930218643873, 'ent_coef': 1.2218505694336263e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:14:39,345]\u001b[0m Trial 15 finished with value: 1.2783794403076172 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.995, 'lr': 0.000998076351492054, 'ent_coef': 9.212797069935619e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:15:59,469]\u001b[0m Trial 18 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:17:20,656]\u001b[0m Trial 19 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:17:56,576]\u001b[0m Trial 17 finished with value: 0.0012737723300233483 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.995, 'lr': 4.1827517947982434e-05, 'ent_coef': 0.007927275972781407, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:21:00,404]\u001b[0m Trial 21 finished with value: -95.18550109863281 and parameters: {'batch_size': 256, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.005519403617255943, 'ent_coef': 1.8533561828192526e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:21:23,465]\u001b[0m Trial 20 finished with value: 0.002464982680976391 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.999, 'lr': 5.558849471198219e-05, 'ent_coef': 0.009850370080090572, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:22:17,384]\u001b[0m Trial 22 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:23:24,994]\u001b[0m Trial 23 finished with value: -95.29508972167969 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.0006347455247162761, 'ent_coef': 1.182866789350006e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:24:44,591]\u001b[0m Trial 24 finished with value: -96.8970947265625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004564986881048647, 'ent_coef': 1.812458385102629e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:25:31,867]\u001b[0m Trial 25 finished with value: -94.97367858886719 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003328959613032049, 'ent_coef': 1.720807405967486e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:27:44,401]\u001b[0m Trial 26 finished with value: -95.3174057006836 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00022276346517513326, 'ent_coef': 2.2254358636035424e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:28:26,264]\u001b[0m Trial 27 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:30:34,326]\u001b[0m Trial 28 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:32:04,037]\u001b[0m Trial 29 finished with value: -95.28102111816406 and parameters: {'batch_size': 256, 'n_steps': 512, 'gamma': 0.99, 'lr': 0.001425974887417801, 'ent_coef': 4.309040123877741e-06, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 0.9}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:34:39,203]\u001b[0m Trial 30 finished with value: 0.2726535201072693 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.99, 'lr': 0.0016392861681431266, 'ent_coef': 4.896998059245534e-06, 'cliprange': 0.4, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:35:57,868]\u001b[0m Trial 32 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:36:03,726]\u001b[0m Trial 31 finished with value: 2.4860361008904874e-05 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.995, 'lr': 1.6381904707237502e-05, 'ent_coef': 6.771572731064265e-06, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:39:58,591]\u001b[0m Trial 34 finished with value: -98.6810073852539 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00025941891930756966, 'ent_coef': 2.101018470568001e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:40:02,722]\u001b[0m Trial 33 finished with value: 0.0030926072504371405 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 2.075612061841899e-05, 'ent_coef': 4.7386229235527475e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:41:55,089]\u001b[0m Trial 36 finished with value: -94.9090805053711 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.00023463422145841254, 'ent_coef': 0.0001704666238849975, 'cliprange': 0.4, 'noptepochs': 50, 'lambda': 0.98}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:44:16,286]\u001b[0m Trial 35 finished with value: 0.044005561619997025 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00025388071016347, 'ent_coef': 6.120748242000219e-05, 'cliprange': 0.2, 'noptepochs': 50, 'lambda': 0.98}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:46:02,561]\u001b[0m Trial 37 finished with value: 0.02585427835583687 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0001490068142386659, 'ent_coef': 6.627490347551006e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:47:03,325]\u001b[0m Trial 38 finished with value: 8.69665473146597e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.98, 'lr': 0.0033691979723057775, 'ent_coef': 2.3396231896736218e-06, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:50:09,723]\u001b[0m Trial 39 finished with value: 0.005883403588086367 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.004564839493630004, 'ent_coef': 2.5071691404528494e-06, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 0.8}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:51:11,292]\u001b[0m Trial 40 finished with value: 0.0005618958966806531 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.002987347206360759, 'ent_coef': 3.337041490327049e-07, 'cliprange': 0.1, 'noptepochs': 10, 'lambda': 0.8}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:51:29,770]\u001b[0m Trial 41 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:55:16,942]\u001b[0m Trial 43 finished with value: 0.016636978834867477 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.0008391796405408698, 'ent_coef': 1.2775984345870495e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:55:24,077]\u001b[0m Trial 42 finished with value: 0.05889545753598213 and parameters: {'batch_size': 32, 'n_steps': 16, 'gamma': 0.95, 'lr': 0.0010358302799514864, 'ent_coef': 1.0664688005798827e-08, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:57:58,962]\u001b[0m Trial 44 finished with value: 8.851862730807625e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.00039362196219130686, 'ent_coef': 2.820888376344373e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:58:12,264]\u001b[0m Trial 45 finished with value: 7.952169653435703e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.0003999640915821973, 'ent_coef': 0.0002191103329495375, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:02:03,697]\u001b[0m Trial 46 finished with value: 1.3915449380874634 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00012595725820717325, 'ent_coef': 1.5030744355756743e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:02:15,276]\u001b[0m Trial 47 finished with value: 0.014185783453285694 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.95, 'lr': 8.520663815158874e-05, 'ent_coef': 4.8461906732719754e-08, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:06:01,885]\u001b[0m Trial 48 finished with value: 0.0038488120771944523 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 8.471115653572753e-05, 'ent_coef': 3.573331899134388e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:06:19,808]\u001b[0m Trial 49 finished with value: 0.007713118102401495 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.995, 'lr': 3.2778023307709526e-05, 'ent_coef': 4.917060413551364e-07, 'cliprange': 0.2, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:07:22,060]\u001b[0m Trial 50 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:07:38,819]\u001b[0m Trial 51 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:11:24,269]\u001b[0m Trial 52 finished with value: 0.02836737595498562 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00018023916274965065, 'ent_coef': 2.753774667804948e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "\u001b[32m[I 2021-03-19 16:11:26,225]\u001b[0m Trial 53 finished with value: -97.8765869140625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00022734878212204266, 'ent_coef': 2.613796803270577e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:12:45,195]\u001b[0m Trial 55 finished with value: -92.1409912109375 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.00043136104621912147, 'ent_coef': 3.2603797929573756e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:14:26,138]\u001b[0m Trial 54 finished with value: -95.23594665527344 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00028952818654938087, 'ent_coef': 1.6902119404432205e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:16:51,152]\u001b[0m Trial 56 finished with value: 0.04701885208487511 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0009626647519757059, 'ent_coef': 1.5481176011066292e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:17:21,083]\u001b[0m Trial 57 finished with value: -78.0025863647461 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.0009297391821852721, 'ent_coef': 0.03438936511680032, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:18:10,912]\u001b[0m Trial 58 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:18:43,128]\u001b[0m Trial 59 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:20:06,044]\u001b[0m Trial 60 finished with value: -45.24247360229492 and parameters: {'batch_size': 256, 'n_steps': 128, 'gamma': 0.9999, 'lr': 0.00012443600940553948, 'ent_coef': 7.617544914943972e-07, 'cliprange': 0.4, 'noptepochs': 50, 'lambda': 0.9}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:22:50,991]\u001b[0m Trial 61 finished with value: 99.69564056396484 and parameters: {'batch_size': 256, 'n_steps': 1024, 'gamma': 0.995, 'lr': 0.8005665430532578, 'ent_coef': 1.3598447335814073e-06, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:24:08,602]\u001b[0m Trial 62 finished with value: 0.09300033748149872 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00020499845516410328, 'ent_coef': 2.515117728050503e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:26:26,913]\u001b[0m Trial 63 finished with value: -96.49452209472656 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00020506935512446304, 'ent_coef': 3.5808102600885877e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:28:14,811]\u001b[0m Trial 64 finished with value: 0.0912131518125534 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00023405275406194835, 'ent_coef': 0.0003275424173782244, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:30:26,742]\u001b[0m Trial 65 finished with value: -98.85881042480469 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00031837946717356387, 'ent_coef': 0.0019182523173234608, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 65 with value: -98.85881042480469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:32:18,128]\u001b[0m Trial 66 finished with value: 0.00020616501569747925 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 5.701832773943125e-05, 'ent_coef': 4.265423110886822e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 65 with value: -98.85881042480469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:34:31,183]\u001b[0m Trial 67 finished with value: 0.0013460059417411685 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 5.3024378165932406e-05, 'ent_coef': 0.002106806743224379, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 65 with value: -98.85881042480469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:36:17,269]\u001b[0m Trial 68 finished with value: -98.95207214355469 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00011089599119554573, 'ent_coef': 0.026229328836409407, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:38:38,824]\u001b[0m Trial 69 finished with value: 0.08180613815784454 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.00011074606759529507, 'ent_coef': 0.014325438686161611, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:40:23,965]\u001b[0m Trial 70 finished with value: 0.011047503910958767 and parameters: {'batch_size': 128, 'n_steps': 512, 'gamma': 0.99, 'lr': 0.00010596227879046445, 'ent_coef': 0.02263139206756122, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:42:44,570]\u001b[0m Trial 71 finished with value: 0.0015987928491085768 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.99, 'lr': 3.229859936361967e-05, 'ent_coef': 0.05361968729130006, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:44:31,324]\u001b[0m Trial 72 finished with value: 0.1176014319062233 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 3.6728581507019164e-05, 'ent_coef': 0.06412908987718782, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:44:43,591]\u001b[0m Trial 73 finished with value: -95.61804962158203 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00032462206358342444, 'ent_coef': 0.00540190931401892, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:48:34,349]\u001b[0m Trial 74 finished with value: 0.15825067460536957 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.00016505119025037214, 'ent_coef': 0.004325363124766509, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:48:46,523]\u001b[0m Trial 75 finished with value: 0.0070901187136769295 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.0001575559969698096, 'ent_coef': 0.0023734833925877336, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:52:34,620]\u001b[0m Trial 76 finished with value: 0.31205815076828003 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004957594480836681, 'ent_coef': 4.385408614042954e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "\u001b[32m[I 2021-03-19 16:52:36,437]\u001b[0m Trial 77 finished with value: -98.65316009521484 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0005034808111289083, 'ent_coef': 0.000944050131903079, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:54:49,647]\u001b[0m Trial 79 finished with value: 29.662200927734375 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.995, 'lr': 0.0013047004456248623, 'ent_coef': 0.0006622121630153109, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 0.92}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:56:38,796]\u001b[0m Trial 78 finished with value: 0.005808723159134388 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.995, 'lr': 6.604901529055241e-05, 'ent_coef': 8.736997813751239e-05, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:58:53,419]\u001b[0m Trial 80 finished with value: 0.019164210185408592 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00031061152333297236, 'ent_coef': 0.010746314558399568, 'cliprange': 0.3, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:00:13,408]\u001b[0m Trial 82 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:00:41,343]\u001b[0m Trial 81 finished with value: 0.0012116010766476393 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0003290036650880054, 'ent_coef': 0.0013161896757492549, 'cliprange': 0.3, 'noptepochs': 1, 'lambda': 0.95}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:02:07,476]\u001b[0m Trial 83 finished with value: -94.64328002929688 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00044736254595484755, 'ent_coef': 0.0011442538972463287, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:03:27,015]\u001b[0m Trial 85 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:03:44,294]\u001b[0m Trial 84 finished with value: -96.91104888916016 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004884728399410808, 'ent_coef': 1.1305356824496258e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:07:33,431]\u001b[0m Trial 86 finished with value: 0.024003325030207634 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000740813344048939, 'ent_coef': 0.00027114825912358183, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 0.99}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:07:51,473]\u001b[0m Trial 87 finished with value: 0.03893955796957016 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0007627105822844693, 'ent_coef': 0.022720034757575627, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.99}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:10:37,027]\u001b[0m Trial 88 finished with value: -96.97540283203125 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005240525195269763, 'ent_coef': 1.7958154610311793e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:11:54,569]\u001b[0m Trial 89 finished with value: 0.014172373339533806 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00047460076517143196, 'ent_coef': 6.947072109224456e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:13:12,982]\u001b[0m Trial 91 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:14:18,455]\u001b[0m Trial 90 finished with value: -98.36399841308594 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0011702084405374696, 'ent_coef': 1.9116191744138204e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:17:16,545]\u001b[0m Trial 92 finished with value: 0.0005861685494892299 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0011736232770393309, 'ent_coef': 2.191941662821961e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:18:20,639]\u001b[0m Trial 93 finished with value: 0.027003884315490723 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0005706058479969515, 'ent_coef': 2.3574909628477325e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:21:17,805]\u001b[0m Trial 95 finished with value: -96.81341552734375 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0014212863676067438, 'ent_coef': 4.068789794100275e-06, 'cliprange': 0.3, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "\u001b[32m[I 2021-03-19 17:21:19,293]\u001b[0m Trial 94 finished with value: 0.12542104721069336 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0005840385973188033, 'ent_coef': 8.441060649500968e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:23:57,902]\u001b[0m Trial 96 finished with value: 9.36886681301985e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.98, 'lr': 0.0023366124388174508, 'ent_coef': 1.0070348663611673e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:24:06,763]\u001b[0m Trial 97 finished with value: 7.351063686655834e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.995, 'lr': 0.0021734645329031686, 'ent_coef': 1.5585092800434085e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:27:57,864]\u001b[0m Trial 98 finished with value: 0.6697169542312622 and parameters: {'batch_size': 128, 'n_steps': 1024, 'gamma': 0.995, 'lr': 0.0003510165004648907, 'ent_coef': 5.315015172431423e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:28:05,909]\u001b[0m Trial 99 finished with value: 0.0416058786213398 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.95, 'lr': 0.00037401123284118285, 'ent_coef': 5.893179466781584e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:29:48,312]\u001b[0m Trial 101 finished with value: -93.72032165527344 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00025539665438072324, 'ent_coef': 6.081252153332487e-07, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:32:00,055]\u001b[0m Trial 100 finished with value: 1.2740747928619385 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.00024092089334396073, 'ent_coef': 5.785412840826238e-07, 'cliprange': 0.3, 'noptepochs': 10, 'lambda': 0.8}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:33:32,633]\u001b[0m Trial 102 finished with value: -98.33882904052734 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00013788519081619764, 'ent_coef': 8.607464393694706e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:36:03,741]\u001b[0m Trial 103 finished with value: 0.02444997802376747 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0008692032564472196, 'ent_coef': 1.8460910488802472e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:37:36,174]\u001b[0m Trial 104 finished with value: 0.003340085269883275 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00010365262155953773, 'ent_coef': 5.6167253478797115e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:40:05,955]\u001b[0m Trial 105 finished with value: 0.01742219179868698 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0001385022877617096, 'ent_coef': 8.313161204377711e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:41:36,463]\u001b[0m Trial 106 finished with value: 0.019930610433220863 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000122504236725259, 'ent_coef': 9.635756005054481e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:44:00,509]\u001b[0m Trial 107 finished with value: 0.604324460029602 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002075692726143082, 'ent_coef': 0.0069741003924048035, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:45:28,972]\u001b[0m Trial 109 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:46:16,801]\u001b[0m Trial 108 finished with value: 0.016669128090143204 and parameters: {'batch_size': 256, 'n_steps': 16, 'gamma': 0.95, 'lr': 0.0001894484733580404, 'ent_coef': 1.372374204582051e-05, 'cliprange': 0.3, 'noptepochs': 50, 'lambda': 0.9}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:46:45,727]\u001b[0m Trial 110 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:50:12,768]\u001b[0m Trial 111 finished with value: 0.011747448705136776 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.9999, 'lr': 6.885063351356414e-05, 'ent_coef': 0.034276301913491874, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:50:46,220]\u001b[0m Trial 112 finished with value: -78.62692260742188 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003985116132021036, 'ent_coef': 3.2992715882681073e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:54:10,417]\u001b[0m Trial 113 finished with value: 0.03707515820860863 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002827915796190683, 'ent_coef': 3.1281315862828304e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:54:40,376]\u001b[0m Trial 114 finished with value: 0.00780072808265686 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003120158881785625, 'ent_coef': 2.2002760593106614e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:55:49,629]\u001b[0m Trial 116 finished with value: -92.72662353515625 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.99, 'lr': 0.001001119463229993, 'ent_coef': 1.8018831826010542e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:58:06,726]\u001b[0m Trial 115 finished with value: -99.14582824707031 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006213010340576837, 'ent_coef': 0.0004927687872681538, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:59:50,528]\u001b[0m Trial 117 finished with value: 0.0030353600159287453 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005292176396237569, 'ent_coef': 0.0008958214593089361, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:02:08,797]\u001b[0m Trial 118 finished with value: 0.011217966675758362 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0016820701650059643, 'ent_coef': 0.00042347006673380317, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:02:49,607]\u001b[0m Trial 119 finished with value: -95.38313293457031 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006906538481612326, 'ent_coef': 0.0005081844772325735, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:06:11,980]\u001b[0m Trial 120 finished with value: 0.4013548493385315 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.00014814799219546848, 'ent_coef': 0.001833963717826568, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:06:54,506]\u001b[0m Trial 121 finished with value: 0.011465411633253098 and parameters: {'batch_size': 128, 'n_steps': 512, 'gamma': 0.9, 'lr': 0.0001571526316764838, 'ent_coef': 0.0038409554569637954, 'cliprange': 0.3, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:09:33,222]\u001b[0m Trial 122 finished with value: -97.9125747680664 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00040362267740253583, 'ent_coef': 1.3528773359994301e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:10:30,264]\u001b[0m Trial 123 finished with value: -98.53755950927734 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0007963618241188463, 'ent_coef': 9.94182835571783e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:11:23,511]\u001b[0m Trial 124 finished with value: -95.31433868408203 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003828225095518836, 'ent_coef': 1.0908666215116572e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:14:36,321]\u001b[0m Trial 125 finished with value: 0.01329109352082014 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.001197013744654082, 'ent_coef': 0.0008672998342964575, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:15:23,928]\u001b[0m Trial 126 finished with value: 0.02515149675309658 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.001202134448905802, 'ent_coef': 3.386110153240021e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:18:30,198]\u001b[0m Trial 127 finished with value: -98.9217529296875 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0008075976537026159, 'ent_coef': 2.989250025661834e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:18:55,614]\u001b[0m Trial 128 finished with value: -96.64897155761719 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0007996223418834765, 'ent_coef': 8.469874668684193e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:19:48,697]\u001b[0m Trial 129 finished with value: -93.34644317626953 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.995, 'lr': 0.003638857587582949, 'ent_coef': 0.0028377620273928477, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:21:10,315]\u001b[0m Trial 130 finished with value: -88.45052337646484 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.98, 'lr': 0.003820058787496653, 'ent_coef': 2.3458613170890472e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:22:32,442]\u001b[0m Trial 131 finished with value: -96.51373291015625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.00022862659328757147, 'ent_coef': 0.01493524381041262, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:24:26,203]\u001b[0m Trial 132 finished with value: -97.82203674316406 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006947475868921188, 'ent_coef': 5.358174017367642e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:25:17,978]\u001b[0m Trial 133 finished with value: -97.3680191040039 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006178437341391515, 'ent_coef': 1.4569245823312265e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:28:28,865]\u001b[0m Trial 134 finished with value: 0.033848777413368225 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006304583019429888, 'ent_coef': 0.00014578773199622853, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:29:19,186]\u001b[0m Trial 135 finished with value: 0.027132604271173477 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0008571252040553644, 'ent_coef': 4.93073710916008e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:32:28,123]\u001b[0m Trial 136 finished with value: 0.010343619622290134 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 9.104223898726212e-05, 'ent_coef': 4.9396842053332606e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:33:17,070]\u001b[0m Trial 137 finished with value: 0.0010548775317147374 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.999, 'lr': 9.263833095669304e-05, 'ent_coef': 2.6946058744460684e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:36:26,666]\u001b[0m Trial 138 finished with value: 0.008498079143464565 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.0003336641913281529, 'ent_coef': 6.67516899140063e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:37:15,738]\u001b[0m Trial 139 finished with value: 0.1534481793642044 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0015927440977004776, 'ent_coef': 7.746051400155031e-05, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:40:22,390]\u001b[0m Trial 140 finished with value: 0.20778241753578186 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0002622844228614189, 'ent_coef': 0.03662797290429042, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:40:57,506]\u001b[0m Trial 141 finished with value: -97.52226257324219 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002954944556834571, 'ent_coef': 0.0002268078388407226, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:44:21,868]\u001b[0m Trial 142 finished with value: 0.6043336987495422 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006083126159168913, 'ent_coef': 0.00027049297860154946, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:44:55,407]\u001b[0m Trial 143 finished with value: 0.11193034797906876 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004174336680023114, 'ent_coef': 0.00031665995267819177, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:47:36,124]\u001b[0m Trial 144 finished with value: -96.7803726196289 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00019372113165969204, 'ent_coef': 0.00041601580839405064, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.99}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:48:54,589]\u001b[0m Trial 145 finished with value: 0.22662410140037537 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00021076744823714553, 'ent_coef': 0.00011395861615088362, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:51:25,179]\u001b[0m Trial 146 finished with value: 0.8570221662521362 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00029579259096354824, 'ent_coef': 0.00011252928622650562, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:52:43,994]\u001b[0m Trial 147 finished with value: 0.586434543132782 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.0003111346227379503, 'ent_coef': 0.0005535157957510076, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:53:53,512]\u001b[0m Trial 148 finished with value: 99.8990478515625 and parameters: {'batch_size': 128, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.001065776941747656, 'ent_coef': 0.07793578052966883, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:55:10,180]\u001b[0m Trial 150 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:56:06,243]\u001b[0m Trial 149 finished with value: -83.8092269897461 and parameters: {'batch_size': 128, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0009864313265425559, 'ent_coef': 0.0002272245587885101, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:56:25,224]\u001b[0m Trial 151 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:59:30,429]\u001b[0m Trial 153 finished with value: -96.99665832519531 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006889647339907287, 'ent_coef': 3.3996836992392113e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:59:59,196]\u001b[0m Trial 152 finished with value: 0.0317843034863472 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0007776934046272647, 'ent_coef': 3.096894555112112e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:01:29,509]\u001b[0m Trial 154 finished with value: -96.03812408447266 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004858344056162611, 'ent_coef': 7.957070623525859e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:03:02,617]\u001b[0m Trial 155 finished with value: -98.10968017578125 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005083916094375768, 'ent_coef': 1.299290598777351e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:05:27,940]\u001b[0m Trial 156 finished with value: 0.001196318306028843 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.006604240872834396, 'ent_coef': 1.2146687830371255e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:05:46,528]\u001b[0m Trial 157 finished with value: -96.79540252685547 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00013423427686430462, 'ent_coef': 4.108016326355803e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:09:28,686]\u001b[0m Trial 158 finished with value: 0.019250649958848953 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003641224510731867, 'ent_coef': 1.975922393093177e-06, 'cliprange': 0.2, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:09:46,498]\u001b[0m Trial 159 finished with value: 0.0020615230314433575 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00041290793654571995, 'ent_coef': 2.0492112101256396e-06, 'cliprange': 0.2, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:12:08,245]\u001b[0m Trial 160 finished with value: 9.005111678561661e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.0002600240672812059, 'ent_coef': 2.2480872470760607e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:12:32,978]\u001b[0m Trial 161 finished with value: 8.392371455556713e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.00023922070247691082, 'ent_coef': 0.0010760593971258568, 'cliprange': 0.4, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:15:36,474]\u001b[0m Trial 163 finished with value: -96.5377197265625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000494367274231595, 'ent_coef': 1.4778449783056615e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:15:49,011]\u001b[0m Trial 162 finished with value: -98.4604263305664 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000579857887413539, 'ent_coef': 1.4857667662537572e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:18:48,243]\u001b[0m Trial 164 finished with value: -98.325927734375 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005172863686063753, 'ent_coef': 1.2178511902204818e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:19:38,820]\u001b[0m Trial 165 finished with value: 0.0007719395798631012 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005744926657357153, 'ent_coef': 9.440206948670885e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:22:42,371]\u001b[0m Trial 166 finished with value: 0.0009641318465583026 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005847541640340018, 'ent_coef': 1.1330332681645236e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:23:32,882]\u001b[0m Trial 167 finished with value: 0.007876078598201275 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0007450472806324615, 'ent_coef': 1.1603937885004847e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:26:34,574]\u001b[0m Trial 168 finished with value: 1.320749044418335 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0007153012424935536, 'ent_coef': 7.307605592292245e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:27:11,689]\u001b[0m Trial 169 finished with value: -98.58610534667969 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0014035679650430445, 'ent_coef': 2.891319781005806e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:30:23,328]\u001b[0m Trial 170 finished with value: 1.3421612977981567 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.00036400610964504597, 'ent_coef': 1.421787447185454e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:31:03,127]\u001b[0m Trial 171 finished with value: 0.01871425285935402 and parameters: {'batch_size': 128, 'n_steps': 512, 'gamma': 0.995, 'lr': 0.0017768917278253155, 'ent_coef': 5.2659024657063024e-06, 'cliprange': 0.2, 'noptepochs': 10, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:32:11,960]\u001b[0m Trial 172 finished with value: -95.25566101074219 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00144359928139906, 'ent_coef': 5.511817465428344e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:34:17,477]\u001b[0m Trial 174 finished with value: -95.44471740722656 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00108584024498652, 'ent_coef': 2.7939537179577804e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:34:53,259]\u001b[0m Trial 173 finished with value: 0.008315620943903923 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00130589938914484, 'ent_coef': 2.84966176032843e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:38:09,846]\u001b[0m Trial 175 finished with value: 0.015005646273493767 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0005369625541553908, 'ent_coef': 2.2358436615699695e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:38:47,863]\u001b[0m Trial 176 finished with value: 0.005355847999453545 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004882641961714151, 'ent_coef': 3.944483721339503e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:42:01,355]\u001b[0m Trial 177 finished with value: 0.03212428465485573 and parameters: {'batch_size': 256, 'n_steps': 1024, 'gamma': 0.95, 'lr': 0.0004543091673856789, 'ent_coef': 0.0006993869386026801, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:42:32,810]\u001b[0m Trial 178 finished with value: -98.4843978881836 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0009449605149443152, 'ent_coef': 3.492955724117443e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:44:19,476]\u001b[0m Trial 179 finished with value: -93.8016357421875 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0009028995144007762, 'ent_coef': 4.500455962442913e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:44:55,231]\u001b[0m Trial 180 finished with value: -95.75627136230469 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.002464745553469906, 'ent_coef': 1.3843580739994574e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:46:46,993]\u001b[0m Trial 181 finished with value: -96.01301574707031 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0007656616910553673, 'ent_coef': 7.601034440598044e-06, 'cliprange': 0.3, 'noptepochs': 50, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:48:26,569]\u001b[0m Trial 182 finished with value: -97.6046371459961 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0007566623950043742, 'ent_coef': 1.497471560911045e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:50:37,570]\u001b[0m Trial 183 finished with value: 0.07731831818819046 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0003017562488582733, 'ent_coef': 2.8137549852855313e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:52:12,429]\u001b[0m Trial 184 finished with value: 0.43261557817459106 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0010857013619621388, 'ent_coef': 1.58998439849309e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:54:27,271]\u001b[0m Trial 185 finished with value: -77.44641876220703 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0009517930796304999, 'ent_coef': 9.029950963184635e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:55:26,417]\u001b[0m Trial 186 finished with value: -97.4489517211914 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0006505421151642288, 'ent_coef': 8.148565546151035e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:58:16,297]\u001b[0m Trial 187 finished with value: 0.05917160585522652 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0012950736716433732, 'ent_coef': 6.647567692308778e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:59:13,941]\u001b[0m Trial 188 finished with value: 0.11283795535564423 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0013782154736386098, 'ent_coef': 3.5984017788804954e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:59:46,465]\u001b[0m Trial 189 finished with value: -94.7684097290039 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.000823246434872857, 'ent_coef': 2.0897233148451175e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:00:58,137]\u001b[0m Trial 190 finished with value: -94.19020080566406 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0008023444654877947, 'ent_coef': 1.0221409912008767e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:02:51,943]\u001b[0m Trial 191 finished with value: 99.8990478515625 and parameters: {'batch_size': 128, 'n_steps': 16, 'gamma': 0.95, 'lr': 0.0005804180665543353, 'ent_coef': 3.921236664013852e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.9}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:04:43,116]\u001b[0m Trial 192 finished with value: 0.04740958660840988 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000365541189145047, 'ent_coef': 4.1788055094343666e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.9}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:05:43,472]\u001b[0m Trial 193 finished with value: -97.18479919433594 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00041482010401417996, 'ent_coef': 6.296816869355058e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:08:33,816]\u001b[0m Trial 194 finished with value: 0.00025652878684923053 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00040144640303220263, 'ent_coef': 1.5456359047440048e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:08:51,756]\u001b[0m Trial 195 finished with value: -97.3554916381836 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00018486317947139034, 'ent_coef': 1.353097716614248e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:12:22,235]\u001b[0m Trial 196 finished with value: 0.4339753985404968 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0001735876106604233, 'ent_coef': 2.6835761321748723e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:12:40,733]\u001b[0m Trial 197 finished with value: 0.008797399699687958 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005794756205119045, 'ent_coef': 3.0066730279381136e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:14:57,182]\u001b[0m Trial 199 finished with value: -97.02223205566406 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.00026294616852319877, 'ent_coef': 1.838027168561182e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:14:59,997]\u001b[0m Trial 198 finished with value: -97.30162048339844 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006013774293913152, 'ent_coef': 2.3717955910257166e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:18:43,762]\u001b[0m Trial 200 finished with value: 0.0006996382726356387 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0019459052968261218, 'ent_coef': 1.1107150858402729e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:18:45,883]\u001b[0m Trial 201 finished with value: 0.02606309950351715 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.0003303250299449272, 'ent_coef': 0.00038779938466322795, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:19:56,933]\u001b[0m Trial 202 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:21:11,430]\u001b[0m Trial 204 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:22:34,592]\u001b[0m Trial 203 finished with value: 0.05973546952009201 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00012500205582749351, 'ent_coef': 0.019181387214913567, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:23:19,608]\u001b[0m Trial 205 finished with value: -95.22431945800781 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.00047465592759495347, 'ent_coef': 0.0018501056684563206, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:26:21,772]\u001b[0m Trial 206 finished with value: 0.032976340502500534 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004783601540629085, 'ent_coef': 0.05374451913323028, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:26:59,148]\u001b[0m Trial 207 finished with value: 0.08030055463314056 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00030341450653133744, 'ent_coef': 5.56482970525841e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:30:06,595]\u001b[0m Trial 208 finished with value: 0.0707663968205452 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0010032960606463056, 'ent_coef': 1.0707049553000092e-06, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:30:48,547]\u001b[0m Trial 209 finished with value: 2.2674670219421387 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0006960576390085161, 'ent_coef': 1.8154026756785226e-06, 'cliprange': 0.4, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:33:49,119]\u001b[0m Trial 210 finished with value: 0.0172454621642828 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.0007207984924814335, 'ent_coef': 1.9897870426857403e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:34:26,365]\u001b[0m Trial 211 finished with value: 0.7714712619781494 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.00015394036460900627, 'ent_coef': 2.9620897366709865e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:36:50,364]\u001b[0m Trial 212 finished with value: -96.22531127929688 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0006216007260775406, 'ent_coef': 8.909697454240612e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:37:43,229]\u001b[0m Trial 213 finished with value: -98.23383331298828 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0006883502037264459, 'ent_coef': 6.588873123979948e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:40:32,550]\u001b[0m Trial 214 finished with value: 0.12074773013591766 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0009079402325391335, 'ent_coef': 4.80233056815152e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:41:26,818]\u001b[0m Trial 215 finished with value: 0.29172390699386597 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.000977700285114588, 'ent_coef': 6.160732447813877e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:44:01,885]\u001b[0m Trial 216 finished with value: -97.9637451171875 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0002374569708444901, 'ent_coef': 2.377698693959595e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:45:08,330]\u001b[0m Trial 217 finished with value: 0.004431598819792271 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00020806557956491343, 'ent_coef': 2.1146850785065334e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:47:39,383]\u001b[0m Trial 219 finished with value: -95.18641662597656 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00035785603955918794, 'ent_coef': 3.6736071407479425e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "\u001b[32m[I 2021-03-19 20:47:39,396]\u001b[0m Trial 218 finished with value: 0.1266222596168518 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0004028150152014701, 'ent_coef': 2.3102428102817716e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:50:10,362]\u001b[0m Trial 221 finished with value: -97.1302719116211 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0005129492437622455, 'ent_coef': 4.849285398874316e-07, 'cliprange': 0.3, 'noptepochs': 10, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:51:22,753]\u001b[0m Trial 220 finished with value: 0.1500674933195114 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0004989453263231713, 'ent_coef': 1.4406966786532506e-05, 'cliprange': 0.3, 'noptepochs': 1, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:53:53,402]\u001b[0m Trial 222 finished with value: 1.0587091445922852 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0002445087632946309, 'ent_coef': 0.0013852257787733719, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:55:04,123]\u001b[0m Trial 223 finished with value: 0.014873094856739044 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.99, 'lr': 0.00029277289181215815, 'ent_coef': 0.0008666348431086978, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:57:28,885]\u001b[0m Trial 224 finished with value: 0.23293793201446533 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.98, 'lr': 0.00018426716068368592, 'ent_coef': 1.7393323724706901e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:58:39,998]\u001b[0m Trial 225 finished with value: -98.83293914794922 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.00018253509461077118, 'ent_coef': 1.314534174053324e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:01:11,057]\u001b[0m Trial 226 finished with value: 0.15421132743358612 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.00010262956310853824, 'ent_coef': 3.2218968454044087e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:02:17,254]\u001b[0m Trial 227 finished with value: 0.009396667592227459 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0011793304774964568, 'ent_coef': 1.2974450159969796e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:03:19,408]\u001b[0m Trial 228 finished with value: -96.47765350341797 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0008126057257023089, 'ent_coef': 1.2458286011749596e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:06:03,443]\u001b[0m Trial 229 finished with value: 0.07231636345386505 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0007555686752279776, 'ent_coef': 1.0271939553038374e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:06:58,602]\u001b[0m Trial 230 finished with value: 0.050098717212677 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0006949125111719768, 'ent_coef': 8.595383591747861e-07, 'cliprange': 0.2, 'noptepochs': 50, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:09:40,516]\u001b[0m Trial 231 finished with value: 1.0154650211334229 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0002501178806064551, 'ent_coef': 6.966846750518051e-07, 'cliprange': 0.2, 'noptepochs': 50, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:10:44,555]\u001b[0m Trial 232 finished with value: 0.03316015377640724 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00014664674084665253, 'ent_coef': 3.6926855847090204e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:13:21,431]\u001b[0m Trial 233 finished with value: 0.038130756467580795 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0001422706217594977, 'ent_coef': 3.637795477200995e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:13:44,329]\u001b[0m Trial 234 finished with value: -98.5754165649414 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0001861987821846229, 'ent_coef': 0.0005690026025407652, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:16:53,857]\u001b[0m Trial 235 finished with value: 0.0022153411991894245 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00020993120807804222, 'ent_coef': 0.0005544254415982179, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:17:30,228]\u001b[0m Trial 236 finished with value: 0.02018238976597786 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003253390268474696, 'ent_coef': 0.0005393139793698305, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:20:31,084]\u001b[0m Trial 237 finished with value: 0.0015482923481613398 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003667268086818086, 'ent_coef': 0.0007680585570436277, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:21:07,445]\u001b[0m Trial 238 finished with value: 0.008122202008962631 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00017261545073048753, 'ent_coef': 0.0006862853578069671, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:24:01,460]\u001b[0m Trial 239 finished with value: 0.0011005664709955454 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00018125823087745139, 'ent_coef': 2.1602414461420257e-06, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:24:37,594]\u001b[0m Trial 240 finished with value: 0.004145464859902859 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005685981570982916, 'ent_coef': 2.358269320655955e-06, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:27:28,182]\u001b[0m Trial 241 finished with value: 0.013390375301241875 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0005909742376072864, 'ent_coef': 6.831377655632671e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:28:13,285]\u001b[0m Trial 242 finished with value: 0.015396269969642162 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00010973852900812692, 'ent_coef': 0.00034618819733067625, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:31:00,725]\u001b[0m Trial 243 finished with value: -98.95348358154297 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00025139424134592306, 'ent_coef': 0.0003572984302688505, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:31:41,908]\u001b[0m Trial 244 finished with value: 0.9295241236686707 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0002608857119352308, 'ent_coef': 1.4211922508154743e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:32:09,857]\u001b[0m Trial 245 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:34:11,605]\u001b[0m Trial 246 finished with value: -96.5904769897461 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004484880473139683, 'ent_coef': 0.0003849977724530602, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:34:13,829]\u001b[0m Trial 247 finished with value: -96.15493774414062 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0004145520085619157, 'ent_coef': 1.7621909325104038e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:35:18,516]\u001b[0m Trial 248 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:36:43,280]\u001b[0m Trial 249 finished with value: -88.78785705566406 and parameters: {'batch_size': 32, 'n_steps': 16, 'gamma': 0.95, 'lr': 0.00020840386425325456, 'ent_coef': 0.0002814005035370942, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:38:46,513]\u001b[0m Trial 250 finished with value: 0.12094694375991821 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.0002973872722495401, 'ent_coef': 1.6489482014104733e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:40:12,905]\u001b[0m Trial 251 finished with value: 0.03279619663953781 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00032518837747449585, 'ent_coef': 9.748020351334367e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:42:11,610]\u001b[0m Trial 252 finished with value: 0.1145012229681015 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0010878026349764418, 'ent_coef': 0.00014417579280603607, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:43:52,197]\u001b[0m Trial 253 finished with value: 1.0760496854782104 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.0011139319273250374, 'ent_coef': 0.00017614243527371338, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:45:39,016]\u001b[0m Trial 254 finished with value: 1.4189367294311523 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.0008367803858663228, 'ent_coef': 3.040348947168964e-05, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:46:05,426]\u001b[0m Trial 255 finished with value: -96.26423645019531 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005072384105102286, 'ent_coef': 2.492492522066414e-05, 'cliprange': 0.2, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:49:11,430]\u001b[0m Trial 256 finished with value: 0.07558340579271317 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004897714484281822, 'ent_coef': 4.355711308730333e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:49:39,641]\u001b[0m Trial 257 finished with value: 0.04781468212604523 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.000633131611410767, 'ent_coef': 4.1765393247444764e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:50:48,217]\u001b[0m Trial 259 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:51:35,917]\u001b[0m Trial 258 finished with value: 9.675153705757111e-06 and parameters: {'batch_size': 64, 'n_steps': 2048, 'gamma': 0.9, 'lr': 0.0006770302939066272, 'ent_coef': 0.0010784868871602617, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:53:30,730]\u001b[0m Trial 260 finished with value: -97.47564697265625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00015545945168523047, 'ent_coef': 0.00023626523076105674, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:55:03,931]\u001b[0m Trial 261 finished with value: 0.09715677052736282 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 8.550156596862408e-05, 'ent_coef': 0.00025623001721833015, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "\u001b[32m[I 2021-03-19 21:55:04,442]\u001b[0m Trial 262 finished with value: -95.82355499267578 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0003700799169625218, 'ent_coef': 5.418364479286328e-07, 'cliprange': 0.3, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:57:40,487]\u001b[0m Trial 264 finished with value: -97.4984359741211 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.99, 'lr': 0.0002262577533780622, 'ent_coef': 2.741685064511299e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:58:40,211]\u001b[0m Trial 263 finished with value: 0.03648483008146286 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0004096490196044685, 'ent_coef': 0.0005714572238751784, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:01:11,604]\u001b[0m Trial 265 finished with value: 0.00859985314309597 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0009034842363742551, 'ent_coef': 1.218051496562463e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:02:10,607]\u001b[0m Trial 266 finished with value: 0.022247646003961563 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000263456456617833, 'ent_coef': 1.487150332223328e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:04:34,514]\u001b[0m Trial 267 finished with value: -38.395713806152344 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00030616457549940447, 'ent_coef': 9.708674565334444e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:05:45,245]\u001b[0m Trial 268 finished with value: 0.05108203738927841 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00011634288248681735, 'ent_coef': 1.089093565061322e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:07:04,917]\u001b[0m Trial 269 finished with value: -95.65583038330078 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00012555194569110437, 'ent_coef': 5.11313092794225e-06, 'cliprange': 0.3, 'noptepochs': 10, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:09:23,766]\u001b[0m Trial 270 finished with value: 0.007828998379409313 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0006690758083873863, 'ent_coef': 2.7099272112281e-06, 'cliprange': 0.3, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:10:24,215]\u001b[0m Trial 271 finished with value: 0.005667927674949169 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.0006265895569571004, 'ent_coef': 1.6631288897371523e-06, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:12:47,692]\u001b[0m Trial 272 finished with value: 0.08671535551548004 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.000516335355194013, 'ent_coef': 2.10521800870748e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:13:49,362]\u001b[0m Trial 273 finished with value: 0.006850909441709518 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0008791635725731562, 'ent_coef': 9.788755702407455e-08, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:13:59,296]\u001b[0m Trial 274 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:16:32,408]\u001b[0m Trial 276 finished with value: -97.23234558105469 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00017171201319687103, 'ent_coef': 0.0004061614789638241, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:17:23,876]\u001b[0m Trial 275 finished with value: 0.001351920422166586 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0014332573745241385, 'ent_coef': 0.006585022172863231, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:19:56,578]\u001b[0m Trial 277 finished with value: 0.3343733251094818 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00036730715949794695, 'ent_coef': 7.134131790328571e-07, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:20:49,874]\u001b[0m Trial 278 finished with value: 0.233188658952713 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003908907627432872, 'ent_coef': 6.290800848255761e-05, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:23:24,305]\u001b[0m Trial 279 finished with value: 0.031327854841947556 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00023356247244136165, 'ent_coef': 3.6534453091974155e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:24:20,774]\u001b[0m Trial 280 finished with value: 0.04041465371847153 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00022219751817185453, 'ent_coef': 2.5063934722167827e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:26:05,494]\u001b[0m Trial 281 finished with value: -97.45672607421875 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0008233002965231975, 'ent_coef': 2.495588457016806e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:27:50,711]\u001b[0m Trial 282 finished with value: 4.464471817016602 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.000773860282454967, 'ent_coef': 0.00010346671065833028, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:28:12,341]\u001b[0m Trial 283 finished with value: -95.00799560546875 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0005458244192705071, 'ent_coef': 8.426322722237431e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:29:35,819]\u001b[0m Trial 284 finished with value: -88.20379638671875 and parameters: {'batch_size': 128, 'n_steps': 32, 'gamma': 0.98, 'lr': 0.0005347448894502415, 'ent_coef': 1.6283376913197238e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:31:27,361]\u001b[0m Trial 285 finished with value: -98.54085540771484 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002989132824760789, 'ent_coef': 0.000675357549384381, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:32:56,534]\u001b[0m Trial 286 finished with value: 1.5000797510147095 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002891401261267621, 'ent_coef': 0.0008006421093886877, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:34:53,508]\u001b[0m Trial 287 finished with value: 0.010536936111748219 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.001080000962813331, 'ent_coef': 0.001354978191712451, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:35:57,695]\u001b[0m Trial 288 finished with value: -97.80298614501953 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0028788070876784124, 'ent_coef': 0.0027803307415303646, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:37:12,057]\u001b[0m Trial 289 finished with value: 8.332468496519141e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.99, 'lr': 0.00017957864059594172, 'ent_coef': 0.0005399624255387583, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:38:21,883]\u001b[0m Trial 290 finished with value: 9.872881491901353e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.002903027946669068, 'ent_coef': 0.001036177935761897, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:38:39,056]\u001b[0m Trial 291 finished with value: -96.2354965209961 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0025256144358446154, 'ent_coef': 0.002977923604901399, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:40:16,135]\u001b[0m Trial 292 finished with value: -96.12925720214844 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0018343653632945313, 'ent_coef': 0.0021598243002033043, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:41:36,785]\u001b[0m Trial 293 finished with value: -98.22249603271484 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.00014883155135890933, 'ent_coef': 0.009320462557354149, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:43:36,700]\u001b[0m Trial 294 finished with value: 0.010051682591438293 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.0035311166812060134, 'ent_coef': 0.0006651757401705804, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:45:17,201]\u001b[0m Trial 295 finished with value: 0.6132508516311646 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 6.330491968910393e-05, 'ent_coef': 2.3980873509110256e-05, 'cliprange': 0.2, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:46:56,889]\u001b[0m Trial 296 finished with value: 0.0023177810944616795 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.00014503301675561982, 'ent_coef': 0.009346359540349925, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:48:49,342]\u001b[0m Trial 297 finished with value: 0.04880461096763611 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.00010876088213124106, 'ent_coef': 0.01130449527514483, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:50:26,792]\u001b[0m Trial 298 finished with value: 0.24413642287254333 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00010226462523519806, 'ent_coef': 0.0015466853761390807, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "\u001b[32m[I 2021-03-19 22:51:13,064]\u001b[0m Trial 299 finished with value: 0.001321527175605297 and parameters: {'batch_size': 128, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.0001654371584857235, 'ent_coef': 0.022620294406348367, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Number of finished trials:  300\n",
            "Best trial:\n",
            "Value:  -99.14582824707031\n",
            "Params: \n",
            "    batch_size: 32\n",
            "    n_steps: 256\n",
            "    gamma: 0.95\n",
            "    lr: 0.0006213010340576837\n",
            "    ent_coef: 0.0004927687872681538\n",
            "    cliprange: 0.2\n",
            "    noptepochs: 5\n",
            "    lambda: 1.0\n",
            "Writing report to logs/ppo2/report_MountainCarContinuous-v0_300-trials-50000-tpe-median_1616194273.csv\n",
            "[77e9fbce07ac:03110] *** Process received signal ***\n",
            "[77e9fbce07ac:03110] Signal: Segmentation fault (11)\n",
            "[77e9fbce07ac:03110] Signal code: Address not mapped (1)\n",
            "[77e9fbce07ac:03110] Failing at address: 0x7fafcb5b420d\n",
            "[77e9fbce07ac:03110] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fafce25f980]\n",
            "[77e9fbce07ac:03110] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fafcde9e8a5]\n",
            "[77e9fbce07ac:03110] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fafce709e44]\n",
            "[77e9fbce07ac:03110] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fafcde9f735]\n",
            "[77e9fbce07ac:03110] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fafce707cb3]\n",
            "[77e9fbce07ac:03110] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwG4delwEeL7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRfdbb1YEfHt"
      },
      "source": [
        "### Optimal combination of hyperparameters for 300 trials\n",
        "  - We run a search model with 1000 trials, up to the 376 model the algorithm stop due to runtime out. It suggested then that the best combination was given by the trial 115\n",
        "  - To avoid runtime probles we ran the model with 300 trials. The best model chosen was the one of 115 trials (see description below)\n",
        "  - We cannot see the model running, but we asume the code (!python train.py --algo ppo2 --env MountainCarContinuous-v0 -n 50000 -optimize --n-trials 300 --n-jobs 2 --sampler tpe --pruner median) does not only search the optimal combination of hyperparameters, but it also trains the adapted model (as the !python suggests)\n",
        "  - The only addition to the !python... order that worked, including the optimised set of hyperparameters) was: !python train.py --algo ppo2 --env MountainCarContinuous-v0 --n-timesteps 256 #--n_envs 16 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uehzE7ds92ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c7c35c-059a-4e97-ccad-4ecf6f846532"
      },
      "source": [
        "\n",
        "'''\n",
        "Optuna search results, 300 trials\n",
        "Params: \n",
        "    batch_size: 32\n",
        "    n_steps: 256\n",
        "    gamma: 0.95\n",
        "    lr: 0.0006213010340576837\n",
        "    ent_coef: 0.0004927687872681538\n",
        "    cliprange: 0.2\n",
        "    noptepochs: 5\n",
        "    lambda: 1.0\n",
        "'''\n",
        "!python train.py --algo ppo2 --env MountainCarContinuous-v0 --n-timesteps 256 #--n_envs 16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MountainCarContinuous-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.94),\n",
            "             ('n_envs', 16),\n",
            "             ('n_steps', 256),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 8),\n",
            "             ('noptepochs', 4),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=256\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MountainCarContinuous-v0_8\n",
            "Saving to logs/ppo2/MountainCarContinuous-v0_8\n",
            "[77e9fbce07ac:08720] *** Process received signal ***\n",
            "[77e9fbce07ac:08720] Signal: Segmentation fault (11)\n",
            "[77e9fbce07ac:08720] Signal code: Address not mapped (1)\n",
            "[77e9fbce07ac:08720] Failing at address: 0x7f08edadf20d\n",
            "[77e9fbce07ac:08720] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f08f078a980]\n",
            "[77e9fbce07ac:08720] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f08f03c98a5]\n",
            "[77e9fbce07ac:08720] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f08f0c34e44]\n",
            "[77e9fbce07ac:08720] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f08f03ca735]\n",
            "[77e9fbce07ac:08720] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f08f0c32cb3]\n",
            "[77e9fbce07ac:08720] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9XuzTLdRgL8"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2zxBgNNqU2o"
      },
      "source": [
        "'''\n",
        "# Tuned\n",
        "MountainCarContinuous-v0:\n",
        "  normalize: true\n",
        "  n_envs: 1\n",
        "  n_timesteps: !!float 20000\n",
        "  policy: 'MlpPolicy'\n",
        "  batch_size: 256\n",
        "  n_steps: 8\n",
        "  gamma: 0.9999\n",
        "  learning_rate: !!float 7.77e-05\n",
        "  ent_coef: 0.00429\n",
        "  clip_range: 0.1\n",
        "  n_epochs: 10\n",
        "  gae_lambda: 0.9\n",
        "  max_grad_norm: 5\n",
        "  vf_coef: 0.19\n",
        "  use_sde: True\n",
        "  policy_kwargs: \"dict(log_std_init=-3.29, ortho_init=False)\"\n",
        "'''\n",
        "#!python train.py --algo trpo --env HotterColder-v0 # it is not in training format yet\n",
        "# algorithms a2c,acer,acktr,dqn,ddpg,her,sac,ppo2,trpo,td3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}