{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL Lab 2 part 2 EM FZ simple model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zarfer007/AprendizajePorRefuerzos/blob/master/testingGit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SytTR8K1oII-"
      },
      "source": [
        "# RL 2 LAB I.1\n",
        "\n",
        "  - Morbidoni, Emilio\n",
        "  - Zarzosa, Fernando\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD-FSSuKoIJF"
      },
      "source": [
        "# Interfaz b치sica stable-baselines\n",
        "\n",
        "### Instalaci칩n de Stable-baselines\n",
        "\n",
        "  - Desde Windows, adem치s, instalar: \n",
        "* Microsoft Visual C++ desde https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
        "* PyType, mediante `conda install -c conda-forge pytype`\n",
        "\n",
        "  - Desde Linux o Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLaFYpurBU7P"
      },
      "source": [
        "#!pip install sphinxcontrib-spelling # if problems with sphinx\n",
        "#!pip install datascience"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ttqOLol_oIJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc762cd-cfe6-4cd3-9065-1c899e7d2111"
      },
      "source": [
        "#@title Instalaci칩n (no modificar)\n",
        "!pip install stable-baselines3[extra,tests,docs]>=0.11.0a4 && pip install sb3-contrib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinxcontrib-spelling 7.1.0 has requirement Sphinx>=3.0.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytest-cov 2.11.1 has requirement pytest>=4.6, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytest-forked 1.3.0 has requirement pytest>=3.10, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytest-xdist 2.2.1 has requirement pytest>=6.0.0, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "Collecting sb3-contrib\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/ce/e9809debd089278cc0b38f6f1fd21cf3442ebebb7e19c6335f79ee3bb9fe/sb3_contrib-1.0-py3-none-any.whl\n",
            "Requirement already satisfied: stable-baselines3[docs,tests]>=1.0 in /usr/local/lib/python3.7/dist-packages (from sb3-contrib) (1.0)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.1.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.8.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.2.2)\n",
            "Requirement already satisfied: sphinx; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.8.5)\n",
            "Collecting sphinxcontrib.spelling; extra == \"docs\"\n",
            "  Using cached https://files.pythonhosted.org/packages/f6/62/796d8ae02732c162f8d53406f520c9f3c886a9ab24de4ef6995404c2b1d8/sphinxcontrib_spelling-7.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: sphinx-autobuild; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2021.3.14)\n",
            "Requirement already satisfied: sphinx-rtd-theme; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.5.1)\n",
            "Requirement already satisfied: sphinx-autodoc-typehints; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.11.1)\n",
            "Requirement already satisfied: isort>=5.0; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (5.7.0)\n",
            "Requirement already satisfied: flake8>=3.8; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.9.0)\n",
            "Requirement already satisfied: pytest-cov; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.11.1)\n",
            "Requirement already satisfied: pytype; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2021.3.10)\n",
            "Requirement already satisfied: pytest-xdist; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.2.1)\n",
            "Requirement already satisfied: pytest; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.6.4)\n",
            "Requirement already satisfied: pytest-env; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.6.2)\n",
            "Requirement already satisfied: black; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,tests]>=1.0->sb3-contrib) (20.8b1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.7.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.4.7)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.6.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.23.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.9.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (20.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.15.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.16)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.7.2)\n",
            "Requirement already satisfied: PyEnchant>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.2.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.4.4)\n",
            "Requirement already satisfied: livereload in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.6.3)\n",
            "Requirement already satisfied: pyflakes<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.3.0)\n",
            "Requirement already satisfied: pycodestyle<2.8.0,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.7.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.6.1)\n",
            "Requirement already satisfied: coverage>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pytest-cov; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (5.5)\n",
            "Requirement already satisfied: pyyaml>=3.11 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.13)\n",
            "Requirement already satisfied: importlab>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.6.1)\n",
            "Requirement already satisfied: typed-ast in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.4.2)\n",
            "Requirement already satisfied: ninja>=1.10.0.post2 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.10.0.post2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (20.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.10.2)\n",
            "Requirement already satisfied: execnet>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytest-xdist; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.8.0)\n",
            "Requirement already satisfied: pytest-forked in /usr/local/lib/python3.7/dist-packages (from pytest-xdist; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (8.7.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.10.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.4.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.4.3)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2021.3.17)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (7.1.2)\n",
            "Requirement already satisfied: pathspec<1,>=0.6 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.24.3)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (3.4.1)\n",
            "Requirement already satisfied: tornado; python_version > \"2.7\" in /usr/local/lib/python3.7/dist-packages (from livereload->sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (5.1.1)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.7/dist-packages (from importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (2.5)\n",
            "Requirement already satisfied: apipkg>=1.4 in /usr/local/lib/python3.7/dist-packages (from execnet>=1.1->pytest-xdist; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (1.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2->importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,tests]>=1.0->sb3-contrib) (4.4.2)\n",
            "\u001b[31mERROR: sphinxcontrib-spelling 7.1.0 has requirement Sphinx>=3.0.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sb3-contrib, sphinxcontrib.spelling\n",
            "Successfully installed sb3-contrib-1.0 sphinxcontrib.spelling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUUkScRxoIJQ"
      },
      "source": [
        "# RL-baselines3-zoo\n",
        "\n",
        "  - Instalaci칩n de RLBaselinesZoo\n",
        "\n",
        "Desde Google Colab\n",
        "\n",
        "  - ---recursive allows to train agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM_pM0mIoIJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f144d9c0-ca2a-4436-d57e-44702d282501"
      },
      "source": [
        "#@title Instalaci칩n de RLBaselinesZoo (no modificar)\n",
        "\n",
        "#if IN_COLAB:\n",
        "!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "!cd rl-baselines3-zoo/\n",
        "!apt-get install swig cmake ffmpeg\n",
        "!pip install -r /content/rl-baselines3-zoo/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines3-zoo'...\n",
            "remote: Enumerating objects: 275, done.\u001b[K\n",
            "remote: Counting objects: 100% (275/275), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 2161 (delta 18), reused 257 (delta 12), pack-reused 1886\u001b[K\n",
            "Receiving objects: 100% (2161/2161), 1.00 MiB | 14.09 MiB/s, done.\n",
            "Resolving deltas: 100% (1363/1363), done.\n",
            "Submodule 'rl-trained-agents' (https://github.com/DLR-RM/rl-trained-agents) registered for path 'rl-trained-agents'\n",
            "Cloning into '/content/rl-baselines3-zoo/rl-trained-agents'...\n",
            "remote: Enumerating objects: 1, done.        \n",
            "remote: Counting objects: 100% (1/1), done.        \n",
            "remote: Total 1416 (delta 0), reused 0 (delta 0), pack-reused 1415\n",
            "Receiving objects: 100% (1416/1416), 977.06 MiB | 32.34 MiB/s, done.\n",
            "Resolving deltas: 100% (220/220), done.\n",
            "Submodule path 'rl-trained-agents': checked out 'd81fcd61cef4599564c859297ea68bacf677db6b'\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (1,300 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: stable-baselines3[docs,extra,tests]>=1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.0)\n",
            "Collecting box2d-py==2.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 450kB 4.6MB/s \n",
            "\u001b[?25hCollecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/9c/7b76db10cdaa69c840b211fe21ce6f31fb80b611b198fe18a64ddb8f374e/pybullet-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (88.7MB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 88.7MB 76kB/s \n",
            "\u001b[?25hCollecting gym-minigrid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/d2/e2b0fc791c9d22c70ea48df762133d7ad930ea09e1bfa95a24a1c86ddf18/gym_minigrid-1.0.2-py3-none-any.whl (47kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 51kB 4.7MB/s \n",
            "\u001b[?25hCollecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 102kB 8.0MB/s \n",
            "\u001b[?25hCollecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/67/ed0af7c66bcfb9a9a56fbafcca7d848452d78433208b59b003741879cc69/optuna-2.6.0-py3-none-any.whl (293kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 296kB 40.4MB/s \n",
            "\u001b[?25hCollecting pytablewriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 102kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r /content/rl-baselines3-zoo/requirements.txt (line 8)) (0.11.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 645kB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sb3-contrib>=1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/rl-baselines3-zoo/requirements.txt (line 10)) (1.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.5.1)\n",
            "Collecting sphinxcontrib.spelling; extra == \"docs\"\n",
            "  Using cached https://files.pythonhosted.org/packages/f6/62/796d8ae02732c162f8d53406f520c9f3c886a9ab24de4ef6995404c2b1d8/sphinxcontrib_spelling-7.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: sphinx; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.8.5)\n",
            "Requirement already satisfied: sphinx-autodoc-typehints; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: sphinx-autobuild; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2021.3.14)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.2.6)\n",
            "Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (5.4.8)\n",
            "Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (4.1.2.30)\n",
            "Requirement already satisfied: tensorboard>=2.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (7.0.0)\n",
            "Requirement already satisfied: flake8>=3.8; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.9.0)\n",
            "Requirement already satisfied: pytest; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.6.4)\n",
            "Requirement already satisfied: pytest-xdist; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: pytest-cov; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.11.1)\n",
            "Requirement already satisfied: isort>=5.0; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (5.7.0)\n",
            "Requirement already satisfied: black; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (20.8b1)\n",
            "Requirement already satisfied: pytype; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2021.3.10)\n",
            "Requirement already satisfied: pytest-env; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.6.2)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r /content/rl-baselines3-zoo/requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r /content/rl-baselines3-zoo/requirements.txt (line 5)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r /content/rl-baselines3-zoo/requirements.txt (line 5)) (1.4.1)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading https://files.pythonhosted.org/packages/01/1f/43b01223a0366171f474320c6e966c39a11587287f098a5f09809b45e05f/cmaes-0.8.2-py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (20.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (4.41.1)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/d6/7d9acb68a77acd140be7fececb7f2701b2a29d2da9c54184cb8f93509590/cliff-3.7.0-py3-none-any.whl (80kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 81kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (1.3.23)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/39/0230290df0519d528d8d0ffdfd900150ed24e0076d13b1f19e279444aab1/colorlog-4.7.2-py2.py3-none-any.whl\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/29/ed5c134aba874053859ba3e8d4705b4a5c1b66156deabc26cbe643e83f2e/alembic-1.5.7-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 163kB 36.3MB/s \n",
            "\u001b[?25hCollecting msgfy<1,>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl\n",
            "Collecting pathvalidate<3,>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/97/d1/096e837c64c21e1ac046fa6333f06979e1139d0e7b2c46da63d9484956e6/pathvalidate-2.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r /content/rl-baselines3-zoo/requirements.txt (line 7)) (54.0.0)\n",
            "Collecting typepy[datetime]<2,>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/bf/2d4cb8e7896bb20f213358dd66fcbab9f61e140b0046a9651df4d4f7dac8/typepy-1.1.4-py3-none-any.whl\n",
            "Collecting mbstrdecoder<2,>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/f6/0e6bb50c3c6380a4982c87d80e70b2f6e366523a57a0c58594aea472206d/mbstrdecoder-1.0.1-py3-none-any.whl\n",
            "Collecting tcolorpy<1,>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/02/51/bbb0cc7f30771c285c354634bf83653a2871d58c6923bd29bfddeb9c9cb1/tcolorpy-0.0.8-py3-none-any.whl\n",
            "Collecting DataProperty<2,>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/2d/e5413965af992f4e489b6f5eebf52db9c17953c772962d1223d434b05cef/DataProperty-0.50.0-py3-none-any.whl\n",
            "Collecting tabledata<2,>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/df/b2/264d9707502f0259a3eb82ec48064df98b1735d5a5f315b6a1d7105263f4/tabledata-1.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.7.2)\n",
            "Requirement already satisfied: PyEnchant>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.9.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.7.12)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.2.4)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.16)\n",
            "Requirement already satisfied: livereload in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: pyflakes<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: pycodestyle<2.8.0,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.7.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (8.7.0)\n",
            "Requirement already satisfied: execnet>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytest-xdist; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: pytest-forked in /usr/local/lib/python3.7/dist-packages (from pytest-xdist; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: coverage>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pytest-cov; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (5.5)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: typed-ast>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: pathspec<1,>=0.6 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2021.3.17)\n",
            "Requirement already satisfied: ninja>=1.10.0.post2 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.10.0.post2)\n",
            "Requirement already satisfied: importlab>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.6.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 51kB 4.4MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/8b/15061b32332bb35ea2a2f6263d0f616779d576e82739ec8e7fcf3c94abf5/cmd2-1.5.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 143kB 37.1MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 112kB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (2.1.0)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 81kB 7.5MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: chardet<5,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->-r /content/rl-baselines3-zoo/requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.1.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: tornado; python_version > \"2.7\" in /usr/local/lib/python3.7/dist-packages (from livereload->sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (5.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: apipkg>=1.4 in /usr/local/lib/python3.7/dist-packages (from execnet>=1.1->pytest-xdist; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (1.5)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.7/dist-packages (from importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (2.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->-r /content/rl-baselines3-zoo/requirements.txt (line 6)) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2->importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.0->-r /content/rl-baselines3-zoo/requirements.txt (line 1)) (4.4.2)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11107 sha256=07b3148205b83d6453d0d568cd1a8dd57abccc2cd050f99b58e81d1d67413778\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n",
            "Successfully built pyperclip\n",
            "\u001b[31mERROR: sphinxcontrib-spelling 7.1.0 has requirement Sphinx>=3.0.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: box2d-py, pybullet, gym-minigrid, pyyaml, pyaml, scikit-optimize, cmaes, pbr, stevedore, pyperclip, cmd2, cliff, colorlog, Mako, python-editor, alembic, optuna, msgfy, pathvalidate, mbstrdecoder, typepy, tcolorpy, DataProperty, tabledata, pytablewriter, sphinxcontrib.spelling\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed DataProperty-0.50.0 Mako-1.1.4 alembic-1.5.7 box2d-py-2.3.8 cliff-3.7.0 cmaes-0.8.2 cmd2-1.5.0 colorlog-4.7.2 gym-minigrid-1.0.2 mbstrdecoder-1.0.1 msgfy-0.1.0 optuna-2.6.0 pathvalidate-2.3.2 pbr-5.5.1 pyaml-20.4.0 pybullet-3.1.0 pyperclip-1.8.2 pytablewriter-0.58.0 python-editor-1.0.4 pyyaml-5.4.1 scikit-optimize-0.8.1 sphinxcontrib.spelling stevedore-3.3.0 tabledata-1.1.3 tcolorpy-0.0.8 typepy-1.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzBEoyPzoIJR"
      },
      "source": [
        "Desde Linux, ejecutando\n",
        "\n",
        "    git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "    cd rl-baselines3-zoo/\n",
        "    sudo apt-get install swig cmake ffmpeg\n",
        "    pip install -r requirements.txt\n",
        "\n",
        "Instalaci칩n desde Nabucodonosor\n",
        "\n",
        "    wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh\n",
        "    chmod 755 Anaconda3-2020.11-Linux-x86_64.sh\n",
        "    ./Anaconda3-2020.11-Linux-x86_64.sh\n",
        "    conda create --name rl\n",
        "    conda activate rl\n",
        "    conda config --add channels conda-forge\n",
        "    conda install jupyter atari_py swig\n",
        "    pip install stable-baselines3[extra,tests,docs]>=0.11.0a4\n",
        "    pip install sb3-contrib "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Morjd1dFoIJH"
      },
      "source": [
        "## Ejecuci칩n de un algoritmo de RL\n",
        "  - Importaciones/inicializaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG7i44kqoIJH"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "#from gym.envs.registration import register\n",
        "\n",
        "from stable_baselines3 import DQN, PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFLZieLNoIJO"
      },
      "source": [
        "Para usar un entorno compatible por esta librer칤a, el mismo tiene que heredar de *gym.Env*. Vemos un ejemplo (cr칠dito: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6HeKF2wRCcD"
      },
      "source": [
        "# Lab 2\n",
        "\n",
        "1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e h칤per-par치metros. \n",
        "\n",
        "    Algunas ideas:\n",
        "\n",
        "    * Transformar GoLeftEnv en una grilla 2D, a침adir paredes / trampas / agua.\n",
        "    * Crear un entorno que juegue a alg칰n juego como el ta-te-ti.\n",
        "    * Crea un entorno totalmente nuevo que sea de tu inter칠s!\n",
        "\n",
        "2. Entrena agentes en entornos m치s complejos con stable-baselines/rl-baselines-zoo. Tener en cuenta:\n",
        "\n",
        "    * Google Colab tiene una limitante en cuanto a cantidad de recursos de CPU/GPU (incluido un \"rendimiento decreciente silencioso\"), lo cu치l reduce la capacidad de entrenar distintos entornos.\n",
        "    * Si el entorno no est치 implementado en stable-baselines, debe hacerse un wrapper a mano, lo que puede ser sencillo o puede llevar algo m치s de trabajo, teniendo que tocar c칩digo subyacente de la librer칤a. \n",
        "\n",
        "\\* pueden ser usando stable-baselines/rl-baselines-zoo o bien utilizando alg칰n otro algoritmo (incluso tabular)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QuB3twMgjjg"
      },
      "source": [
        "# 2.2 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6TCyu_UoIJR"
      },
      "source": [
        "## Ejecuci칩n\n",
        "\n",
        "Los agentes pueden ser llamados desde la consola mediante comandos como\n",
        "\n",
        "`python train.py --algo algo_name --env env_id`\n",
        "\n",
        "Los cuales pueden ser llamados usando"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSfIitK7oIJR"
      },
      "source": [
        "\n",
        "Tambi칠n es posible grabar un video! Ver https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#record-a-video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke2LldyZmi36"
      },
      "source": [
        "# MountainCarContinuous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yk12ZnNmkKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d786df-8d07-4078-c755-59ebf284e3cb"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev ffmpeg freeglut3-dev xvfb\n",
        "!pip install stable-baselines[mpi] --upgrade\n",
        "!pip install pybullet\n",
        "!pip install box2d box2d-kengz pyyaml pytablewriter optuna scikit-optimize\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connected to cloud.r-pro\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n",
            "Collecting stable-baselines[mpi]\n",
            "  Using cached https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->stable-baselines[mpi]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.1\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.7/dist-packages (2.3.10)\n",
            "Requirement already satisfied: box2d-kengz in /usr/local/lib/python3.7/dist-packages (2.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (5.4.1)\n",
            "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.7/dist-packages (0.58.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (1.0.1)\n",
            "Requirement already satisfied: typepy[datetime]<2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (1.1.4)\n",
            "Requirement already satisfied: tabledata<2,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (1.1.3)\n",
            "Requirement already satisfied: pathvalidate<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (2.3.2)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (54.0.0)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (0.0.8)\n",
            "Requirement already satisfied: msgfy<1,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (0.1.0)\n",
            "Requirement already satisfied: DataProperty<2,>=0.50.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter) (0.50.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (4.7.2)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (20.9)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.3.23)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.5.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2018.9; extra == \"datetime\" in /usr/local/lib/python3.7/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2018.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\" in /usr/local/lib/python3.7/dist-packages (from typepy[datetime]<2,>=1.1.1->pytablewriter) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (1.5.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.1.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.5.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.4)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\"->typepy[datetime]<2,>=1.1.1->pytablewriter) (1.15.0)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2>=1.0.0->cliff->optuna) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zm4i3bd8ZdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2805d6eb-797f-4a36-d2b6-e902a2f53b3b"
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines-zoo'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 1843 (delta 0), reused 1 (delta 0), pack-reused 1840\u001b[K\n",
            "Receiving objects: 100% (1843/1843), 375.67 MiB | 33.32 MiB/s, done.\n",
            "Resolving deltas: 100% (1091/1091), done.\n",
            "Checking out files: 100% (333/333), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi1LIM6FQ-aJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897baa60-009f-40b4-dc42-25ad0d6b92c8"
      },
      "source": [
        "cd rl-baselines-zoo/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rl-baselines-zoo/rl-baselines-zoo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKvv66bsI0--"
      },
      "source": [
        "### Source:\n",
        "https://gym.openai.com/envs/MountainCarContinuous-v0/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk2E1RoOJD2U",
        "outputId": "0950babf-48d9-4c70-99c8-33b6f3fc3e8b"
      },
      "source": [
        "tensorflow_version # tensorflow version for gym is 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unknown TensorFlow version: # tensorflow version for gym is 1.x\n",
            "Currently selected TF version: 1.x\n",
            "Available versions:\n",
            " * 1.x\n",
            " * 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feS7q6Kr8emj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07da02e2-4767-4dc2-bbeb-d8ba42d398b0"
      },
      "source": [
        "# Baseline Model\n",
        "!python train.py --algo ppo2 --env MountainCarContinuous-v0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MountainCarContinuous-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.94),\n",
            "             ('n_envs', 16),\n",
            "             ('n_steps', 256),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 8),\n",
            "             ('noptepochs', 4),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MountainCarContinuous-v0_1\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00063773367 |\n",
            "| clipfrac           | 0.0014648438  |\n",
            "| explained_variance | -0.00818      |\n",
            "| fps                | 4562          |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 1.4159632     |\n",
            "| policy_loss        | -0.0009888856 |\n",
            "| serial_timesteps   | 256           |\n",
            "| time_elapsed       | 2.12e-05      |\n",
            "| total_timesteps    | 4096          |\n",
            "| value_loss         | 0.63798976    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00024230995 |\n",
            "| clipfrac           | 0.00012207031 |\n",
            "| explained_variance | -0.0423       |\n",
            "| fps                | 9277          |\n",
            "| n_updates          | 2             |\n",
            "| policy_entropy     | 1.4080168     |\n",
            "| policy_loss        | -0.0016784458 |\n",
            "| serial_timesteps   | 512           |\n",
            "| time_elapsed       | 0.898         |\n",
            "| total_timesteps    | 8192          |\n",
            "| value_loss         | 0.10600683    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 999.00 +/- 0.00\n",
            "New best mean reward!\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0003233038 |\n",
            "| clipfrac           | 0.0006713867 |\n",
            "| ep_len_mean        | 724          |\n",
            "| ep_reward_mean     | 63.2         |\n",
            "| explained_variance | 0.00723      |\n",
            "| fps                | 755          |\n",
            "| n_updates          | 3            |\n",
            "| policy_entropy     | 1.3991096    |\n",
            "| policy_loss        | -0.001596961 |\n",
            "| serial_timesteps   | 768          |\n",
            "| time_elapsed       | 1.34         |\n",
            "| total_timesteps    | 12288        |\n",
            "| value_loss         | 0.18878943   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00011489005 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 982           |\n",
            "| ep_reward_mean     | -44.1         |\n",
            "| explained_variance | -0.0196       |\n",
            "| fps                | 8960          |\n",
            "| n_updates          | 4             |\n",
            "| policy_entropy     | 1.3932282     |\n",
            "| policy_loss        | -0.0005603612 |\n",
            "| serial_timesteps   | 1024          |\n",
            "| time_elapsed       | 6.77          |\n",
            "| total_timesteps    | 16384         |\n",
            "| value_loss         | 0.05970907    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-0.14 +/- 0.10\n",
            "Episode length: 999.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00014312149 |\n",
            "| clipfrac           | 6.1035156e-05 |\n",
            "| ep_len_mean        | 982           |\n",
            "| ep_reward_mean     | -44.1         |\n",
            "| explained_variance | 0.259         |\n",
            "| fps                | 766           |\n",
            "| n_updates          | 5             |\n",
            "| policy_entropy     | 1.3842179     |\n",
            "| policy_loss        | -0.0017475053 |\n",
            "| serial_timesteps   | 1280          |\n",
            "| time_elapsed       | 7.22          |\n",
            "| total_timesteps    | 20480         |\n",
            "| value_loss         | 0.041302748   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 7.780737e-05  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 982           |\n",
            "| ep_reward_mean     | -44.1         |\n",
            "| explained_variance | 0.286         |\n",
            "| fps                | 9208          |\n",
            "| n_updates          | 6             |\n",
            "| policy_entropy     | 1.3735335     |\n",
            "| policy_loss        | -0.0014714969 |\n",
            "| serial_timesteps   | 1536          |\n",
            "| time_elapsed       | 12.6          |\n",
            "| total_timesteps    | 24576         |\n",
            "| value_loss         | 0.038392603   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00038899435  |\n",
            "| clipfrac           | 0.00012207031  |\n",
            "| ep_len_mean        | 952            |\n",
            "| ep_reward_mean     | -32.9          |\n",
            "| explained_variance | 0.082          |\n",
            "| fps                | 9207           |\n",
            "| n_updates          | 7              |\n",
            "| policy_entropy     | 1.365754       |\n",
            "| policy_loss        | -0.00055942487 |\n",
            "| serial_timesteps   | 1792           |\n",
            "| time_elapsed       | 13             |\n",
            "| total_timesteps    | 28672          |\n",
            "| value_loss         | 0.23562996     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-0.49 +/- 0.28\n",
            "Episode length: 999.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00080637005 |\n",
            "| clipfrac           | 0.0034179688  |\n",
            "| ep_len_mean        | 956           |\n",
            "| ep_reward_mean     | -29.8         |\n",
            "| explained_variance | 0.178         |\n",
            "| fps                | 771           |\n",
            "| n_updates          | 8             |\n",
            "| policy_entropy     | 1.3631157     |\n",
            "| policy_loss        | -0.0013422171 |\n",
            "| serial_timesteps   | 2048          |\n",
            "| time_elapsed       | 13.5          |\n",
            "| total_timesteps    | 32768         |\n",
            "| value_loss         | 0.2717248     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00014840695  |\n",
            "| clipfrac           | 6.1035156e-05  |\n",
            "| ep_len_mean        | 956            |\n",
            "| ep_reward_mean     | -29.8          |\n",
            "| explained_variance | 0.444          |\n",
            "| fps                | 9167           |\n",
            "| n_updates          | 9              |\n",
            "| policy_entropy     | 1.3589132      |\n",
            "| policy_loss        | -0.00080389733 |\n",
            "| serial_timesteps   | 2304           |\n",
            "| time_elapsed       | 18.8           |\n",
            "| total_timesteps    | 36864          |\n",
            "| value_loss         | 0.0070542847   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=98.73 +/- 0.15\n",
            "Episode length: 562.00 +/- 125.55\n",
            "New best mean reward!\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0003834102   |\n",
            "| clipfrac           | 0.00030517578  |\n",
            "| ep_len_mean        | 906            |\n",
            "| ep_reward_mean     | -14.3          |\n",
            "| explained_variance | 0.202          |\n",
            "| fps                | 1278           |\n",
            "| n_updates          | 10             |\n",
            "| policy_entropy     | 1.3530284      |\n",
            "| policy_loss        | -0.00020252068 |\n",
            "| serial_timesteps   | 2560           |\n",
            "| time_elapsed       | 19.2           |\n",
            "| total_timesteps    | 40960          |\n",
            "| value_loss         | 0.63613397     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00015615112  |\n",
            "| clipfrac           | 6.1035156e-05  |\n",
            "| ep_len_mean        | 860            |\n",
            "| ep_reward_mean     | 1.21           |\n",
            "| explained_variance | 0.262          |\n",
            "| fps                | 9194           |\n",
            "| n_updates          | 11             |\n",
            "| policy_entropy     | 1.3513309      |\n",
            "| policy_loss        | -0.00021627017 |\n",
            "| serial_timesteps   | 2816           |\n",
            "| time_elapsed       | 22.4           |\n",
            "| total_timesteps    | 45056          |\n",
            "| value_loss         | 0.8352934      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001277488   |\n",
            "| clipfrac           | 0.010864258   |\n",
            "| ep_len_mean        | 805           |\n",
            "| ep_reward_mean     | 13            |\n",
            "| explained_variance | 0.279         |\n",
            "| fps                | 9123          |\n",
            "| n_updates          | 12            |\n",
            "| policy_entropy     | 1.3500423     |\n",
            "| policy_loss        | -0.0015151295 |\n",
            "| serial_timesteps   | 3072          |\n",
            "| time_elapsed       | 22.9          |\n",
            "| total_timesteps    | 49152         |\n",
            "| value_loss         | 0.8931359     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=98.10 +/- 0.20\n",
            "Episode length: 285.80 +/- 57.12\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00026775082  |\n",
            "| clipfrac           | 0.0004272461   |\n",
            "| ep_len_mean        | 738            |\n",
            "| ep_reward_mean     | 24.6           |\n",
            "| explained_variance | 0.31           |\n",
            "| fps                | 2202           |\n",
            "| n_updates          | 13             |\n",
            "| policy_entropy     | 1.3494128      |\n",
            "| policy_loss        | -0.00037004612 |\n",
            "| serial_timesteps   | 3328           |\n",
            "| time_elapsed       | 23.3           |\n",
            "| total_timesteps    | 53248          |\n",
            "| value_loss         | 1.1325355      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00096868444 |\n",
            "| clipfrac           | 0.006958008   |\n",
            "| ep_len_mean        | 694           |\n",
            "| ep_reward_mean     | 31.8          |\n",
            "| explained_variance | 0.321         |\n",
            "| fps                | 9017          |\n",
            "| n_updates          | 14            |\n",
            "| policy_entropy     | 1.3485408     |\n",
            "| policy_loss        | -0.0009966518 |\n",
            "| serial_timesteps   | 3584          |\n",
            "| time_elapsed       | 25.2          |\n",
            "| total_timesteps    | 57344         |\n",
            "| value_loss         | 0.92920935    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=97.36 +/- 0.50\n",
            "Episode length: 358.80 +/- 88.15\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0001269721   |\n",
            "| clipfrac           | 0.00012207031  |\n",
            "| ep_len_mean        | 653            |\n",
            "| ep_reward_mean     | 37.9           |\n",
            "| explained_variance | 0.337          |\n",
            "| fps                | 1856           |\n",
            "| n_updates          | 15             |\n",
            "| policy_entropy     | 1.3468248      |\n",
            "| policy_loss        | -5.1903844e-05 |\n",
            "| serial_timesteps   | 3840           |\n",
            "| time_elapsed       | 25.6           |\n",
            "| total_timesteps    | 61440          |\n",
            "| value_loss         | 0.90399534     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.8822865e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 566           |\n",
            "| ep_reward_mean     | 51.5          |\n",
            "| explained_variance | 0.354         |\n",
            "| fps                | 8933          |\n",
            "| n_updates          | 16            |\n",
            "| policy_entropy     | 1.3460795     |\n",
            "| policy_loss        | 0.00014231703 |\n",
            "| serial_timesteps   | 4096          |\n",
            "| time_elapsed       | 27.8          |\n",
            "| total_timesteps    | 65536         |\n",
            "| value_loss         | 1.4555706     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007535221  |\n",
            "| clipfrac           | 0.004272461   |\n",
            "| ep_len_mean        | 478           |\n",
            "| ep_reward_mean     | 66            |\n",
            "| explained_variance | 0.38          |\n",
            "| fps                | 9195          |\n",
            "| n_updates          | 17            |\n",
            "| policy_entropy     | 1.3458685     |\n",
            "| policy_loss        | -0.0011037786 |\n",
            "| serial_timesteps   | 4352          |\n",
            "| time_elapsed       | 28.3          |\n",
            "| total_timesteps    | 69632         |\n",
            "| value_loss         | 1.0541351     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=96.59 +/- 0.48\n",
            "Episode length: 246.80 +/- 55.62\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016596606  |\n",
            "| clipfrac           | 0.016967773   |\n",
            "| ep_len_mean        | 372           |\n",
            "| ep_reward_mean     | 81.3          |\n",
            "| explained_variance | 0.382         |\n",
            "| fps                | 2454          |\n",
            "| n_updates          | 18            |\n",
            "| policy_entropy     | 1.3445667     |\n",
            "| policy_loss        | -0.0026447587 |\n",
            "| serial_timesteps   | 4608          |\n",
            "| time_elapsed       | 28.7          |\n",
            "| total_timesteps    | 73728         |\n",
            "| value_loss         | 1.400949      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00010847484 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 312           |\n",
            "| ep_reward_mean     | 84.1          |\n",
            "| explained_variance | 0.398         |\n",
            "| fps                | 8964          |\n",
            "| n_updates          | 19            |\n",
            "| policy_entropy     | 1.3427347     |\n",
            "| policy_loss        | 0.00021173581 |\n",
            "| serial_timesteps   | 4864          |\n",
            "| time_elapsed       | 30.4          |\n",
            "| total_timesteps    | 77824         |\n",
            "| value_loss         | 1.3197522     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=95.78 +/- 0.52\n",
            "Episode length: 168.20 +/- 27.00\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00035537797  |\n",
            "| clipfrac           | 0.00012207031  |\n",
            "| ep_len_mean        | 284            |\n",
            "| ep_reward_mean     | 85.4           |\n",
            "| explained_variance | 0.418          |\n",
            "| fps                | 3235           |\n",
            "| n_updates          | 20             |\n",
            "| policy_entropy     | 1.3413336      |\n",
            "| policy_loss        | -0.00029151462 |\n",
            "| serial_timesteps   | 5120           |\n",
            "| time_elapsed       | 30.9           |\n",
            "| total_timesteps    | 81920          |\n",
            "| value_loss         | 1.1617074      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013768872  |\n",
            "| clipfrac           | 0.009033203   |\n",
            "| ep_len_mean        | 256           |\n",
            "| ep_reward_mean     | 86.5          |\n",
            "| explained_variance | 0.441         |\n",
            "| fps                | 9147          |\n",
            "| n_updates          | 21            |\n",
            "| policy_entropy     | 1.3403745     |\n",
            "| policy_loss        | -0.0017986721 |\n",
            "| serial_timesteps   | 5376          |\n",
            "| time_elapsed       | 32.1          |\n",
            "| total_timesteps    | 86016         |\n",
            "| value_loss         | 1.2866377     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=95.09 +/- 0.62\n",
            "Episode length: 172.60 +/- 65.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00088767963 |\n",
            "| clipfrac           | 0.0041503906  |\n",
            "| ep_len_mean        | 240           |\n",
            "| ep_reward_mean     | 87.2          |\n",
            "| explained_variance | 0.447         |\n",
            "| fps                | 3178          |\n",
            "| n_updates          | 22            |\n",
            "| policy_entropy     | 1.3390881     |\n",
            "| policy_loss        | -0.0017364244 |\n",
            "| serial_timesteps   | 5632          |\n",
            "| time_elapsed       | 32.6          |\n",
            "| total_timesteps    | 90112         |\n",
            "| value_loss         | 1.3449855     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.234688e-05   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 225            |\n",
            "| ep_reward_mean     | 87.8           |\n",
            "| explained_variance | 0.481          |\n",
            "| fps                | 9135           |\n",
            "| n_updates          | 23             |\n",
            "| policy_entropy     | 1.3370923      |\n",
            "| policy_loss        | -5.6136094e-05 |\n",
            "| serial_timesteps   | 5888           |\n",
            "| time_elapsed       | 33.9           |\n",
            "| total_timesteps    | 94208          |\n",
            "| value_loss         | 1.3086979      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0001582752   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 220            |\n",
            "| ep_reward_mean     | 87.9           |\n",
            "| explained_variance | 0.492          |\n",
            "| fps                | 9032           |\n",
            "| n_updates          | 24             |\n",
            "| policy_entropy     | 1.3352418      |\n",
            "| policy_loss        | -0.00024079271 |\n",
            "| serial_timesteps   | 6144           |\n",
            "| time_elapsed       | 34.3           |\n",
            "| total_timesteps    | 98304          |\n",
            "| value_loss         | 0.8511597      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=94.60 +/- 0.83\n",
            "Episode length: 166.00 +/- 40.70\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00029419118  |\n",
            "| clipfrac           | 0.00036621094  |\n",
            "| ep_len_mean        | 216            |\n",
            "| ep_reward_mean     | 88             |\n",
            "| explained_variance | 0.508          |\n",
            "| fps                | 3262           |\n",
            "| n_updates          | 25             |\n",
            "| policy_entropy     | 1.3333396      |\n",
            "| policy_loss        | -0.00034853638 |\n",
            "| serial_timesteps   | 6400           |\n",
            "| time_elapsed       | 34.8           |\n",
            "| total_timesteps    | 102400         |\n",
            "| value_loss         | 1.2811728      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013951753  |\n",
            "| clipfrac           | 0.010131836   |\n",
            "| ep_len_mean        | 209           |\n",
            "| ep_reward_mean     | 88.3          |\n",
            "| explained_variance | 0.523         |\n",
            "| fps                | 9056          |\n",
            "| n_updates          | 26            |\n",
            "| policy_entropy     | 1.3321781     |\n",
            "| policy_loss        | -0.0019389394 |\n",
            "| serial_timesteps   | 6656          |\n",
            "| time_elapsed       | 36            |\n",
            "| total_timesteps    | 106496        |\n",
            "| value_loss         | 1.1145148     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=93.13 +/- 0.58\n",
            "Episode length: 167.80 +/- 39.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0009684791  |\n",
            "| clipfrac           | 0.006286621   |\n",
            "| ep_len_mean        | 209           |\n",
            "| ep_reward_mean     | 88.3          |\n",
            "| explained_variance | 0.553         |\n",
            "| fps                | 3248          |\n",
            "| n_updates          | 27            |\n",
            "| policy_entropy     | 1.3297985     |\n",
            "| policy_loss        | -0.0009089707 |\n",
            "| serial_timesteps   | 6912          |\n",
            "| time_elapsed       | 36.5          |\n",
            "| total_timesteps    | 110592        |\n",
            "| value_loss         | 0.991483      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00022482661  |\n",
            "| clipfrac           | 0.00012207031  |\n",
            "| ep_len_mean        | 206            |\n",
            "| ep_reward_mean     | 88.3           |\n",
            "| explained_variance | 0.534          |\n",
            "| fps                | 9191           |\n",
            "| n_updates          | 28             |\n",
            "| policy_entropy     | 1.3277144      |\n",
            "| policy_loss        | -0.00025423497 |\n",
            "| serial_timesteps   | 7168           |\n",
            "| time_elapsed       | 37.7           |\n",
            "| total_timesteps    | 114688         |\n",
            "| value_loss         | 1.1957119      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00064609305  |\n",
            "| clipfrac           | 0.0012207031   |\n",
            "| ep_len_mean        | 196            |\n",
            "| ep_reward_mean     | 88.7           |\n",
            "| explained_variance | 0.578          |\n",
            "| fps                | 9099           |\n",
            "| n_updates          | 29             |\n",
            "| policy_entropy     | 1.326226       |\n",
            "| policy_loss        | -0.00033477746 |\n",
            "| serial_timesteps   | 7424           |\n",
            "| time_elapsed       | 38.2           |\n",
            "| total_timesteps    | 118784         |\n",
            "| value_loss         | 1.1974596      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=91.89 +/- 0.88\n",
            "Episode length: 153.60 +/- 25.22\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014385049  |\n",
            "| clipfrac           | 0.011413574   |\n",
            "| ep_len_mean        | 184           |\n",
            "| ep_reward_mean     | 89.1          |\n",
            "| explained_variance | 0.554         |\n",
            "| fps                | 3404          |\n",
            "| n_updates          | 30            |\n",
            "| policy_entropy     | 1.3257326     |\n",
            "| policy_loss        | -0.0013982106 |\n",
            "| serial_timesteps   | 7680          |\n",
            "| time_elapsed       | 38.6          |\n",
            "| total_timesteps    | 122880        |\n",
            "| value_loss         | 1.1093516     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013921428  |\n",
            "| clipfrac           | 0.008728027   |\n",
            "| ep_len_mean        | 171           |\n",
            "| ep_reward_mean     | 89.7          |\n",
            "| explained_variance | 0.562         |\n",
            "| fps                | 9098          |\n",
            "| n_updates          | 31            |\n",
            "| policy_entropy     | 1.3243852     |\n",
            "| policy_loss        | -0.0010323271 |\n",
            "| serial_timesteps   | 7936          |\n",
            "| time_elapsed       | 39.8          |\n",
            "| total_timesteps    | 126976        |\n",
            "| value_loss         | 1.1970359     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=92.53 +/- 0.57\n",
            "Episode length: 125.00 +/- 5.33\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00027550213  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 166            |\n",
            "| ep_reward_mean     | 89.9           |\n",
            "| explained_variance | 0.61           |\n",
            "| fps                | 3817           |\n",
            "| n_updates          | 32             |\n",
            "| policy_entropy     | 1.3224286      |\n",
            "| policy_loss        | -0.00010743344 |\n",
            "| serial_timesteps   | 8192           |\n",
            "| time_elapsed       | 40.3           |\n",
            "| total_timesteps    | 131072         |\n",
            "| value_loss         | 0.87541014     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0012554618 |\n",
            "| clipfrac           | 0.007873535  |\n",
            "| ep_len_mean        | 159          |\n",
            "| ep_reward_mean     | 90.3         |\n",
            "| explained_variance | 0.596        |\n",
            "| fps                | 9232         |\n",
            "| n_updates          | 33           |\n",
            "| policy_entropy     | 1.3207898    |\n",
            "| policy_loss        | -0.001183795 |\n",
            "| serial_timesteps   | 8448         |\n",
            "| time_elapsed       | 41.4         |\n",
            "| total_timesteps    | 135168       |\n",
            "| value_loss         | 1.0733858    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0012545856  |\n",
            "| clipfrac           | 0.0040893555  |\n",
            "| ep_len_mean        | 161           |\n",
            "| ep_reward_mean     | 90            |\n",
            "| explained_variance | 0.627         |\n",
            "| fps                | 9133          |\n",
            "| n_updates          | 34            |\n",
            "| policy_entropy     | 1.3193638     |\n",
            "| policy_loss        | -0.0013008668 |\n",
            "| serial_timesteps   | 8704          |\n",
            "| time_elapsed       | 41.8          |\n",
            "| total_timesteps    | 139264        |\n",
            "| value_loss         | 1.0090857     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=91.26 +/- 1.38\n",
            "Episode length: 161.40 +/- 44.01\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0005931215  |\n",
            "| clipfrac           | 0.0020751953  |\n",
            "| ep_len_mean        | 159           |\n",
            "| ep_reward_mean     | 90.1          |\n",
            "| explained_variance | 0.66          |\n",
            "| fps                | 3293          |\n",
            "| n_updates          | 35            |\n",
            "| policy_entropy     | 1.316557      |\n",
            "| policy_loss        | -0.0004459348 |\n",
            "| serial_timesteps   | 8960          |\n",
            "| time_elapsed       | 42.3          |\n",
            "| total_timesteps    | 143360        |\n",
            "| value_loss         | 0.944732      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0012791159  |\n",
            "| clipfrac           | 0.009460449   |\n",
            "| ep_len_mean        | 157           |\n",
            "| ep_reward_mean     | 90.1          |\n",
            "| explained_variance | 0.655         |\n",
            "| fps                | 9152          |\n",
            "| n_updates          | 36            |\n",
            "| policy_entropy     | 1.3153255     |\n",
            "| policy_loss        | -0.0012792717 |\n",
            "| serial_timesteps   | 9216          |\n",
            "| time_elapsed       | 43.5          |\n",
            "| total_timesteps    | 147456        |\n",
            "| value_loss         | 0.8744713     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=91.46 +/- 1.83\n",
            "Episode length: 146.00 +/- 42.30\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017354793  |\n",
            "| clipfrac           | 0.017089844   |\n",
            "| ep_len_mean        | 154           |\n",
            "| ep_reward_mean     | 90.3          |\n",
            "| explained_variance | 0.66          |\n",
            "| fps                | 3458          |\n",
            "| n_updates          | 37            |\n",
            "| policy_entropy     | 1.314505      |\n",
            "| policy_loss        | -0.0018566981 |\n",
            "| serial_timesteps   | 9472          |\n",
            "| time_elapsed       | 44            |\n",
            "| total_timesteps    | 151552        |\n",
            "| value_loss         | 0.7961665     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0010204143   |\n",
            "| clipfrac           | 0.0022583008   |\n",
            "| ep_len_mean        | 149            |\n",
            "| ep_reward_mean     | 90.5           |\n",
            "| explained_variance | 0.697          |\n",
            "| fps                | 9037           |\n",
            "| n_updates          | 38             |\n",
            "| policy_entropy     | 1.3138628      |\n",
            "| policy_loss        | -0.00085733266 |\n",
            "| serial_timesteps   | 9728           |\n",
            "| time_elapsed       | 45.1           |\n",
            "| total_timesteps    | 155648         |\n",
            "| value_loss         | 0.80199033     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000732605   |\n",
            "| clipfrac           | 0.0026245117  |\n",
            "| ep_len_mean        | 146           |\n",
            "| ep_reward_mean     | 90.6          |\n",
            "| explained_variance | 0.69          |\n",
            "| fps                | 9318          |\n",
            "| n_updates          | 39            |\n",
            "| policy_entropy     | 1.3130946     |\n",
            "| policy_loss        | -0.0007523696 |\n",
            "| serial_timesteps   | 9984          |\n",
            "| time_elapsed       | 45.6          |\n",
            "| total_timesteps    | 159744        |\n",
            "| value_loss         | 0.7425685     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=91.45 +/- 0.65\n",
            "Episode length: 122.00 +/- 7.01\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00078791135  |\n",
            "| clipfrac           | 0.0013427734   |\n",
            "| ep_len_mean        | 146            |\n",
            "| ep_reward_mean     | 90.5           |\n",
            "| explained_variance | 0.714          |\n",
            "| fps                | 3836           |\n",
            "| n_updates          | 40             |\n",
            "| policy_entropy     | 1.3115542      |\n",
            "| policy_loss        | -0.00039434282 |\n",
            "| serial_timesteps   | 10240          |\n",
            "| time_elapsed       | 46             |\n",
            "| total_timesteps    | 163840         |\n",
            "| value_loss         | 0.7103055      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014954954  |\n",
            "| clipfrac           | 0.0099487305  |\n",
            "| ep_len_mean        | 148           |\n",
            "| ep_reward_mean     | 90.5          |\n",
            "| explained_variance | 0.721         |\n",
            "| fps                | 9075          |\n",
            "| n_updates          | 41            |\n",
            "| policy_entropy     | 1.310074      |\n",
            "| policy_loss        | -0.0012003073 |\n",
            "| serial_timesteps   | 10496         |\n",
            "| time_elapsed       | 47.1          |\n",
            "| total_timesteps    | 167936        |\n",
            "| value_loss         | 0.59745306    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=91.38 +/- 0.37\n",
            "Episode length: 132.80 +/- 23.61\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00052760215  |\n",
            "| clipfrac           | 0.0021972656   |\n",
            "| ep_len_mean        | 152            |\n",
            "| ep_reward_mean     | 90.3           |\n",
            "| explained_variance | 0.706          |\n",
            "| fps                | 3661           |\n",
            "| n_updates          | 42             |\n",
            "| policy_entropy     | 1.3083035      |\n",
            "| policy_loss        | -0.00039695576 |\n",
            "| serial_timesteps   | 10752          |\n",
            "| time_elapsed       | 47.6           |\n",
            "| total_timesteps    | 172032         |\n",
            "| value_loss         | 0.6525627      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002343637   |\n",
            "| clipfrac           | 0.020080566   |\n",
            "| ep_len_mean        | 153           |\n",
            "| ep_reward_mean     | 90.2          |\n",
            "| explained_variance | 0.763         |\n",
            "| fps                | 9267          |\n",
            "| n_updates          | 43            |\n",
            "| policy_entropy     | 1.3070252     |\n",
            "| policy_loss        | -0.0022955183 |\n",
            "| serial_timesteps   | 11008         |\n",
            "| time_elapsed       | 48.7          |\n",
            "| total_timesteps    | 176128        |\n",
            "| value_loss         | 0.6126123     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=91.00 +/- 0.38\n",
            "Episode length: 139.00 +/- 23.09\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00092276244 |\n",
            "| clipfrac           | 0.0043945312  |\n",
            "| ep_len_mean        | 150           |\n",
            "| ep_reward_mean     | 90.3          |\n",
            "| explained_variance | 0.778         |\n",
            "| fps                | 3599          |\n",
            "| n_updates          | 44            |\n",
            "| policy_entropy     | 1.3044404     |\n",
            "| policy_loss        | -0.0006239482 |\n",
            "| serial_timesteps   | 11264         |\n",
            "| time_elapsed       | 49.1          |\n",
            "| total_timesteps    | 180224        |\n",
            "| value_loss         | 0.5265067     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0009445493   |\n",
            "| clipfrac           | 0.0060424805   |\n",
            "| ep_len_mean        | 145            |\n",
            "| ep_reward_mean     | 90.4           |\n",
            "| explained_variance | 0.801          |\n",
            "| fps                | 9083           |\n",
            "| n_updates          | 45             |\n",
            "| policy_entropy     | 1.303377       |\n",
            "| policy_loss        | -0.00055867864 |\n",
            "| serial_timesteps   | 11520          |\n",
            "| time_elapsed       | 50.3           |\n",
            "| total_timesteps    | 184320         |\n",
            "| value_loss         | 0.5156635      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00041096765 |\n",
            "| clipfrac           | 0.0006713867  |\n",
            "| ep_len_mean        | 140           |\n",
            "| ep_reward_mean     | 90.6          |\n",
            "| explained_variance | 0.805         |\n",
            "| fps                | 8881          |\n",
            "| n_updates          | 46            |\n",
            "| policy_entropy     | 1.302605      |\n",
            "| policy_loss        | -0.00026799   |\n",
            "| serial_timesteps   | 11776         |\n",
            "| time_elapsed       | 50.7          |\n",
            "| total_timesteps    | 188416        |\n",
            "| value_loss         | 0.49115652    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=90.67 +/- 0.63\n",
            "Episode length: 128.00 +/- 16.52\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0005736157   |\n",
            "| clipfrac           | 0.0008544922   |\n",
            "| ep_len_mean        | 137            |\n",
            "| ep_reward_mean     | 90.6           |\n",
            "| explained_variance | 0.847          |\n",
            "| fps                | 3720           |\n",
            "| n_updates          | 47             |\n",
            "| policy_entropy     | 1.3015773      |\n",
            "| policy_loss        | -0.00034689653 |\n",
            "| serial_timesteps   | 12032          |\n",
            "| time_elapsed       | 51.2           |\n",
            "| total_timesteps    | 192512         |\n",
            "| value_loss         | 0.44819415     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020203863  |\n",
            "| clipfrac           | 0.014404297   |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | 90.7          |\n",
            "| explained_variance | 0.857         |\n",
            "| fps                | 9124          |\n",
            "| n_updates          | 48            |\n",
            "| policy_entropy     | 1.2999446     |\n",
            "| policy_loss        | -0.0014070398 |\n",
            "| serial_timesteps   | 12288         |\n",
            "| time_elapsed       | 52.3          |\n",
            "| total_timesteps    | 196608        |\n",
            "| value_loss         | 0.36292106    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=90.18 +/- 0.08\n",
            "Episode length: 119.60 +/- 3.07\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014598289  |\n",
            "| clipfrac           | 0.014404297   |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | 90.5          |\n",
            "| explained_variance | 0.876         |\n",
            "| fps                | 3946          |\n",
            "| n_updates          | 49            |\n",
            "| policy_entropy     | 1.2980978     |\n",
            "| policy_loss        | -0.0014909082 |\n",
            "| serial_timesteps   | 12544         |\n",
            "| time_elapsed       | 52.7          |\n",
            "| total_timesteps    | 200704        |\n",
            "| value_loss         | 0.35852858    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010870256  |\n",
            "| clipfrac           | 0.007873535   |\n",
            "| ep_len_mean        | 131           |\n",
            "| ep_reward_mean     | 90.7          |\n",
            "| explained_variance | 0.879         |\n",
            "| fps                | 8938          |\n",
            "| n_updates          | 50            |\n",
            "| policy_entropy     | 1.2989712     |\n",
            "| policy_loss        | -0.0006958501 |\n",
            "| serial_timesteps   | 12800         |\n",
            "| time_elapsed       | 53.8          |\n",
            "| total_timesteps    | 204800        |\n",
            "| value_loss         | 0.34317034    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011052471  |\n",
            "| clipfrac           | 0.0071411133  |\n",
            "| ep_len_mean        | 127           |\n",
            "| ep_reward_mean     | 90.9          |\n",
            "| explained_variance | 0.85          |\n",
            "| fps                | 9150          |\n",
            "| n_updates          | 51            |\n",
            "| policy_entropy     | 1.2988592     |\n",
            "| policy_loss        | -0.0012025639 |\n",
            "| serial_timesteps   | 13056         |\n",
            "| time_elapsed       | 54.2          |\n",
            "| total_timesteps    | 208896        |\n",
            "| value_loss         | 0.4104728     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=90.46 +/- 0.18\n",
            "Episode length: 135.40 +/- 27.46\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019317223  |\n",
            "| clipfrac           | 0.016174316   |\n",
            "| ep_len_mean        | 125           |\n",
            "| ep_reward_mean     | 90.9          |\n",
            "| explained_variance | 0.897         |\n",
            "| fps                | 3639          |\n",
            "| n_updates          | 52            |\n",
            "| policy_entropy     | 1.2982085     |\n",
            "| policy_loss        | -0.0016681437 |\n",
            "| serial_timesteps   | 13312         |\n",
            "| time_elapsed       | 54.7          |\n",
            "| total_timesteps    | 212992        |\n",
            "| value_loss         | 0.29610124    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00046486993 |\n",
            "| clipfrac           | 0.001159668   |\n",
            "| ep_len_mean        | 122           |\n",
            "| ep_reward_mean     | 91.2          |\n",
            "| explained_variance | 0.868         |\n",
            "| fps                | 9157          |\n",
            "| n_updates          | 53            |\n",
            "| policy_entropy     | 1.296438      |\n",
            "| policy_loss        | -0.0005712336 |\n",
            "| serial_timesteps   | 13568         |\n",
            "| time_elapsed       | 55.8          |\n",
            "| total_timesteps    | 217088        |\n",
            "| value_loss         | 0.34948385    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=91.62 +/- 1.36\n",
            "Episode length: 112.00 +/- 1.67\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0021498266 |\n",
            "| clipfrac           | 0.020202637  |\n",
            "| ep_len_mean        | 120          |\n",
            "| ep_reward_mean     | 91.3         |\n",
            "| explained_variance | 0.858        |\n",
            "| fps                | 4097         |\n",
            "| n_updates          | 54           |\n",
            "| policy_entropy     | 1.2923168    |\n",
            "| policy_loss        | -0.001398867 |\n",
            "| serial_timesteps   | 13824        |\n",
            "| time_elapsed       | 56.2         |\n",
            "| total_timesteps    | 221184       |\n",
            "| value_loss         | 0.36559907   |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00020889383  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 119            |\n",
            "| ep_reward_mean     | 91.4           |\n",
            "| explained_variance | 0.882          |\n",
            "| fps                | 8887           |\n",
            "| n_updates          | 55             |\n",
            "| policy_entropy     | 1.2883958      |\n",
            "| policy_loss        | -0.00027590455 |\n",
            "| serial_timesteps   | 14080          |\n",
            "| time_elapsed       | 57.2           |\n",
            "| total_timesteps    | 225280         |\n",
            "| value_loss         | 0.3038763      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0031217833  |\n",
            "| clipfrac           | 0.034179688   |\n",
            "| ep_len_mean        | 114           |\n",
            "| ep_reward_mean     | 91.6          |\n",
            "| explained_variance | 0.911         |\n",
            "| fps                | 9172          |\n",
            "| n_updates          | 56            |\n",
            "| policy_entropy     | 1.2867789     |\n",
            "| policy_loss        | -0.0027657344 |\n",
            "| serial_timesteps   | 14336         |\n",
            "| time_elapsed       | 57.7          |\n",
            "| total_timesteps    | 229376        |\n",
            "| value_loss         | 0.24845307    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=92.05 +/- 1.37\n",
            "Episode length: 101.40 +/- 13.17\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010592624  |\n",
            "| clipfrac           | 0.0068359375  |\n",
            "| ep_len_mean        | 109           |\n",
            "| ep_reward_mean     | 92            |\n",
            "| explained_variance | 0.901         |\n",
            "| fps                | 4243          |\n",
            "| n_updates          | 57            |\n",
            "| policy_entropy     | 1.2847216     |\n",
            "| policy_loss        | -0.0010236755 |\n",
            "| serial_timesteps   | 14592         |\n",
            "| time_elapsed       | 58.1          |\n",
            "| total_timesteps    | 233472        |\n",
            "| value_loss         | 0.23346923    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007369584  |\n",
            "| clipfrac           | 0.0045166016  |\n",
            "| ep_len_mean        | 106           |\n",
            "| ep_reward_mean     | 92.1          |\n",
            "| explained_variance | 0.909         |\n",
            "| fps                | 8568          |\n",
            "| n_updates          | 58            |\n",
            "| policy_entropy     | 1.2818439     |\n",
            "| policy_loss        | -0.0009168299 |\n",
            "| serial_timesteps   | 14848         |\n",
            "| time_elapsed       | 59.1          |\n",
            "| total_timesteps    | 237568        |\n",
            "| value_loss         | 0.23323779    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=93.46 +/- 0.31\n",
            "Episode length: 86.20 +/- 11.91\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0035313356 |\n",
            "| clipfrac           | 0.037109375  |\n",
            "| ep_len_mean        | 103          |\n",
            "| ep_reward_mean     | 92.3         |\n",
            "| explained_variance | 0.921        |\n",
            "| fps                | 4646         |\n",
            "| n_updates          | 59           |\n",
            "| policy_entropy     | 1.2782278    |\n",
            "| policy_loss        | -0.002642808 |\n",
            "| serial_timesteps   | 15104        |\n",
            "| time_elapsed       | 59.6         |\n",
            "| total_timesteps    | 241664       |\n",
            "| value_loss         | 0.1853612    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0025692754 |\n",
            "| clipfrac           | 0.02130127   |\n",
            "| ep_len_mean        | 95.3         |\n",
            "| ep_reward_mean     | 92.7         |\n",
            "| explained_variance | 0.902        |\n",
            "| fps                | 9259         |\n",
            "| n_updates          | 60           |\n",
            "| policy_entropy     | 1.2753567    |\n",
            "| policy_loss        | -0.00263962  |\n",
            "| serial_timesteps   | 15360        |\n",
            "| time_elapsed       | 60.5         |\n",
            "| total_timesteps    | 245760       |\n",
            "| value_loss         | 0.22634219   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016481538  |\n",
            "| clipfrac           | 0.0072021484  |\n",
            "| ep_len_mean        | 93.7          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.935         |\n",
            "| fps                | 8993          |\n",
            "| n_updates          | 61            |\n",
            "| policy_entropy     | 1.272197      |\n",
            "| policy_loss        | -0.0011460261 |\n",
            "| serial_timesteps   | 15616         |\n",
            "| time_elapsed       | 60.9          |\n",
            "| total_timesteps    | 249856        |\n",
            "| value_loss         | 0.14550707    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=92.07 +/- 0.92\n",
            "Episode length: 96.20 +/- 17.41\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0027855346  |\n",
            "| clipfrac           | 0.027893066   |\n",
            "| ep_len_mean        | 94            |\n",
            "| ep_reward_mean     | 92.7          |\n",
            "| explained_variance | 0.923         |\n",
            "| fps                | 4331          |\n",
            "| n_updates          | 62            |\n",
            "| policy_entropy     | 1.2710639     |\n",
            "| policy_loss        | -0.0021899808 |\n",
            "| serial_timesteps   | 15872         |\n",
            "| time_elapsed       | 61.4          |\n",
            "| total_timesteps    | 253952        |\n",
            "| value_loss         | 0.17355551    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020051207  |\n",
            "| clipfrac           | 0.015197754   |\n",
            "| ep_len_mean        | 91.6          |\n",
            "| ep_reward_mean     | 92.6          |\n",
            "| explained_variance | 0.95          |\n",
            "| fps                | 8925          |\n",
            "| n_updates          | 63            |\n",
            "| policy_entropy     | 1.2693342     |\n",
            "| policy_loss        | -0.0011152201 |\n",
            "| serial_timesteps   | 16128         |\n",
            "| time_elapsed       | 62.3          |\n",
            "| total_timesteps    | 258048        |\n",
            "| value_loss         | 0.105210625   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=92.96 +/- 0.05\n",
            "Episode length: 81.80 +/- 1.94\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0039767576  |\n",
            "| clipfrac           | 0.03967285    |\n",
            "| ep_len_mean        | 89.5          |\n",
            "| ep_reward_mean     | 92.7          |\n",
            "| explained_variance | 0.958         |\n",
            "| fps                | 4770          |\n",
            "| n_updates          | 64            |\n",
            "| policy_entropy     | 1.2652084     |\n",
            "| policy_loss        | -0.0028530678 |\n",
            "| serial_timesteps   | 16384         |\n",
            "| time_elapsed       | 62.8          |\n",
            "| total_timesteps    | 262144        |\n",
            "| value_loss         | 0.08784507    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017880815  |\n",
            "| clipfrac           | 0.0146484375  |\n",
            "| ep_len_mean        | 86.6          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.962         |\n",
            "| fps                | 8796          |\n",
            "| n_updates          | 65            |\n",
            "| policy_entropy     | 1.2613373     |\n",
            "| policy_loss        | -0.0021385686 |\n",
            "| serial_timesteps   | 16640         |\n",
            "| time_elapsed       | 63.6          |\n",
            "| total_timesteps    | 266240        |\n",
            "| value_loss         | 0.07482786    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=92.74 +/- 0.08\n",
            "Episode length: 79.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0024177853  |\n",
            "| clipfrac           | 0.021118164   |\n",
            "| ep_len_mean        | 83.8          |\n",
            "| ep_reward_mean     | 92.9          |\n",
            "| explained_variance | 0.978         |\n",
            "| fps                | 4784          |\n",
            "| n_updates          | 66            |\n",
            "| policy_entropy     | 1.2595257     |\n",
            "| policy_loss        | -0.0022521242 |\n",
            "| serial_timesteps   | 16896         |\n",
            "| time_elapsed       | 64.1          |\n",
            "| total_timesteps    | 270336        |\n",
            "| value_loss         | 0.042821314   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016304394  |\n",
            "| clipfrac           | 0.010925293   |\n",
            "| ep_len_mean        | 83.6          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.962         |\n",
            "| fps                | 9141          |\n",
            "| n_updates          | 67            |\n",
            "| policy_entropy     | 1.2556022     |\n",
            "| policy_loss        | -0.0007689992 |\n",
            "| serial_timesteps   | 17152         |\n",
            "| time_elapsed       | 65            |\n",
            "| total_timesteps    | 274432        |\n",
            "| value_loss         | 0.07079701    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00093613303 |\n",
            "| clipfrac           | 0.004333496   |\n",
            "| ep_len_mean        | 84.7          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.957         |\n",
            "| fps                | 9058          |\n",
            "| n_updates          | 68            |\n",
            "| policy_entropy     | 1.2530931     |\n",
            "| policy_loss        | -0.0010429104 |\n",
            "| serial_timesteps   | 17408         |\n",
            "| time_elapsed       | 65.4          |\n",
            "| total_timesteps    | 278528        |\n",
            "| value_loss         | 0.07533977    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=92.64 +/- 0.16\n",
            "Episode length: 79.40 +/- 1.50\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0024895533   |\n",
            "| clipfrac           | 0.016174316    |\n",
            "| ep_len_mean        | 83.2           |\n",
            "| ep_reward_mean     | 92.9           |\n",
            "| explained_variance | 0.965          |\n",
            "| fps                | 4801           |\n",
            "| n_updates          | 69             |\n",
            "| policy_entropy     | 1.2524825      |\n",
            "| policy_loss        | -0.00096313556 |\n",
            "| serial_timesteps   | 17664          |\n",
            "| time_elapsed       | 65.9           |\n",
            "| total_timesteps    | 282624         |\n",
            "| value_loss         | 0.060762584    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010619557  |\n",
            "| clipfrac           | 0.008911133   |\n",
            "| ep_len_mean        | 81.5          |\n",
            "| ep_reward_mean     | 92.9          |\n",
            "| explained_variance | 0.979         |\n",
            "| fps                | 8804          |\n",
            "| n_updates          | 70            |\n",
            "| policy_entropy     | 1.2504092     |\n",
            "| policy_loss        | -0.0012954067 |\n",
            "| serial_timesteps   | 17920         |\n",
            "| time_elapsed       | 66.7          |\n",
            "| total_timesteps    | 286720        |\n",
            "| value_loss         | 0.036569845   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=92.73 +/- 0.12\n",
            "Episode length: 77.40 +/- 1.02\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0023094993   |\n",
            "| clipfrac           | 0.017578125    |\n",
            "| ep_len_mean        | 81.4           |\n",
            "| ep_reward_mean     | 92.8           |\n",
            "| explained_variance | 0.974          |\n",
            "| fps                | 4902           |\n",
            "| n_updates          | 71             |\n",
            "| policy_entropy     | 1.2466652      |\n",
            "| policy_loss        | -0.00069468934 |\n",
            "| serial_timesteps   | 18176          |\n",
            "| time_elapsed       | 67.2           |\n",
            "| total_timesteps    | 290816         |\n",
            "| value_loss         | 0.046455227    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021658724  |\n",
            "| clipfrac           | 0.021606445   |\n",
            "| ep_len_mean        | 81.3          |\n",
            "| ep_reward_mean     | 92.8          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 8885          |\n",
            "| n_updates          | 72            |\n",
            "| policy_entropy     | 1.2408988     |\n",
            "| policy_loss        | -0.0018771308 |\n",
            "| serial_timesteps   | 18432         |\n",
            "| time_elapsed       | 68            |\n",
            "| total_timesteps    | 294912        |\n",
            "| value_loss         | 0.01836875    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0029891892 |\n",
            "| clipfrac           | 0.028076172  |\n",
            "| ep_len_mean        | 79.8         |\n",
            "| ep_reward_mean     | 92.9         |\n",
            "| explained_variance | 0.984        |\n",
            "| fps                | 9051         |\n",
            "| n_updates          | 73           |\n",
            "| policy_entropy     | 1.2362334    |\n",
            "| policy_loss        | -0.001988478 |\n",
            "| serial_timesteps   | 18688        |\n",
            "| time_elapsed       | 68.5         |\n",
            "| total_timesteps    | 299008       |\n",
            "| value_loss         | 0.026266124  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=92.67 +/- 0.16\n",
            "Episode length: 78.20 +/- 1.60\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018653179  |\n",
            "| clipfrac           | 0.0119018555  |\n",
            "| ep_len_mean        | 79.1          |\n",
            "| ep_reward_mean     | 92.9          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 4737          |\n",
            "| n_updates          | 74            |\n",
            "| policy_entropy     | 1.2340086     |\n",
            "| policy_loss        | -0.0014214541 |\n",
            "| serial_timesteps   | 18944         |\n",
            "| time_elapsed       | 68.9          |\n",
            "| total_timesteps    | 303104        |\n",
            "| value_loss         | 0.020808853   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020707042  |\n",
            "| clipfrac           | 0.017089844   |\n",
            "| ep_len_mean        | 78.9          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.981         |\n",
            "| fps                | 9161          |\n",
            "| n_updates          | 75            |\n",
            "| policy_entropy     | 1.2310703     |\n",
            "| policy_loss        | -0.0018013336 |\n",
            "| serial_timesteps   | 19200         |\n",
            "| time_elapsed       | 69.8          |\n",
            "| total_timesteps    | 307200        |\n",
            "| value_loss         | 0.030380182   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=92.84 +/- 0.06\n",
            "Episode length: 76.40 +/- 0.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0024610725 |\n",
            "| clipfrac           | 0.021362305  |\n",
            "| ep_len_mean        | 79.4         |\n",
            "| ep_reward_mean     | 93           |\n",
            "| explained_variance | 0.984        |\n",
            "| fps                | 4865         |\n",
            "| n_updates          | 76           |\n",
            "| policy_entropy     | 1.2263135    |\n",
            "| policy_loss        | -0.002018512 |\n",
            "| serial_timesteps   | 19456        |\n",
            "| time_elapsed       | 70.2         |\n",
            "| total_timesteps    | 311296       |\n",
            "| value_loss         | 0.024690459  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021503903  |\n",
            "| clipfrac           | 0.018981934   |\n",
            "| ep_len_mean        | 78.2          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 8608          |\n",
            "| n_updates          | 77            |\n",
            "| policy_entropy     | 1.2193449     |\n",
            "| policy_loss        | -0.0015503656 |\n",
            "| serial_timesteps   | 19712         |\n",
            "| time_elapsed       | 71.1          |\n",
            "| total_timesteps    | 315392        |\n",
            "| value_loss         | 0.014034519   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020209407  |\n",
            "| clipfrac           | 0.0154418945  |\n",
            "| ep_len_mean        | 77.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.989         |\n",
            "| fps                | 9064          |\n",
            "| n_updates          | 78            |\n",
            "| policy_entropy     | 1.2130351     |\n",
            "| policy_loss        | -0.0011855318 |\n",
            "| serial_timesteps   | 19968         |\n",
            "| time_elapsed       | 71.6          |\n",
            "| total_timesteps    | 319488        |\n",
            "| value_loss         | 0.017359262   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=92.84 +/- 0.16\n",
            "Episode length: 76.00 +/- 1.55\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0029826385  |\n",
            "| clipfrac           | 0.03112793    |\n",
            "| ep_len_mean        | 77.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 4948          |\n",
            "| n_updates          | 79            |\n",
            "| policy_entropy     | 1.2109988     |\n",
            "| policy_loss        | -0.0016414225 |\n",
            "| serial_timesteps   | 20224         |\n",
            "| time_elapsed       | 72            |\n",
            "| total_timesteps    | 323584        |\n",
            "| value_loss         | 0.026610067   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0028888439  |\n",
            "| clipfrac           | 0.028198242   |\n",
            "| ep_len_mean        | 76.2          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.992         |\n",
            "| fps                | 9025          |\n",
            "| n_updates          | 80            |\n",
            "| policy_entropy     | 1.2095045     |\n",
            "| policy_loss        | -0.0015884343 |\n",
            "| serial_timesteps   | 20480         |\n",
            "| time_elapsed       | 72.8          |\n",
            "| total_timesteps    | 327680        |\n",
            "| value_loss         | 0.012537284   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=93.00 +/- 0.06\n",
            "Episode length: 74.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0006790412  |\n",
            "| clipfrac           | 0.0017089844  |\n",
            "| ep_len_mean        | 76.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 4954          |\n",
            "| n_updates          | 81            |\n",
            "| policy_entropy     | 1.2062923     |\n",
            "| policy_loss        | -0.0005725756 |\n",
            "| serial_timesteps   | 20736         |\n",
            "| time_elapsed       | 73.3          |\n",
            "| total_timesteps    | 331776        |\n",
            "| value_loss         | 0.02687036    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0014634768 |\n",
            "| clipfrac           | 0.012329102  |\n",
            "| ep_len_mean        | 76.4         |\n",
            "| ep_reward_mean     | 93.1         |\n",
            "| explained_variance | 0.99         |\n",
            "| fps                | 8666         |\n",
            "| n_updates          | 82           |\n",
            "| policy_entropy     | 1.2058649    |\n",
            "| policy_loss        | -0.000986564 |\n",
            "| serial_timesteps   | 20992        |\n",
            "| time_elapsed       | 74.1         |\n",
            "| total_timesteps    | 335872       |\n",
            "| value_loss         | 0.015152198  |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0019563462   |\n",
            "| clipfrac           | 0.0068969727   |\n",
            "| ep_len_mean        | 76             |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.978          |\n",
            "| fps                | 8912           |\n",
            "| n_updates          | 83             |\n",
            "| policy_entropy     | 1.203012       |\n",
            "| policy_loss        | -0.00035793678 |\n",
            "| serial_timesteps   | 21248          |\n",
            "| time_elapsed       | 74.6           |\n",
            "| total_timesteps    | 339968         |\n",
            "| value_loss         | 0.03301808     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=92.83 +/- 0.17\n",
            "Episode length: 76.00 +/- 1.41\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017725084  |\n",
            "| clipfrac           | 0.0134887695  |\n",
            "| ep_len_mean        | 78.1          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.974         |\n",
            "| fps                | 4916          |\n",
            "| n_updates          | 84            |\n",
            "| policy_entropy     | 1.2002263     |\n",
            "| policy_loss        | -0.0016871826 |\n",
            "| serial_timesteps   | 21504         |\n",
            "| time_elapsed       | 75.1          |\n",
            "| total_timesteps    | 344064        |\n",
            "| value_loss         | 0.038713045   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026338326  |\n",
            "| clipfrac           | 0.029846191   |\n",
            "| ep_len_mean        | 78.1          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.989         |\n",
            "| fps                | 9067          |\n",
            "| n_updates          | 85            |\n",
            "| policy_entropy     | 1.1952573     |\n",
            "| policy_loss        | -0.0024997098 |\n",
            "| serial_timesteps   | 21760         |\n",
            "| time_elapsed       | 75.9          |\n",
            "| total_timesteps    | 348160        |\n",
            "| value_loss         | 0.013942654   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=92.97 +/- 0.14\n",
            "Episode length: 74.80 +/- 1.33\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0017003253   |\n",
            "| clipfrac           | 0.007751465    |\n",
            "| ep_len_mean        | 75.7           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.975          |\n",
            "| fps                | 4915           |\n",
            "| n_updates          | 86             |\n",
            "| policy_entropy     | 1.1874748      |\n",
            "| policy_loss        | -0.00084896607 |\n",
            "| serial_timesteps   | 22016          |\n",
            "| time_elapsed       | 76.3           |\n",
            "| total_timesteps    | 352256         |\n",
            "| value_loss         | 0.038954947    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020859437  |\n",
            "| clipfrac           | 0.020324707   |\n",
            "| ep_len_mean        | 76.6          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 8793          |\n",
            "| n_updates          | 87            |\n",
            "| policy_entropy     | 1.1827289     |\n",
            "| policy_loss        | -0.0019232596 |\n",
            "| serial_timesteps   | 22272         |\n",
            "| time_elapsed       | 77.2          |\n",
            "| total_timesteps    | 356352        |\n",
            "| value_loss         | 0.013185246   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=92.95 +/- 0.13\n",
            "Episode length: 74.40 +/- 1.20\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015575635  |\n",
            "| clipfrac           | 0.009765625   |\n",
            "| ep_len_mean        | 75            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 5012          |\n",
            "| n_updates          | 88            |\n",
            "| policy_entropy     | 1.1792734     |\n",
            "| policy_loss        | -0.0008385437 |\n",
            "| serial_timesteps   | 22528         |\n",
            "| time_elapsed       | 77.6          |\n",
            "| total_timesteps    | 360448        |\n",
            "| value_loss         | 0.013353261   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0028300183  |\n",
            "| clipfrac           | 0.018859863   |\n",
            "| ep_len_mean        | 75.7          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.974         |\n",
            "| fps                | 9051          |\n",
            "| n_updates          | 89            |\n",
            "| policy_entropy     | 1.1754652     |\n",
            "| policy_loss        | -0.0009416995 |\n",
            "| serial_timesteps   | 22784         |\n",
            "| time_elapsed       | 78.5          |\n",
            "| total_timesteps    | 364544        |\n",
            "| value_loss         | 0.037533928   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014494399  |\n",
            "| clipfrac           | 0.013244629   |\n",
            "| ep_len_mean        | 76.4          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.992         |\n",
            "| fps                | 9007          |\n",
            "| n_updates          | 90            |\n",
            "| policy_entropy     | 1.1726099     |\n",
            "| policy_loss        | -0.0007278698 |\n",
            "| serial_timesteps   | 23040         |\n",
            "| time_elapsed       | 78.9          |\n",
            "| total_timesteps    | 368640        |\n",
            "| value_loss         | 0.011642298   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=93.01 +/- 0.07\n",
            "Episode length: 73.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016613116  |\n",
            "| clipfrac           | 0.0115356445  |\n",
            "| ep_len_mean        | 74.2          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 4962          |\n",
            "| n_updates          | 91            |\n",
            "| policy_entropy     | 1.170605      |\n",
            "| policy_loss        | -0.0012608211 |\n",
            "| serial_timesteps   | 23296         |\n",
            "| time_elapsed       | 79.4          |\n",
            "| total_timesteps    | 372736        |\n",
            "| value_loss         | 0.009136288   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020665205  |\n",
            "| clipfrac           | 0.018981934   |\n",
            "| ep_len_mean        | 74            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 8847          |\n",
            "| n_updates          | 92            |\n",
            "| policy_entropy     | 1.1700994     |\n",
            "| policy_loss        | -0.0009201903 |\n",
            "| serial_timesteps   | 23552         |\n",
            "| time_elapsed       | 80.2          |\n",
            "| total_timesteps    | 376832        |\n",
            "| value_loss         | 0.017410014   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=93.10 +/- 0.08\n",
            "Episode length: 72.80 +/- 0.75\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0023564836 |\n",
            "| clipfrac           | 0.020019531  |\n",
            "| ep_len_mean        | 74.9         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.991        |\n",
            "| fps                | 4981         |\n",
            "| n_updates          | 93           |\n",
            "| policy_entropy     | 1.1694504    |\n",
            "| policy_loss        | -0.001153175 |\n",
            "| serial_timesteps   | 23808        |\n",
            "| time_elapsed       | 80.7         |\n",
            "| total_timesteps    | 380928       |\n",
            "| value_loss         | 0.012364595  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015069724  |\n",
            "| clipfrac           | 0.013061523   |\n",
            "| ep_len_mean        | 74            |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 8948          |\n",
            "| n_updates          | 94            |\n",
            "| policy_entropy     | 1.1672832     |\n",
            "| policy_loss        | -0.0008207856 |\n",
            "| serial_timesteps   | 24064         |\n",
            "| time_elapsed       | 81.5          |\n",
            "| total_timesteps    | 385024        |\n",
            "| value_loss         | 0.012967207   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013104427  |\n",
            "| clipfrac           | 0.0068969727  |\n",
            "| ep_len_mean        | 75.8          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.968         |\n",
            "| fps                | 8824          |\n",
            "| n_updates          | 95            |\n",
            "| policy_entropy     | 1.1651328     |\n",
            "| policy_loss        | -0.0006356531 |\n",
            "| serial_timesteps   | 24320         |\n",
            "| time_elapsed       | 81.9          |\n",
            "| total_timesteps    | 389120        |\n",
            "| value_loss         | 0.04498485    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=93.25 +/- 0.09\n",
            "Episode length: 74.40 +/- 1.62\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0018208643   |\n",
            "| clipfrac           | 0.0119018555   |\n",
            "| ep_len_mean        | 76.6           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.963          |\n",
            "| fps                | 4862           |\n",
            "| n_updates          | 96             |\n",
            "| policy_entropy     | 1.1633968      |\n",
            "| policy_loss        | -0.00055674004 |\n",
            "| serial_timesteps   | 24576          |\n",
            "| time_elapsed       | 82.4           |\n",
            "| total_timesteps    | 393216         |\n",
            "| value_loss         | 0.05265722     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0027500323  |\n",
            "| clipfrac           | 0.025512695   |\n",
            "| ep_len_mean        | 76.2          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.985         |\n",
            "| fps                | 8991          |\n",
            "| n_updates          | 97            |\n",
            "| policy_entropy     | 1.1597486     |\n",
            "| policy_loss        | -0.0009069642 |\n",
            "| serial_timesteps   | 24832         |\n",
            "| time_elapsed       | 83.2          |\n",
            "| total_timesteps    | 397312        |\n",
            "| value_loss         | 0.020903047   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=93.15 +/- 0.04\n",
            "Episode length: 72.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014502078  |\n",
            "| clipfrac           | 0.012084961   |\n",
            "| ep_len_mean        | 74.7          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 5026          |\n",
            "| n_updates          | 98            |\n",
            "| policy_entropy     | 1.1564305     |\n",
            "| policy_loss        | -0.0008661269 |\n",
            "| serial_timesteps   | 25088         |\n",
            "| time_elapsed       | 83.7          |\n",
            "| total_timesteps    | 401408        |\n",
            "| value_loss         | 0.012698836   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0030901337  |\n",
            "| clipfrac           | 0.03100586    |\n",
            "| ep_len_mean        | 75            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 8743          |\n",
            "| n_updates          | 99            |\n",
            "| policy_entropy     | 1.1542797     |\n",
            "| policy_loss        | -0.0014180403 |\n",
            "| serial_timesteps   | 25344         |\n",
            "| time_elapsed       | 84.5          |\n",
            "| total_timesteps    | 405504        |\n",
            "| value_loss         | 0.017618146   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001732287   |\n",
            "| clipfrac           | 0.013061523   |\n",
            "| ep_len_mean        | 75.1          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.972         |\n",
            "| fps                | 9079          |\n",
            "| n_updates          | 100           |\n",
            "| policy_entropy     | 1.1521333     |\n",
            "| policy_loss        | -0.0008473006 |\n",
            "| serial_timesteps   | 25600         |\n",
            "| time_elapsed       | 85            |\n",
            "| total_timesteps    | 409600        |\n",
            "| value_loss         | 0.03777656    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=93.18 +/- 0.08\n",
            "Episode length: 72.20 +/- 0.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0021111912 |\n",
            "| clipfrac           | 0.019042969  |\n",
            "| ep_len_mean        | 75           |\n",
            "| ep_reward_mean     | 93.1         |\n",
            "| explained_variance | 0.98         |\n",
            "| fps                | 4951         |\n",
            "| n_updates          | 101          |\n",
            "| policy_entropy     | 1.1501899    |\n",
            "| policy_loss        | -0.002828372 |\n",
            "| serial_timesteps   | 25856        |\n",
            "| time_elapsed       | 85.4         |\n",
            "| total_timesteps    | 413696       |\n",
            "| value_loss         | 0.028833274  |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0009241298   |\n",
            "| clipfrac           | 0.003540039    |\n",
            "| ep_len_mean        | 74.1           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.993          |\n",
            "| fps                | 8766           |\n",
            "| n_updates          | 102            |\n",
            "| policy_entropy     | 1.1492468      |\n",
            "| policy_loss        | -0.00071983825 |\n",
            "| serial_timesteps   | 26112          |\n",
            "| time_elapsed       | 86.3           |\n",
            "| total_timesteps    | 417792         |\n",
            "| value_loss         | 0.008843963    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=93.18 +/- 0.14\n",
            "Episode length: 73.40 +/- 1.20\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013545379  |\n",
            "| clipfrac           | 0.010253906   |\n",
            "| ep_len_mean        | 75            |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.983         |\n",
            "| fps                | 4936          |\n",
            "| n_updates          | 103           |\n",
            "| policy_entropy     | 1.1439459     |\n",
            "| policy_loss        | -0.0019320524 |\n",
            "| serial_timesteps   | 26368         |\n",
            "| time_elapsed       | 86.7          |\n",
            "| total_timesteps    | 421888        |\n",
            "| value_loss         | 0.025797496   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0010232922 |\n",
            "| clipfrac           | 0.0038452148 |\n",
            "| ep_len_mean        | 76.6         |\n",
            "| ep_reward_mean     | 93           |\n",
            "| explained_variance | 0.987        |\n",
            "| fps                | 8852         |\n",
            "| n_updates          | 104          |\n",
            "| policy_entropy     | 1.1399909    |\n",
            "| policy_loss        | -0.000779503 |\n",
            "| serial_timesteps   | 26624        |\n",
            "| time_elapsed       | 87.6         |\n",
            "| total_timesteps    | 425984       |\n",
            "| value_loss         | 0.017453305  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=93.14 +/- 0.02\n",
            "Episode length: 72.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0025570705  |\n",
            "| clipfrac           | 0.020202637   |\n",
            "| ep_len_mean        | 75.9          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.978         |\n",
            "| fps                | 4964          |\n",
            "| n_updates          | 105           |\n",
            "| policy_entropy     | 1.1386156     |\n",
            "| policy_loss        | -0.0022654831 |\n",
            "| serial_timesteps   | 26880         |\n",
            "| time_elapsed       | 88            |\n",
            "| total_timesteps    | 430080        |\n",
            "| value_loss         | 0.030228104   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0034666273  |\n",
            "| clipfrac           | 0.038879395   |\n",
            "| ep_len_mean        | 75.8          |\n",
            "| ep_reward_mean     | 92.9          |\n",
            "| explained_variance | 0.988         |\n",
            "| fps                | 8651          |\n",
            "| n_updates          | 106           |\n",
            "| policy_entropy     | 1.1393476     |\n",
            "| policy_loss        | -0.0024343668 |\n",
            "| serial_timesteps   | 27136         |\n",
            "| time_elapsed       | 88.8          |\n",
            "| total_timesteps    | 434176        |\n",
            "| value_loss         | 0.015937027   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0030093794 |\n",
            "| clipfrac           | 0.03149414   |\n",
            "| ep_len_mean        | 76.7         |\n",
            "| ep_reward_mean     | 92.9         |\n",
            "| explained_variance | 0.977        |\n",
            "| fps                | 9110         |\n",
            "| n_updates          | 107          |\n",
            "| policy_entropy     | 1.1389333    |\n",
            "| policy_loss        | -0.002684035 |\n",
            "| serial_timesteps   | 27392        |\n",
            "| time_elapsed       | 89.3         |\n",
            "| total_timesteps    | 438272       |\n",
            "| value_loss         | 0.033727136  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=93.11 +/- 0.10\n",
            "Episode length: 73.80 +/- 1.33\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020208664  |\n",
            "| clipfrac           | 0.015563965   |\n",
            "| ep_len_mean        | 76.2          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.981         |\n",
            "| fps                | 4887          |\n",
            "| n_updates          | 108           |\n",
            "| policy_entropy     | 1.1351308     |\n",
            "| policy_loss        | -0.0011906835 |\n",
            "| serial_timesteps   | 27648         |\n",
            "| time_elapsed       | 89.8          |\n",
            "| total_timesteps    | 442368        |\n",
            "| value_loss         | 0.026470173   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0007889287   |\n",
            "| clipfrac           | 0.0013427734   |\n",
            "| ep_len_mean        | 76             |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.98           |\n",
            "| fps                | 9092           |\n",
            "| n_updates          | 109            |\n",
            "| policy_entropy     | 1.1312487      |\n",
            "| policy_loss        | -0.00029410102 |\n",
            "| serial_timesteps   | 27904          |\n",
            "| time_elapsed       | 90.6           |\n",
            "| total_timesteps    | 446464         |\n",
            "| value_loss         | 0.029351162    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=92.98 +/- 0.07\n",
            "Episode length: 73.20 +/- 0.75\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0010898183   |\n",
            "| clipfrac           | 0.006591797    |\n",
            "| ep_len_mean        | 75.1           |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.993          |\n",
            "| fps                | 4920           |\n",
            "| n_updates          | 110            |\n",
            "| policy_entropy     | 1.1270221      |\n",
            "| policy_loss        | -0.00027365662 |\n",
            "| serial_timesteps   | 28160          |\n",
            "| time_elapsed       | 91.1           |\n",
            "| total_timesteps    | 450560         |\n",
            "| value_loss         | 0.0101409685   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010190652  |\n",
            "| clipfrac           | 0.008056641   |\n",
            "| ep_len_mean        | 73.6          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 8842          |\n",
            "| n_updates          | 111           |\n",
            "| policy_entropy     | 1.1237168     |\n",
            "| policy_loss        | -0.0010579274 |\n",
            "| serial_timesteps   | 28416         |\n",
            "| time_elapsed       | 91.9          |\n",
            "| total_timesteps    | 454656        |\n",
            "| value_loss         | 0.010715986   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021864467  |\n",
            "| clipfrac           | 0.023010254   |\n",
            "| ep_len_mean        | 73.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 9208          |\n",
            "| n_updates          | 112           |\n",
            "| policy_entropy     | 1.1247873     |\n",
            "| policy_loss        | -0.0011558882 |\n",
            "| serial_timesteps   | 28672         |\n",
            "| time_elapsed       | 92.4          |\n",
            "| total_timesteps    | 458752        |\n",
            "| value_loss         | 0.009876047   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=93.03 +/- 0.11\n",
            "Episode length: 72.80 +/- 0.98\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019526285  |\n",
            "| clipfrac           | 0.01739502    |\n",
            "| ep_len_mean        | 73            |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 4989          |\n",
            "| n_updates          | 113           |\n",
            "| policy_entropy     | 1.1238596     |\n",
            "| policy_loss        | -0.0014393808 |\n",
            "| serial_timesteps   | 28928         |\n",
            "| time_elapsed       | 92.8          |\n",
            "| total_timesteps    | 462848        |\n",
            "| value_loss         | 0.007000324   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00023582704  |\n",
            "| clipfrac           | 0.0009765625   |\n",
            "| ep_len_mean        | 72.3           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.994          |\n",
            "| fps                | 8866           |\n",
            "| n_updates          | 114            |\n",
            "| policy_entropy     | 1.1233605      |\n",
            "| policy_loss        | -0.00049778726 |\n",
            "| serial_timesteps   | 29184          |\n",
            "| time_elapsed       | 93.6           |\n",
            "| total_timesteps    | 466944         |\n",
            "| value_loss         | 0.008271157    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=93.15 +/- 0.10\n",
            "Episode length: 71.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018774949  |\n",
            "| clipfrac           | 0.01953125    |\n",
            "| ep_len_mean        | 72.3          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5026          |\n",
            "| n_updates          | 115           |\n",
            "| policy_entropy     | 1.1235694     |\n",
            "| policy_loss        | -0.0011730279 |\n",
            "| serial_timesteps   | 29440         |\n",
            "| time_elapsed       | 94.1          |\n",
            "| total_timesteps    | 471040        |\n",
            "| value_loss         | 0.006204659   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0009414187   |\n",
            "| clipfrac           | 0.0079956055   |\n",
            "| ep_len_mean        | 72.8           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.991          |\n",
            "| fps                | 8642           |\n",
            "| n_updates          | 116            |\n",
            "| policy_entropy     | 1.1223918      |\n",
            "| policy_loss        | -0.00041461198 |\n",
            "| serial_timesteps   | 29696          |\n",
            "| time_elapsed       | 94.9           |\n",
            "| total_timesteps    | 475136         |\n",
            "| value_loss         | 0.012251118    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0031949386   |\n",
            "| clipfrac           | 0.037719727    |\n",
            "| ep_len_mean        | 73             |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.98           |\n",
            "| fps                | 9068           |\n",
            "| n_updates          | 117            |\n",
            "| policy_entropy     | 1.1233569      |\n",
            "| policy_loss        | -0.00052883907 |\n",
            "| serial_timesteps   | 29952          |\n",
            "| time_elapsed       | 95.4           |\n",
            "| total_timesteps    | 479232         |\n",
            "| value_loss         | 0.025589671    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=93.19 +/- 0.10\n",
            "Episode length: 71.40 +/- 0.80\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0006788093   |\n",
            "| clipfrac           | 0.0037231445   |\n",
            "| ep_len_mean        | 73.2           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.993          |\n",
            "| fps                | 4936           |\n",
            "| n_updates          | 118            |\n",
            "| policy_entropy     | 1.1234741      |\n",
            "| policy_loss        | -0.00060355046 |\n",
            "| serial_timesteps   | 30208          |\n",
            "| time_elapsed       | 95.8           |\n",
            "| total_timesteps    | 483328         |\n",
            "| value_loss         | 0.007876844    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0013248831   |\n",
            "| clipfrac           | 0.0056152344   |\n",
            "| ep_len_mean        | 73.5           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.96           |\n",
            "| fps                | 9015           |\n",
            "| n_updates          | 119            |\n",
            "| policy_entropy     | 1.1227366      |\n",
            "| policy_loss        | -0.00045091903 |\n",
            "| serial_timesteps   | 30464          |\n",
            "| time_elapsed       | 96.7           |\n",
            "| total_timesteps    | 487424         |\n",
            "| value_loss         | 0.05379707     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=93.23 +/- 0.09\n",
            "Episode length: 70.80 +/- 0.75\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00079174095  |\n",
            "| clipfrac           | 0.0045776367   |\n",
            "| ep_len_mean        | 75             |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.959          |\n",
            "| fps                | 5180           |\n",
            "| n_updates          | 120            |\n",
            "| policy_entropy     | 1.1228034      |\n",
            "| policy_loss        | -0.00032618444 |\n",
            "| serial_timesteps   | 30720          |\n",
            "| time_elapsed       | 97.1           |\n",
            "| total_timesteps    | 491520         |\n",
            "| value_loss         | 0.05228583     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00054307043  |\n",
            "| clipfrac           | 0.0017700195   |\n",
            "| ep_len_mean        | 74.3           |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.988          |\n",
            "| fps                | 8542           |\n",
            "| n_updates          | 121            |\n",
            "| policy_entropy     | 1.1234343      |\n",
            "| policy_loss        | -0.00041724264 |\n",
            "| serial_timesteps   | 30976          |\n",
            "| time_elapsed       | 97.9           |\n",
            "| total_timesteps    | 495616         |\n",
            "| value_loss         | 0.016246792    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0010562054   |\n",
            "| clipfrac           | 0.0053710938   |\n",
            "| ep_len_mean        | 73.6           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.972          |\n",
            "| fps                | 8757           |\n",
            "| n_updates          | 122            |\n",
            "| policy_entropy     | 1.1248379      |\n",
            "| policy_loss        | -0.00029396548 |\n",
            "| serial_timesteps   | 31232          |\n",
            "| time_elapsed       | 98.4           |\n",
            "| total_timesteps    | 499712         |\n",
            "| value_loss         | 0.037000757    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=93.15 +/- 0.07\n",
            "Episode length: 71.40 +/- 0.49\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0026325267 |\n",
            "| clipfrac           | 0.027770996  |\n",
            "| ep_len_mean        | 73.8         |\n",
            "| ep_reward_mean     | 93           |\n",
            "| explained_variance | 0.994        |\n",
            "| fps                | 4987         |\n",
            "| n_updates          | 123          |\n",
            "| policy_entropy     | 1.1255587    |\n",
            "| policy_loss        | -0.001705773 |\n",
            "| serial_timesteps   | 31488        |\n",
            "| time_elapsed       | 98.9         |\n",
            "| total_timesteps    | 503808       |\n",
            "| value_loss         | 0.0067765196 |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0016580928   |\n",
            "| clipfrac           | 0.010192871    |\n",
            "| ep_len_mean        | 72             |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.989          |\n",
            "| fps                | 8965           |\n",
            "| n_updates          | 124            |\n",
            "| policy_entropy     | 1.1262232      |\n",
            "| policy_loss        | -0.00027016038 |\n",
            "| serial_timesteps   | 31744          |\n",
            "| time_elapsed       | 99.7           |\n",
            "| total_timesteps    | 507904         |\n",
            "| value_loss         | 0.015871467    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=93.21 +/- 0.08\n",
            "Episode length: 70.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0013217429  |\n",
            "| clipfrac           | 0.0071411133  |\n",
            "| ep_len_mean        | 72.5          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 4990          |\n",
            "| n_updates          | 125           |\n",
            "| policy_entropy     | 1.1237668     |\n",
            "| policy_loss        | -0.0011571252 |\n",
            "| serial_timesteps   | 32000         |\n",
            "| time_elapsed       | 100           |\n",
            "| total_timesteps    | 512000        |\n",
            "| value_loss         | 0.008914845   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026993807  |\n",
            "| clipfrac           | 0.020019531   |\n",
            "| ep_len_mean        | 72.8          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.978         |\n",
            "| fps                | 8588          |\n",
            "| n_updates          | 126           |\n",
            "| policy_entropy     | 1.1219394     |\n",
            "| policy_loss        | -0.0009262915 |\n",
            "| serial_timesteps   | 32256         |\n",
            "| time_elapsed       | 101           |\n",
            "| total_timesteps    | 516096        |\n",
            "| value_loss         | 0.029637503   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=93.26 +/- 0.10\n",
            "Episode length: 71.00 +/- 0.89\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0008564173  |\n",
            "| clipfrac           | 0.004638672   |\n",
            "| ep_len_mean        | 74.4          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.98          |\n",
            "| fps                | 5105          |\n",
            "| n_updates          | 127           |\n",
            "| policy_entropy     | 1.1228205     |\n",
            "| policy_loss        | 0.00014508565 |\n",
            "| serial_timesteps   | 32512         |\n",
            "| time_elapsed       | 101           |\n",
            "| total_timesteps    | 520192        |\n",
            "| value_loss         | 0.025901519   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0014177558   |\n",
            "| clipfrac           | 0.012268066    |\n",
            "| ep_len_mean        | 74.5           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.98           |\n",
            "| fps                | 8903           |\n",
            "| n_updates          | 128            |\n",
            "| policy_entropy     | 1.1222221      |\n",
            "| policy_loss        | -0.00032599765 |\n",
            "| serial_timesteps   | 32768          |\n",
            "| time_elapsed       | 102            |\n",
            "| total_timesteps    | 524288         |\n",
            "| value_loss         | 0.028557349    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0004567027   |\n",
            "| clipfrac           | 0.0015258789   |\n",
            "| ep_len_mean        | 73.2           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.984          |\n",
            "| fps                | 9127           |\n",
            "| n_updates          | 129            |\n",
            "| policy_entropy     | 1.1172142      |\n",
            "| policy_loss        | -0.00025464344 |\n",
            "| serial_timesteps   | 33024          |\n",
            "| time_elapsed       | 103            |\n",
            "| total_timesteps    | 528384         |\n",
            "| value_loss         | 0.020502225    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=93.18 +/- 0.07\n",
            "Episode length: 71.00 +/- 0.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018474316  |\n",
            "| clipfrac           | 0.014282227   |\n",
            "| ep_len_mean        | 73.8          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.96          |\n",
            "| fps                | 4996          |\n",
            "| n_updates          | 130           |\n",
            "| policy_entropy     | 1.1133116     |\n",
            "| policy_loss        | -0.0024002937 |\n",
            "| serial_timesteps   | 33280         |\n",
            "| time_elapsed       | 103           |\n",
            "| total_timesteps    | 532480        |\n",
            "| value_loss         | 0.052122973   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0024711068  |\n",
            "| clipfrac           | 0.022827148   |\n",
            "| ep_len_mean        | 73.3          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.978         |\n",
            "| fps                | 8638          |\n",
            "| n_updates          | 131           |\n",
            "| policy_entropy     | 1.1113757     |\n",
            "| policy_loss        | -0.0027051622 |\n",
            "| serial_timesteps   | 33536         |\n",
            "| time_elapsed       | 104           |\n",
            "| total_timesteps    | 536576        |\n",
            "| value_loss         | 0.029162534   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=93.30 +/- 0.02\n",
            "Episode length: 70.60 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0023545835  |\n",
            "| clipfrac           | 0.02331543    |\n",
            "| ep_len_mean        | 72.1          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.994         |\n",
            "| fps                | 5081          |\n",
            "| n_updates          | 132           |\n",
            "| policy_entropy     | 1.1065309     |\n",
            "| policy_loss        | -0.0016371054 |\n",
            "| serial_timesteps   | 33792         |\n",
            "| time_elapsed       | 104           |\n",
            "| total_timesteps    | 540672        |\n",
            "| value_loss         | 0.0056633493  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.001157626    |\n",
            "| clipfrac           | 0.007751465    |\n",
            "| ep_len_mean        | 71.8           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.971          |\n",
            "| fps                | 9140           |\n",
            "| n_updates          | 133            |\n",
            "| policy_entropy     | 1.1012688      |\n",
            "| policy_loss        | -0.00065842154 |\n",
            "| serial_timesteps   | 34048          |\n",
            "| time_elapsed       | 105            |\n",
            "| total_timesteps    | 544768         |\n",
            "| value_loss         | 0.03948348     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011044679  |\n",
            "| clipfrac           | 0.005493164   |\n",
            "| ep_len_mean        | 73.7          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.99          |\n",
            "| fps                | 8932          |\n",
            "| n_updates          | 134           |\n",
            "| policy_entropy     | 1.0974734     |\n",
            "| policy_loss        | -0.0009964219 |\n",
            "| serial_timesteps   | 34304         |\n",
            "| time_elapsed       | 106           |\n",
            "| total_timesteps    | 548864        |\n",
            "| value_loss         | 0.01353194    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=93.21 +/- 0.05\n",
            "Episode length: 71.20 +/- 0.98\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0022885636  |\n",
            "| clipfrac           | 0.023071289   |\n",
            "| ep_len_mean        | 74.5          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 5016          |\n",
            "| n_updates          | 135           |\n",
            "| policy_entropy     | 1.0939764     |\n",
            "| policy_loss        | -0.0008722857 |\n",
            "| serial_timesteps   | 34560         |\n",
            "| time_elapsed       | 106           |\n",
            "| total_timesteps    | 552960        |\n",
            "| value_loss         | 0.023766777   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0026697898 |\n",
            "| clipfrac           | 0.024108887  |\n",
            "| ep_len_mean        | 73.1         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.993        |\n",
            "| fps                | 8702         |\n",
            "| n_updates          | 136          |\n",
            "| policy_entropy     | 1.0910286    |\n",
            "| policy_loss        | -0.0024013   |\n",
            "| serial_timesteps   | 34816        |\n",
            "| time_elapsed       | 107          |\n",
            "| total_timesteps    | 557056       |\n",
            "| value_loss         | 0.008335906  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=93.22 +/- 0.08\n",
            "Episode length: 71.20 +/- 1.17\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007967509  |\n",
            "| clipfrac           | 0.0036010742  |\n",
            "| ep_len_mean        | 71.8          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 4978          |\n",
            "| n_updates          | 137           |\n",
            "| policy_entropy     | 1.0899779     |\n",
            "| policy_loss        | -0.0008679172 |\n",
            "| serial_timesteps   | 35072         |\n",
            "| time_elapsed       | 107           |\n",
            "| total_timesteps    | 561152        |\n",
            "| value_loss         | 0.006207731   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002873818   |\n",
            "| clipfrac           | 0.025878906   |\n",
            "| ep_len_mean        | 71.4          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 9224          |\n",
            "| n_updates          | 138           |\n",
            "| policy_entropy     | 1.0887471     |\n",
            "| policy_loss        | -0.0008570291 |\n",
            "| serial_timesteps   | 35328         |\n",
            "| time_elapsed       | 108           |\n",
            "| total_timesteps    | 565248        |\n",
            "| value_loss         | 0.0059282607  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00060890673 |\n",
            "| clipfrac           | 0.003967285   |\n",
            "| ep_len_mean        | 72.4          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 8853          |\n",
            "| n_updates          | 139           |\n",
            "| policy_entropy     | 1.0855757     |\n",
            "| policy_loss        | -0.0005984674 |\n",
            "| serial_timesteps   | 35584         |\n",
            "| time_elapsed       | 109           |\n",
            "| total_timesteps    | 569344        |\n",
            "| value_loss         | 0.023671575   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=93.14 +/- 0.14\n",
            "Episode length: 71.40 +/- 1.20\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021829493  |\n",
            "| clipfrac           | 0.021240234   |\n",
            "| ep_len_mean        | 73.6          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 4965          |\n",
            "| n_updates          | 140           |\n",
            "| policy_entropy     | 1.0849055     |\n",
            "| policy_loss        | -0.0029825638 |\n",
            "| serial_timesteps   | 35840         |\n",
            "| time_elapsed       | 109           |\n",
            "| total_timesteps    | 573440        |\n",
            "| value_loss         | 0.006808467   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0022490008  |\n",
            "| clipfrac           | 0.013977051   |\n",
            "| ep_len_mean        | 71.7          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 8953          |\n",
            "| n_updates          | 141           |\n",
            "| policy_entropy     | 1.0847715     |\n",
            "| policy_loss        | -0.0008285123 |\n",
            "| serial_timesteps   | 36096         |\n",
            "| time_elapsed       | 110           |\n",
            "| total_timesteps    | 577536        |\n",
            "| value_loss         | 0.008546036   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=93.22 +/- 0.04\n",
            "Episode length: 71.00 +/- 0.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014091072  |\n",
            "| clipfrac           | 0.009765625   |\n",
            "| ep_len_mean        | 72            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 4974          |\n",
            "| n_updates          | 142           |\n",
            "| policy_entropy     | 1.0827029     |\n",
            "| policy_loss        | -0.0012377995 |\n",
            "| serial_timesteps   | 36352         |\n",
            "| time_elapsed       | 110           |\n",
            "| total_timesteps    | 581632        |\n",
            "| value_loss         | 0.0044287494  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021225775  |\n",
            "| clipfrac           | 0.016296387   |\n",
            "| ep_len_mean        | 71            |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 9025          |\n",
            "| n_updates          | 143           |\n",
            "| policy_entropy     | 1.0797361     |\n",
            "| policy_loss        | -0.0008131997 |\n",
            "| serial_timesteps   | 36608         |\n",
            "| time_elapsed       | 111           |\n",
            "| total_timesteps    | 585728        |\n",
            "| value_loss         | 0.0043751798  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00090155797  |\n",
            "| clipfrac           | 0.0018310547   |\n",
            "| ep_len_mean        | 71.2           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.997          |\n",
            "| fps                | 9082           |\n",
            "| n_updates          | 144            |\n",
            "| policy_entropy     | 1.0779752      |\n",
            "| policy_loss        | -0.00011144995 |\n",
            "| serial_timesteps   | 36864          |\n",
            "| time_elapsed       | 112            |\n",
            "| total_timesteps    | 589824         |\n",
            "| value_loss         | 0.0045161545   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=93.26 +/- 0.05\n",
            "Episode length: 70.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00049707183 |\n",
            "| clipfrac           | 0.0018920898  |\n",
            "| ep_len_mean        | 72            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.991         |\n",
            "| fps                | 5006          |\n",
            "| n_updates          | 145           |\n",
            "| policy_entropy     | 1.0764785     |\n",
            "| policy_loss        | -0.0001547783 |\n",
            "| serial_timesteps   | 37120         |\n",
            "| time_elapsed       | 112           |\n",
            "| total_timesteps    | 593920        |\n",
            "| value_loss         | 0.011775521   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0014869735 |\n",
            "| clipfrac           | 0.012573242  |\n",
            "| ep_len_mean        | 72.7         |\n",
            "| ep_reward_mean     | 93.1         |\n",
            "| explained_variance | 0.992        |\n",
            "| fps                | 9027         |\n",
            "| n_updates          | 146          |\n",
            "| policy_entropy     | 1.075016     |\n",
            "| policy_loss        | -0.001427044 |\n",
            "| serial_timesteps   | 37376        |\n",
            "| time_elapsed       | 113          |\n",
            "| total_timesteps    | 598016       |\n",
            "| value_loss         | 0.0101920795 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=93.21 +/- 0.08\n",
            "Episode length: 70.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001363626   |\n",
            "| clipfrac           | 0.008911133   |\n",
            "| ep_len_mean        | 72.7          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5008          |\n",
            "| n_updates          | 147           |\n",
            "| policy_entropy     | 1.0751841     |\n",
            "| policy_loss        | -0.0006900954 |\n",
            "| serial_timesteps   | 37632         |\n",
            "| time_elapsed       | 113           |\n",
            "| total_timesteps    | 602112        |\n",
            "| value_loss         | 0.005081827   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0023804265 |\n",
            "| clipfrac           | 0.01928711   |\n",
            "| ep_len_mean        | 72.1         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.986        |\n",
            "| fps                | 8841         |\n",
            "| n_updates          | 148          |\n",
            "| policy_entropy     | 1.0729965    |\n",
            "| policy_loss        | -0.000698169 |\n",
            "| serial_timesteps   | 37888        |\n",
            "| time_elapsed       | 114          |\n",
            "| total_timesteps    | 606208       |\n",
            "| value_loss         | 0.018630914  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=93.19 +/- 0.08\n",
            "Episode length: 70.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000819333   |\n",
            "| clipfrac           | 0.0013427734  |\n",
            "| ep_len_mean        | 71.7          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5018          |\n",
            "| n_updates          | 149           |\n",
            "| policy_entropy     | 1.0718333     |\n",
            "| policy_loss        | -0.0004631989 |\n",
            "| serial_timesteps   | 38144         |\n",
            "| time_elapsed       | 115           |\n",
            "| total_timesteps    | 610304        |\n",
            "| value_loss         | 0.0058516148  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001401042   |\n",
            "| clipfrac           | 0.010681152   |\n",
            "| ep_len_mean        | 71.1          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 9164          |\n",
            "| n_updates          | 150           |\n",
            "| policy_entropy     | 1.0710053     |\n",
            "| policy_loss        | -0.0015404017 |\n",
            "| serial_timesteps   | 38400         |\n",
            "| time_elapsed       | 116           |\n",
            "| total_timesteps    | 614400        |\n",
            "| value_loss         | 0.009715884   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000862051   |\n",
            "| clipfrac           | 0.0032348633  |\n",
            "| ep_len_mean        | 71.2          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.994         |\n",
            "| fps                | 9038          |\n",
            "| n_updates          | 151           |\n",
            "| policy_entropy     | 1.0675416     |\n",
            "| policy_loss        | -0.0004491282 |\n",
            "| serial_timesteps   | 38656         |\n",
            "| time_elapsed       | 116           |\n",
            "| total_timesteps    | 618496        |\n",
            "| value_loss         | 0.0074911285  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=93.22 +/- 0.06\n",
            "Episode length: 70.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0016859807  |\n",
            "| clipfrac           | 0.016540527   |\n",
            "| ep_len_mean        | 71.2          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.984         |\n",
            "| fps                | 4990          |\n",
            "| n_updates          | 152           |\n",
            "| policy_entropy     | 1.0655063     |\n",
            "| policy_loss        | -0.0028189428 |\n",
            "| serial_timesteps   | 38912         |\n",
            "| time_elapsed       | 116           |\n",
            "| total_timesteps    | 622592        |\n",
            "| value_loss         | 0.020260563   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0011001634   |\n",
            "| clipfrac           | 0.003112793    |\n",
            "| ep_len_mean        | 71.6           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.993          |\n",
            "| fps                | 9064           |\n",
            "| n_updates          | 153            |\n",
            "| policy_entropy     | 1.0649483      |\n",
            "| policy_loss        | -0.00046202887 |\n",
            "| serial_timesteps   | 39168          |\n",
            "| time_elapsed       | 117            |\n",
            "| total_timesteps    | 626688         |\n",
            "| value_loss         | 0.008296734    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=93.26 +/- 0.05\n",
            "Episode length: 70.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00043879115 |\n",
            "| clipfrac           | 0.00024414062 |\n",
            "| ep_len_mean        | 72.9          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.977         |\n",
            "| fps                | 4997          |\n",
            "| n_updates          | 154           |\n",
            "| policy_entropy     | 1.0611807     |\n",
            "| policy_loss        | 0.00011919158 |\n",
            "| serial_timesteps   | 39424         |\n",
            "| time_elapsed       | 118           |\n",
            "| total_timesteps    | 630784        |\n",
            "| value_loss         | 0.030867599   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026732632  |\n",
            "| clipfrac           | 0.01776123    |\n",
            "| ep_len_mean        | 73.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.993         |\n",
            "| fps                | 8855          |\n",
            "| n_updates          | 155           |\n",
            "| policy_entropy     | 1.0583186     |\n",
            "| policy_loss        | -0.0009757672 |\n",
            "| serial_timesteps   | 39680         |\n",
            "| time_elapsed       | 119           |\n",
            "| total_timesteps    | 634880        |\n",
            "| value_loss         | 0.008613228   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00075702486  |\n",
            "| clipfrac           | 0.0036621094   |\n",
            "| ep_len_mean        | 72.8           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.973          |\n",
            "| fps                | 8956           |\n",
            "| n_updates          | 156            |\n",
            "| policy_entropy     | 1.0538877      |\n",
            "| policy_loss        | -0.00069806905 |\n",
            "| serial_timesteps   | 39936          |\n",
            "| time_elapsed       | 119            |\n",
            "| total_timesteps    | 638976         |\n",
            "| value_loss         | 0.034999155    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=93.23 +/- 0.09\n",
            "Episode length: 70.40 +/- 0.80\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00038077362  |\n",
            "| clipfrac           | 0.0017089844   |\n",
            "| ep_len_mean        | 73.3           |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.987          |\n",
            "| fps                | 5013           |\n",
            "| n_updates          | 157            |\n",
            "| policy_entropy     | 1.0520688      |\n",
            "| policy_loss        | -0.00082969526 |\n",
            "| serial_timesteps   | 40192          |\n",
            "| time_elapsed       | 119            |\n",
            "| total_timesteps    | 643072         |\n",
            "| value_loss         | 0.016334439    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0035696006  |\n",
            "| clipfrac           | 0.03729248    |\n",
            "| ep_len_mean        | 72.5          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 9027          |\n",
            "| n_updates          | 158           |\n",
            "| policy_entropy     | 1.047641      |\n",
            "| policy_loss        | -0.0015670377 |\n",
            "| serial_timesteps   | 40448         |\n",
            "| time_elapsed       | 120           |\n",
            "| total_timesteps    | 647168        |\n",
            "| value_loss         | 0.016632274   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=93.21 +/- 0.11\n",
            "Episode length: 70.80 +/- 0.75\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0017183356   |\n",
            "| clipfrac           | 0.011047363    |\n",
            "| ep_len_mean        | 72.4           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.985          |\n",
            "| fps                | 4950           |\n",
            "| n_updates          | 159            |\n",
            "| policy_entropy     | 1.0435436      |\n",
            "| policy_loss        | -0.00057278597 |\n",
            "| serial_timesteps   | 40704          |\n",
            "| time_elapsed       | 121            |\n",
            "| total_timesteps    | 651264         |\n",
            "| value_loss         | 0.019279083    |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.001664211  |\n",
            "| clipfrac           | 0.007446289  |\n",
            "| ep_len_mean        | 71           |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.994        |\n",
            "| fps                | 8964         |\n",
            "| n_updates          | 160          |\n",
            "| policy_entropy     | 1.04007      |\n",
            "| policy_loss        | -0.000687655 |\n",
            "| serial_timesteps   | 40960        |\n",
            "| time_elapsed       | 122          |\n",
            "| total_timesteps    | 655360       |\n",
            "| value_loss         | 0.007072909  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0036813496  |\n",
            "| clipfrac           | 0.04034424    |\n",
            "| ep_len_mean        | 70.8          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 8997          |\n",
            "| n_updates          | 161           |\n",
            "| policy_entropy     | 1.0373577     |\n",
            "| policy_loss        | -0.0022605604 |\n",
            "| serial_timesteps   | 41216         |\n",
            "| time_elapsed       | 122           |\n",
            "| total_timesteps    | 659456        |\n",
            "| value_loss         | 0.016868724   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=93.21 +/- 0.06\n",
            "Episode length: 70.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002886178   |\n",
            "| clipfrac           | 0.030029297   |\n",
            "| ep_len_mean        | 71.5          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 5039          |\n",
            "| n_updates          | 162           |\n",
            "| policy_entropy     | 1.0342388     |\n",
            "| policy_loss        | -0.0017321452 |\n",
            "| serial_timesteps   | 41472         |\n",
            "| time_elapsed       | 122           |\n",
            "| total_timesteps    | 663552        |\n",
            "| value_loss         | 0.006980278   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026118548  |\n",
            "| clipfrac           | 0.026367188   |\n",
            "| ep_len_mean        | 71.1          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 9219          |\n",
            "| n_updates          | 163           |\n",
            "| policy_entropy     | 1.0299901     |\n",
            "| policy_loss        | -0.0012522239 |\n",
            "| serial_timesteps   | 41728         |\n",
            "| time_elapsed       | 123           |\n",
            "| total_timesteps    | 667648        |\n",
            "| value_loss         | 0.003249707   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=93.21 +/- 0.05\n",
            "Episode length: 70.40 +/- 0.49\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0019114097   |\n",
            "| clipfrac           | 0.014404297    |\n",
            "| ep_len_mean        | 70.6           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.985          |\n",
            "| fps                | 5060           |\n",
            "| n_updates          | 164            |\n",
            "| policy_entropy     | 1.0280612      |\n",
            "| policy_loss        | -0.00028324808 |\n",
            "| serial_timesteps   | 41984          |\n",
            "| time_elapsed       | 124            |\n",
            "| total_timesteps    | 671744         |\n",
            "| value_loss         | 0.019245727    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019571814  |\n",
            "| clipfrac           | 0.020812988   |\n",
            "| ep_len_mean        | 71.4          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.994         |\n",
            "| fps                | 8905          |\n",
            "| n_updates          | 165           |\n",
            "| policy_entropy     | 1.0286758     |\n",
            "| policy_loss        | -0.0014461031 |\n",
            "| serial_timesteps   | 42240         |\n",
            "| time_elapsed       | 125           |\n",
            "| total_timesteps    | 675840        |\n",
            "| value_loss         | 0.006778305   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0022561492 |\n",
            "| clipfrac           | 0.021484375  |\n",
            "| ep_len_mean        | 70.7         |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.995        |\n",
            "| fps                | 8848         |\n",
            "| n_updates          | 166          |\n",
            "| policy_entropy     | 1.0281183    |\n",
            "| policy_loss        | -0.001391975 |\n",
            "| serial_timesteps   | 42496        |\n",
            "| time_elapsed       | 125          |\n",
            "| total_timesteps    | 679936       |\n",
            "| value_loss         | 0.0063530537 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=93.24 +/- 0.06\n",
            "Episode length: 70.60 +/- 0.80\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002084418   |\n",
            "| clipfrac           | 0.01928711    |\n",
            "| ep_len_mean        | 71.2          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5043          |\n",
            "| n_updates          | 167           |\n",
            "| policy_entropy     | 1.0281222     |\n",
            "| policy_loss        | -0.0007292931 |\n",
            "| serial_timesteps   | 42752         |\n",
            "| time_elapsed       | 125           |\n",
            "| total_timesteps    | 684032        |\n",
            "| value_loss         | 0.003978324   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0017106569   |\n",
            "| clipfrac           | 0.012634277    |\n",
            "| ep_len_mean        | 70.5           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.997          |\n",
            "| fps                | 8758           |\n",
            "| n_updates          | 168            |\n",
            "| policy_entropy     | 1.0270643      |\n",
            "| policy_loss        | -0.00063879654 |\n",
            "| serial_timesteps   | 43008          |\n",
            "| time_elapsed       | 126            |\n",
            "| total_timesteps    | 688128         |\n",
            "| value_loss         | 0.0036688556   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=93.29 +/- 0.06\n",
            "Episode length: 69.60 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007312311  |\n",
            "| clipfrac           | 0.00018310547 |\n",
            "| ep_len_mean        | 70.9          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5032          |\n",
            "| n_updates          | 169           |\n",
            "| policy_entropy     | 1.0257148     |\n",
            "| policy_loss        | 0.0003571429  |\n",
            "| serial_timesteps   | 43264         |\n",
            "| time_elapsed       | 127           |\n",
            "| total_timesteps    | 692224        |\n",
            "| value_loss         | 0.004610252   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020797388  |\n",
            "| clipfrac           | 0.01940918    |\n",
            "| ep_len_mean        | 71.3          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 8967          |\n",
            "| n_updates          | 170           |\n",
            "| policy_entropy     | 1.0258571     |\n",
            "| policy_loss        | -0.0015892468 |\n",
            "| serial_timesteps   | 43520         |\n",
            "| time_elapsed       | 128           |\n",
            "| total_timesteps    | 696320        |\n",
            "| value_loss         | 0.004893308   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=93.27 +/- 0.05\n",
            "Episode length: 69.60 +/- 0.49\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0032573952   |\n",
            "| clipfrac           | 0.029846191    |\n",
            "| ep_len_mean        | 71.2           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.984          |\n",
            "| fps                | 5057           |\n",
            "| n_updates          | 171            |\n",
            "| policy_entropy     | 1.0272143      |\n",
            "| policy_loss        | -0.00085250475 |\n",
            "| serial_timesteps   | 43776          |\n",
            "| time_elapsed       | 128            |\n",
            "| total_timesteps    | 700416         |\n",
            "| value_loss         | 0.018975172    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0017214849   |\n",
            "| clipfrac           | 0.01184082     |\n",
            "| ep_len_mean        | 71.7           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.982          |\n",
            "| fps                | 8793           |\n",
            "| n_updates          | 172            |\n",
            "| policy_entropy     | 1.0284258      |\n",
            "| policy_loss        | -0.00075348304 |\n",
            "| serial_timesteps   | 44032          |\n",
            "| time_elapsed       | 129            |\n",
            "| total_timesteps    | 704512         |\n",
            "| value_loss         | 0.023171067    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0022846253   |\n",
            "| clipfrac           | 0.018920898    |\n",
            "| ep_len_mean        | 73.2           |\n",
            "| ep_reward_mean     | 93             |\n",
            "| explained_variance | 0.973          |\n",
            "| fps                | 8888           |\n",
            "| n_updates          | 173            |\n",
            "| policy_entropy     | 1.0271412      |\n",
            "| policy_loss        | -0.00034841255 |\n",
            "| serial_timesteps   | 44288          |\n",
            "| time_elapsed       | 129            |\n",
            "| total_timesteps    | 708608         |\n",
            "| value_loss         | 0.03280614     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=93.24 +/- 0.07\n",
            "Episode length: 70.00 +/- 0.63\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011266595  |\n",
            "| clipfrac           | 0.008117676   |\n",
            "| ep_len_mean        | 74.4          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.968         |\n",
            "| fps                | 5100          |\n",
            "| n_updates          | 174           |\n",
            "| policy_entropy     | 1.0247691     |\n",
            "| policy_loss        | -0.0005538147 |\n",
            "| serial_timesteps   | 44544         |\n",
            "| time_elapsed       | 130           |\n",
            "| total_timesteps    | 712704        |\n",
            "| value_loss         | 0.042497262   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002040951   |\n",
            "| clipfrac           | 0.013305664   |\n",
            "| ep_len_mean        | 73.6          |\n",
            "| ep_reward_mean     | 93            |\n",
            "| explained_variance | 0.992         |\n",
            "| fps                | 9004          |\n",
            "| n_updates          | 175           |\n",
            "| policy_entropy     | 1.0245847     |\n",
            "| policy_loss        | -0.0011798439 |\n",
            "| serial_timesteps   | 44800         |\n",
            "| time_elapsed       | 131           |\n",
            "| total_timesteps    | 716800        |\n",
            "| value_loss         | 0.007677209   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=93.25 +/- 0.07\n",
            "Episode length: 69.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000946492   |\n",
            "| clipfrac           | 0.003479004   |\n",
            "| ep_len_mean        | 70.8          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 5028          |\n",
            "| n_updates          | 176           |\n",
            "| policy_entropy     | 1.0234425     |\n",
            "| policy_loss        | -0.0007398287 |\n",
            "| serial_timesteps   | 45056         |\n",
            "| time_elapsed       | 131           |\n",
            "| total_timesteps    | 720896        |\n",
            "| value_loss         | 0.0048745675  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.001495343    |\n",
            "| clipfrac           | 0.010131836    |\n",
            "| ep_len_mean        | 71.4           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.988          |\n",
            "| fps                | 8666           |\n",
            "| n_updates          | 177            |\n",
            "| policy_entropy     | 1.0225705      |\n",
            "| policy_loss        | -0.00025057275 |\n",
            "| serial_timesteps   | 45312          |\n",
            "| time_elapsed       | 132            |\n",
            "| total_timesteps    | 724992         |\n",
            "| value_loss         | 0.0146764815   |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0017950466 |\n",
            "| clipfrac           | 0.019104004  |\n",
            "| ep_len_mean        | 71.2         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.997        |\n",
            "| fps                | 8704         |\n",
            "| n_updates          | 178          |\n",
            "| policy_entropy     | 1.0220932    |\n",
            "| policy_loss        | -0.001013648 |\n",
            "| serial_timesteps   | 45568        |\n",
            "| time_elapsed       | 132          |\n",
            "| total_timesteps    | 729088       |\n",
            "| value_loss         | 0.0034361356 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=93.30 +/- 0.06\n",
            "Episode length: 69.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017575936  |\n",
            "| clipfrac           | 0.016174316   |\n",
            "| ep_len_mean        | 71.5          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 5072          |\n",
            "| n_updates          | 179           |\n",
            "| policy_entropy     | 1.021483      |\n",
            "| policy_loss        | -0.0002941657 |\n",
            "| serial_timesteps   | 45824         |\n",
            "| time_elapsed       | 133           |\n",
            "| total_timesteps    | 733184        |\n",
            "| value_loss         | 0.016013589   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0015923649  |\n",
            "| clipfrac           | 0.011230469   |\n",
            "| ep_len_mean        | 73            |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 9012          |\n",
            "| n_updates          | 180           |\n",
            "| policy_entropy     | 1.0176681     |\n",
            "| policy_loss        | -0.0005879683 |\n",
            "| serial_timesteps   | 46080         |\n",
            "| time_elapsed       | 134           |\n",
            "| total_timesteps    | 737280        |\n",
            "| value_loss         | 0.0055492767  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=93.34 +/- 0.06\n",
            "Episode length: 69.40 +/- 0.49\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0007181146   |\n",
            "| clipfrac           | 0.002746582    |\n",
            "| ep_len_mean        | 72.5           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.984          |\n",
            "| fps                | 5052           |\n",
            "| n_updates          | 181            |\n",
            "| policy_entropy     | 1.0160574      |\n",
            "| policy_loss        | -3.0937124e-05 |\n",
            "| serial_timesteps   | 46336          |\n",
            "| time_elapsed       | 134            |\n",
            "| total_timesteps    | 741376         |\n",
            "| value_loss         | 0.020603884    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011349499  |\n",
            "| clipfrac           | 0.0045166016  |\n",
            "| ep_len_mean        | 71.9          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 8814          |\n",
            "| n_updates          | 182           |\n",
            "| policy_entropy     | 1.013315      |\n",
            "| policy_loss        | -0.0003961437 |\n",
            "| serial_timesteps   | 46592         |\n",
            "| time_elapsed       | 135           |\n",
            "| total_timesteps    | 745472        |\n",
            "| value_loss         | 0.0056012357  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0008075772   |\n",
            "| clipfrac           | 0.002746582    |\n",
            "| ep_len_mean        | 72.1           |\n",
            "| ep_reward_mean     | 93.1           |\n",
            "| explained_variance | 0.99           |\n",
            "| fps                | 8765           |\n",
            "| n_updates          | 183            |\n",
            "| policy_entropy     | 1.0111918      |\n",
            "| policy_loss        | -4.9218972e-05 |\n",
            "| serial_timesteps   | 46848          |\n",
            "| time_elapsed       | 135            |\n",
            "| total_timesteps    | 749568         |\n",
            "| value_loss         | 0.012738828    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=93.26 +/- 0.08\n",
            "Episode length: 69.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.001376541   |\n",
            "| clipfrac           | 0.010803223   |\n",
            "| ep_len_mean        | 70.8          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5098          |\n",
            "| n_updates          | 184           |\n",
            "| policy_entropy     | 1.0112281     |\n",
            "| policy_loss        | -0.0012957355 |\n",
            "| serial_timesteps   | 47104         |\n",
            "| time_elapsed       | 136           |\n",
            "| total_timesteps    | 753664        |\n",
            "| value_loss         | 0.0041761985  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002631586   |\n",
            "| clipfrac           | 0.027770996   |\n",
            "| ep_len_mean        | 70.5          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 9085          |\n",
            "| n_updates          | 185           |\n",
            "| policy_entropy     | 1.0094347     |\n",
            "| policy_loss        | -0.0020354479 |\n",
            "| serial_timesteps   | 47360         |\n",
            "| time_elapsed       | 137           |\n",
            "| total_timesteps    | 757760        |\n",
            "| value_loss         | 0.003911192   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=93.25 +/- 0.08\n",
            "Episode length: 69.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0041485974  |\n",
            "| clipfrac           | 0.044433594   |\n",
            "| ep_len_mean        | 70            |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5115          |\n",
            "| n_updates          | 186           |\n",
            "| policy_entropy     | 1.0061643     |\n",
            "| policy_loss        | -0.0024525293 |\n",
            "| serial_timesteps   | 47616         |\n",
            "| time_elapsed       | 137           |\n",
            "| total_timesteps    | 761856        |\n",
            "| value_loss         | 0.0031077827  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019265163  |\n",
            "| clipfrac           | 0.016540527   |\n",
            "| ep_len_mean        | 71.2          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.984         |\n",
            "| fps                | 8780          |\n",
            "| n_updates          | 187           |\n",
            "| policy_entropy     | 1.0018964     |\n",
            "| policy_loss        | -0.0018553918 |\n",
            "| serial_timesteps   | 47872         |\n",
            "| time_elapsed       | 138           |\n",
            "| total_timesteps    | 765952        |\n",
            "| value_loss         | 0.019214738   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=93.27 +/- 0.08\n",
            "Episode length: 69.80 +/- 0.75\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014669569  |\n",
            "| clipfrac           | 0.007446289   |\n",
            "| ep_len_mean        | 71            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 5123          |\n",
            "| n_updates          | 188           |\n",
            "| policy_entropy     | 1.0010583     |\n",
            "| policy_loss        | -0.0011816891 |\n",
            "| serial_timesteps   | 48128         |\n",
            "| time_elapsed       | 138           |\n",
            "| total_timesteps    | 770048        |\n",
            "| value_loss         | 0.0061999536  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00082365196 |\n",
            "| clipfrac           | 0.0031738281  |\n",
            "| ep_len_mean        | 70.5          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 9104          |\n",
            "| n_updates          | 189           |\n",
            "| policy_entropy     | 0.9983723     |\n",
            "| policy_loss        | -0.0005972707 |\n",
            "| serial_timesteps   | 48384         |\n",
            "| time_elapsed       | 139           |\n",
            "| total_timesteps    | 774144        |\n",
            "| value_loss         | 0.002601088   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014580752  |\n",
            "| clipfrac           | 0.008605957   |\n",
            "| ep_len_mean        | 70.1          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 7051          |\n",
            "| n_updates          | 190           |\n",
            "| policy_entropy     | 0.99626064    |\n",
            "| policy_loss        | -0.0015373583 |\n",
            "| serial_timesteps   | 48640         |\n",
            "| time_elapsed       | 140           |\n",
            "| total_timesteps    | 778240        |\n",
            "| value_loss         | 0.0026423098  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=93.31 +/- 0.04\n",
            "Episode length: 69.20 +/- 0.40\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0032745076 |\n",
            "| clipfrac           | 0.029968262  |\n",
            "| ep_len_mean        | 69.8         |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.999        |\n",
            "| fps                | 4019         |\n",
            "| n_updates          | 191          |\n",
            "| policy_entropy     | 0.99392927   |\n",
            "| policy_loss        | -0.002364782 |\n",
            "| serial_timesteps   | 48896        |\n",
            "| time_elapsed       | 140          |\n",
            "| total_timesteps    | 782336       |\n",
            "| value_loss         | 0.0015361821 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0035799008  |\n",
            "| clipfrac           | 0.037963867   |\n",
            "| ep_len_mean        | 69.2          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.992         |\n",
            "| fps                | 7777          |\n",
            "| n_updates          | 192           |\n",
            "| policy_entropy     | 0.99163914    |\n",
            "| policy_loss        | -0.0018031708 |\n",
            "| serial_timesteps   | 49152         |\n",
            "| time_elapsed       | 141           |\n",
            "| total_timesteps    | 786432        |\n",
            "| value_loss         | 0.008931938   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=93.33 +/- 0.12\n",
            "Episode length: 70.00 +/- 1.10\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0037696057  |\n",
            "| clipfrac           | 0.04498291    |\n",
            "| ep_len_mean        | 70.3          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.995         |\n",
            "| fps                | 4018          |\n",
            "| n_updates          | 193           |\n",
            "| policy_entropy     | 0.9896802     |\n",
            "| policy_loss        | -0.0029581788 |\n",
            "| serial_timesteps   | 49408         |\n",
            "| time_elapsed       | 142           |\n",
            "| total_timesteps    | 790528        |\n",
            "| value_loss         | 0.005859166   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00078556564  |\n",
            "| clipfrac           | 0.0022583008   |\n",
            "| ep_len_mean        | 71.4           |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.998          |\n",
            "| fps                | 7015           |\n",
            "| n_updates          | 194            |\n",
            "| policy_entropy     | 0.98692167     |\n",
            "| policy_loss        | -6.7113244e-05 |\n",
            "| serial_timesteps   | 49664          |\n",
            "| time_elapsed       | 143            |\n",
            "| total_timesteps    | 794624         |\n",
            "| value_loss         | 0.0028234953   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014252805  |\n",
            "| clipfrac           | 0.008178711   |\n",
            "| ep_len_mean        | 69.9          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 7062          |\n",
            "| n_updates          | 195           |\n",
            "| policy_entropy     | 0.985239      |\n",
            "| policy_loss        | -0.0011221194 |\n",
            "| serial_timesteps   | 49920         |\n",
            "| time_elapsed       | 143           |\n",
            "| total_timesteps    | 798720        |\n",
            "| value_loss         | 0.0016449802  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=93.31 +/- 0.04\n",
            "Episode length: 69.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0025442026  |\n",
            "| clipfrac           | 0.027770996   |\n",
            "| ep_len_mean        | 69.4          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 4018          |\n",
            "| n_updates          | 196           |\n",
            "| policy_entropy     | 0.98420787    |\n",
            "| policy_loss        | -0.0011928912 |\n",
            "| serial_timesteps   | 50176         |\n",
            "| time_elapsed       | 144           |\n",
            "| total_timesteps    | 802816        |\n",
            "| value_loss         | 0.015201605   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0023513439  |\n",
            "| clipfrac           | 0.021911621   |\n",
            "| ep_len_mean        | 70            |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 7262          |\n",
            "| n_updates          | 197           |\n",
            "| policy_entropy     | 0.98382545    |\n",
            "| policy_loss        | -0.0018125889 |\n",
            "| serial_timesteps   | 50432         |\n",
            "| time_elapsed       | 145           |\n",
            "| total_timesteps    | 806912        |\n",
            "| value_loss         | 0.0020176766  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=93.33 +/- 0.05\n",
            "Episode length: 69.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018545159  |\n",
            "| clipfrac           | 0.015686035   |\n",
            "| ep_len_mean        | 70.5          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 3796          |\n",
            "| n_updates          | 198           |\n",
            "| policy_entropy     | 0.98245347    |\n",
            "| policy_loss        | -0.0011562721 |\n",
            "| serial_timesteps   | 50688         |\n",
            "| time_elapsed       | 145           |\n",
            "| total_timesteps    | 811008        |\n",
            "| value_loss         | 0.019986618   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0027737184  |\n",
            "| clipfrac           | 0.029724121   |\n",
            "| ep_len_mean        | 70.6          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 6984          |\n",
            "| n_updates          | 199           |\n",
            "| policy_entropy     | 0.9848331     |\n",
            "| policy_loss        | -0.0016181681 |\n",
            "| serial_timesteps   | 50944         |\n",
            "| time_elapsed       | 147           |\n",
            "| total_timesteps    | 815104        |\n",
            "| value_loss         | 0.003573839   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0013749007   |\n",
            "| clipfrac           | 0.0119018555   |\n",
            "| ep_len_mean        | 69.5           |\n",
            "| ep_reward_mean     | 93.4           |\n",
            "| explained_variance | 0.998          |\n",
            "| fps                | 8884           |\n",
            "| n_updates          | 200            |\n",
            "| policy_entropy     | 0.9829637      |\n",
            "| policy_loss        | -0.00069565594 |\n",
            "| serial_timesteps   | 51200          |\n",
            "| time_elapsed       | 147            |\n",
            "| total_timesteps    | 819200         |\n",
            "| value_loss         | 0.002744375    |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=93.32 +/- 0.05\n",
            "Episode length: 69.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002594882   |\n",
            "| clipfrac           | 0.03100586    |\n",
            "| ep_len_mean        | 70.5          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.98          |\n",
            "| fps                | 5057          |\n",
            "| n_updates          | 201           |\n",
            "| policy_entropy     | 0.9801421     |\n",
            "| policy_loss        | -0.0017637433 |\n",
            "| serial_timesteps   | 51456         |\n",
            "| time_elapsed       | 148           |\n",
            "| total_timesteps    | 823296        |\n",
            "| value_loss         | 0.023057692   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0030481233   |\n",
            "| clipfrac           | 0.027160645    |\n",
            "| ep_len_mean        | 71             |\n",
            "| ep_reward_mean     | 93.2           |\n",
            "| explained_variance | 0.98           |\n",
            "| fps                | 7614           |\n",
            "| n_updates          | 202            |\n",
            "| policy_entropy     | 0.9773828      |\n",
            "| policy_loss        | -0.00046997724 |\n",
            "| serial_timesteps   | 51712          |\n",
            "| time_elapsed       | 148            |\n",
            "| total_timesteps    | 827392         |\n",
            "| value_loss         | 0.02435643     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=93.30 +/- 0.08\n",
            "Episode length: 69.60 +/- 0.49\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0013309259   |\n",
            "| clipfrac           | 0.009277344    |\n",
            "| ep_len_mean        | 70.2           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.999          |\n",
            "| fps                | 4230           |\n",
            "| n_updates          | 203            |\n",
            "| policy_entropy     | 0.9757458      |\n",
            "| policy_loss        | -0.00092064193 |\n",
            "| serial_timesteps   | 51968          |\n",
            "| time_elapsed       | 149            |\n",
            "| total_timesteps    | 831488         |\n",
            "| value_loss         | 0.0013955397   |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0023969093 |\n",
            "| clipfrac           | 0.024841309  |\n",
            "| ep_len_mean        | 69           |\n",
            "| ep_reward_mean     | 93.4         |\n",
            "| explained_variance | 0.998        |\n",
            "| fps                | 9129         |\n",
            "| n_updates          | 204          |\n",
            "| policy_entropy     | 0.97433984   |\n",
            "| policy_loss        | -0.002009152 |\n",
            "| serial_timesteps   | 52224        |\n",
            "| time_elapsed       | 150          |\n",
            "| total_timesteps    | 835584       |\n",
            "| value_loss         | 0.0016747448 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0020616276  |\n",
            "| clipfrac           | 0.015075684   |\n",
            "| ep_len_mean        | 68.9          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 8749          |\n",
            "| n_updates          | 205           |\n",
            "| policy_entropy     | 0.96896845    |\n",
            "| policy_loss        | -0.0013049929 |\n",
            "| serial_timesteps   | 52480         |\n",
            "| time_elapsed       | 150           |\n",
            "| total_timesteps    | 839680        |\n",
            "| value_loss         | 0.004675874   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=93.41 +/- 0.06\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.003455069   |\n",
            "| clipfrac           | 0.041809082   |\n",
            "| ep_len_mean        | 68.7          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 5051          |\n",
            "| n_updates          | 206           |\n",
            "| policy_entropy     | 0.9645439     |\n",
            "| policy_loss        | -0.0039781774 |\n",
            "| serial_timesteps   | 52736         |\n",
            "| time_elapsed       | 151           |\n",
            "| total_timesteps    | 843776        |\n",
            "| value_loss         | 0.0010865644  |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0014268032 |\n",
            "| clipfrac           | 0.012573242  |\n",
            "| ep_len_mean        | 69.7         |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.951        |\n",
            "| fps                | 8871         |\n",
            "| n_updates          | 207          |\n",
            "| policy_entropy     | 0.96112055   |\n",
            "| policy_loss        | 0.0010459687 |\n",
            "| serial_timesteps   | 52992        |\n",
            "| time_elapsed       | 152          |\n",
            "| total_timesteps    | 847872       |\n",
            "| value_loss         | 0.05133278   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=93.44 +/- 0.05\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002455697   |\n",
            "| clipfrac           | 0.0154418945  |\n",
            "| ep_len_mean        | 72.3          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.98          |\n",
            "| fps                | 5099          |\n",
            "| n_updates          | 208           |\n",
            "| policy_entropy     | 0.95870864    |\n",
            "| policy_loss        | -0.0012330301 |\n",
            "| serial_timesteps   | 53248         |\n",
            "| time_elapsed       | 152           |\n",
            "| total_timesteps    | 851968        |\n",
            "| value_loss         | 0.022903906   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0017374377 |\n",
            "| clipfrac           | 0.011962891  |\n",
            "| ep_len_mean        | 70           |\n",
            "| ep_reward_mean     | 93.3         |\n",
            "| explained_variance | 0.998        |\n",
            "| fps                | 8992         |\n",
            "| n_updates          | 209          |\n",
            "| policy_entropy     | 0.9555863    |\n",
            "| policy_loss        | -0.001311685 |\n",
            "| serial_timesteps   | 53504        |\n",
            "| time_elapsed       | 153          |\n",
            "| total_timesteps    | 856064       |\n",
            "| value_loss         | 0.0016555322 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=93.45 +/- 0.02\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018923223  |\n",
            "| clipfrac           | 0.013916016   |\n",
            "| ep_len_mean        | 68.6          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5020          |\n",
            "| n_updates          | 210           |\n",
            "| policy_entropy     | 0.95355844    |\n",
            "| policy_loss        | -0.0014007961 |\n",
            "| serial_timesteps   | 53760         |\n",
            "| time_elapsed       | 153           |\n",
            "| total_timesteps    | 860160        |\n",
            "| value_loss         | 0.0031152056  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0022482264  |\n",
            "| clipfrac           | 0.020690918   |\n",
            "| ep_len_mean        | 69.1          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 9065          |\n",
            "| n_updates          | 211           |\n",
            "| policy_entropy     | 0.9530529     |\n",
            "| policy_loss        | -0.0019914352 |\n",
            "| serial_timesteps   | 54016         |\n",
            "| time_elapsed       | 154           |\n",
            "| total_timesteps    | 864256        |\n",
            "| value_loss         | 0.0023346744  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017448405  |\n",
            "| clipfrac           | 0.016967773   |\n",
            "| ep_len_mean        | 69.3          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.984         |\n",
            "| fps                | 8952          |\n",
            "| n_updates          | 212           |\n",
            "| policy_entropy     | 0.95242083    |\n",
            "| policy_loss        | -0.0006345095 |\n",
            "| serial_timesteps   | 54272         |\n",
            "| time_elapsed       | 155           |\n",
            "| total_timesteps    | 868352        |\n",
            "| value_loss         | 0.016657604   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=93.47 +/- 0.11\n",
            "Episode length: 69.80 +/- 3.12\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0032295326  |\n",
            "| clipfrac           | 0.03857422    |\n",
            "| ep_len_mean        | 70.6          |\n",
            "| ep_reward_mean     | 93.3          |\n",
            "| explained_variance | 0.981         |\n",
            "| fps                | 4933          |\n",
            "| n_updates          | 213           |\n",
            "| policy_entropy     | 0.95250916    |\n",
            "| policy_loss        | -0.0015168063 |\n",
            "| serial_timesteps   | 54528         |\n",
            "| time_elapsed       | 155           |\n",
            "| total_timesteps    | 872448        |\n",
            "| value_loss         | 0.021882413   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0016593855   |\n",
            "| clipfrac           | 0.012207031    |\n",
            "| ep_len_mean        | 71.2           |\n",
            "| ep_reward_mean     | 93.3           |\n",
            "| explained_variance | 0.995          |\n",
            "| fps                | 8692           |\n",
            "| n_updates          | 214            |\n",
            "| policy_entropy     | 0.9513757      |\n",
            "| policy_loss        | -0.00068152655 |\n",
            "| serial_timesteps   | 54784          |\n",
            "| time_elapsed       | 156            |\n",
            "| total_timesteps    | 876544         |\n",
            "| value_loss         | 0.0053161313   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=93.40 +/- 0.06\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0024662353  |\n",
            "| clipfrac           | 0.02166748    |\n",
            "| ep_len_mean        | 69.9          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5032          |\n",
            "| n_updates          | 215           |\n",
            "| policy_entropy     | 0.9499858     |\n",
            "| policy_loss        | -0.0015288083 |\n",
            "| serial_timesteps   | 55040         |\n",
            "| time_elapsed       | 156           |\n",
            "| total_timesteps    | 880640        |\n",
            "| value_loss         | 0.0036770161  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0016009127   |\n",
            "| clipfrac           | 0.013366699    |\n",
            "| ep_len_mean        | 69             |\n",
            "| ep_reward_mean     | 93.4           |\n",
            "| explained_variance | 0.999          |\n",
            "| fps                | 8702           |\n",
            "| n_updates          | 216            |\n",
            "| policy_entropy     | 0.9509027      |\n",
            "| policy_loss        | -0.00072149443 |\n",
            "| serial_timesteps   | 55296          |\n",
            "| time_elapsed       | 157            |\n",
            "| total_timesteps    | 884736         |\n",
            "| value_loss         | 0.0013506272   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014850679  |\n",
            "| clipfrac           | 0.010437012   |\n",
            "| ep_len_mean        | 68.8          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 8871          |\n",
            "| n_updates          | 217           |\n",
            "| policy_entropy     | 0.9533832     |\n",
            "| policy_loss        | -0.0004890441 |\n",
            "| serial_timesteps   | 55552         |\n",
            "| time_elapsed       | 158           |\n",
            "| total_timesteps    | 888832        |\n",
            "| value_loss         | 0.002468859   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=93.41 +/- 0.06\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021185237  |\n",
            "| clipfrac           | 0.018676758   |\n",
            "| ep_len_mean        | 68.9          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 5113          |\n",
            "| n_updates          | 218           |\n",
            "| policy_entropy     | 0.95473623    |\n",
            "| policy_loss        | -0.0023115815 |\n",
            "| serial_timesteps   | 55808         |\n",
            "| time_elapsed       | 158           |\n",
            "| total_timesteps    | 892928        |\n",
            "| value_loss         | 0.0014367852  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018252275  |\n",
            "| clipfrac           | 0.013916016   |\n",
            "| ep_len_mean        | 68.4          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 8914          |\n",
            "| n_updates          | 219           |\n",
            "| policy_entropy     | 0.9562224     |\n",
            "| policy_loss        | -0.001644514  |\n",
            "| serial_timesteps   | 56064         |\n",
            "| time_elapsed       | 159           |\n",
            "| total_timesteps    | 897024        |\n",
            "| value_loss         | 0.00095795275 |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=93.45 +/- 0.01\n",
            "Episode length: 68.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0017526112  |\n",
            "| clipfrac           | 0.016235352   |\n",
            "| ep_len_mean        | 68.8          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.997         |\n",
            "| fps                | 5126          |\n",
            "| n_updates          | 220           |\n",
            "| policy_entropy     | 0.9541        |\n",
            "| policy_loss        | -0.0013216339 |\n",
            "| serial_timesteps   | 56320         |\n",
            "| time_elapsed       | 159           |\n",
            "| total_timesteps    | 901120        |\n",
            "| value_loss         | 0.0035293028  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002246479   |\n",
            "| clipfrac           | 0.020996094   |\n",
            "| ep_len_mean        | 68.8          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 8752          |\n",
            "| n_updates          | 221           |\n",
            "| policy_entropy     | 0.95132506    |\n",
            "| policy_loss        | -0.0013472794 |\n",
            "| serial_timesteps   | 56576         |\n",
            "| time_elapsed       | 160           |\n",
            "| total_timesteps    | 905216        |\n",
            "| value_loss         | 0.0023436225  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0028805009  |\n",
            "| clipfrac           | 0.033203125   |\n",
            "| ep_len_mean        | 68.3          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 8967          |\n",
            "| n_updates          | 222           |\n",
            "| policy_entropy     | 0.9506912     |\n",
            "| policy_loss        | -0.0030356403 |\n",
            "| serial_timesteps   | 56832         |\n",
            "| time_elapsed       | 161           |\n",
            "| total_timesteps    | 909312        |\n",
            "| value_loss         | 0.0009974954  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=93.50 +/- 0.10\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0038252587  |\n",
            "| clipfrac           | 0.04888916    |\n",
            "| ep_len_mean        | 69            |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 5007          |\n",
            "| n_updates          | 223           |\n",
            "| policy_entropy     | 0.9487778     |\n",
            "| policy_loss        | -0.0013134595 |\n",
            "| serial_timesteps   | 57088         |\n",
            "| time_elapsed       | 161           |\n",
            "| total_timesteps    | 913408        |\n",
            "| value_loss         | 0.018009067   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0006939989 |\n",
            "| clipfrac           | 0.0014648438 |\n",
            "| ep_len_mean        | 71.5         |\n",
            "| ep_reward_mean     | 93.2         |\n",
            "| explained_variance | 0.954        |\n",
            "| fps                | 8763         |\n",
            "| n_updates          | 224          |\n",
            "| policy_entropy     | 0.9471356    |\n",
            "| policy_loss        | 0.0005213215 |\n",
            "| serial_timesteps   | 57344        |\n",
            "| time_elapsed       | 162          |\n",
            "| total_timesteps    | 917504       |\n",
            "| value_loss         | 0.048271757  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=93.44 +/- 0.01\n",
            "Episode length: 68.00 +/- 0.00\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014398084  |\n",
            "| clipfrac           | 0.0087890625  |\n",
            "| ep_len_mean        | 71.3          |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.989         |\n",
            "| fps                | 5106          |\n",
            "| n_updates          | 225           |\n",
            "| policy_entropy     | 0.94743276    |\n",
            "| policy_loss        | -0.0010080175 |\n",
            "| serial_timesteps   | 57600         |\n",
            "| time_elapsed       | 162           |\n",
            "| total_timesteps    | 921600        |\n",
            "| value_loss         | 0.012370775   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026776649  |\n",
            "| clipfrac           | 0.023803711   |\n",
            "| ep_len_mean        | 69.2          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 9149          |\n",
            "| n_updates          | 226           |\n",
            "| policy_entropy     | 0.94678974    |\n",
            "| policy_loss        | -0.0008429637 |\n",
            "| serial_timesteps   | 57856         |\n",
            "| time_elapsed       | 163           |\n",
            "| total_timesteps    | 925696        |\n",
            "| value_loss         | 0.0017217437  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0016225511   |\n",
            "| clipfrac           | 0.012329102    |\n",
            "| ep_len_mean        | 68.3           |\n",
            "| ep_reward_mean     | 93.5           |\n",
            "| explained_variance | 0.996          |\n",
            "| fps                | 8839           |\n",
            "| n_updates          | 227            |\n",
            "| policy_entropy     | 0.94309163     |\n",
            "| policy_loss        | -0.00061969255 |\n",
            "| serial_timesteps   | 58112          |\n",
            "| time_elapsed       | 164            |\n",
            "| total_timesteps    | 929792         |\n",
            "| value_loss         | 0.0041111284   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=93.41 +/- 0.05\n",
            "Episode length: 68.40 +/- 0.49\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0040247836  |\n",
            "| clipfrac           | 0.045715332   |\n",
            "| ep_len_mean        | 68.8          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.968         |\n",
            "| fps                | 5055          |\n",
            "| n_updates          | 228           |\n",
            "| policy_entropy     | 0.9406805     |\n",
            "| policy_loss        | -0.0018935627 |\n",
            "| serial_timesteps   | 58368         |\n",
            "| time_elapsed       | 164           |\n",
            "| total_timesteps    | 933888        |\n",
            "| value_loss         | 0.033924233   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0019905309  |\n",
            "| clipfrac           | 0.016052246   |\n",
            "| ep_len_mean        | 72.3          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.941         |\n",
            "| fps                | 8814          |\n",
            "| n_updates          | 229           |\n",
            "| policy_entropy     | 0.9416084     |\n",
            "| policy_loss        | -0.0009846184 |\n",
            "| serial_timesteps   | 58624         |\n",
            "| time_elapsed       | 165           |\n",
            "| total_timesteps    | 937984        |\n",
            "| value_loss         | 0.06237421    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=93.49 +/- 0.08\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00473955    |\n",
            "| clipfrac           | 0.057800293   |\n",
            "| ep_len_mean        | 72.8          |\n",
            "| ep_reward_mean     | 93.1          |\n",
            "| explained_variance | 0.951         |\n",
            "| fps                | 5053          |\n",
            "| n_updates          | 230           |\n",
            "| policy_entropy     | 0.94092405    |\n",
            "| policy_loss        | -0.0005160108 |\n",
            "| serial_timesteps   | 58880         |\n",
            "| time_elapsed       | 165           |\n",
            "| total_timesteps    | 942080        |\n",
            "| value_loss         | 0.05444293    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0026493922  |\n",
            "| clipfrac           | 0.024902344   |\n",
            "| ep_len_mean        | 71            |\n",
            "| ep_reward_mean     | 93.2          |\n",
            "| explained_variance | 0.987         |\n",
            "| fps                | 8854          |\n",
            "| n_updates          | 231           |\n",
            "| policy_entropy     | 0.9391272     |\n",
            "| policy_loss        | -0.0010599977 |\n",
            "| serial_timesteps   | 59136         |\n",
            "| time_elapsed       | 166           |\n",
            "| total_timesteps    | 946176        |\n",
            "| value_loss         | 0.01139138    |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=93.46 +/- 0.01\n",
            "Episode length: 68.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.001098187  |\n",
            "| clipfrac           | 0.0040283203 |\n",
            "| ep_len_mean        | 68.8         |\n",
            "| ep_reward_mean     | 93.5         |\n",
            "| explained_variance | 0.996        |\n",
            "| fps                | 5189         |\n",
            "| n_updates          | 232          |\n",
            "| policy_entropy     | 0.9382913    |\n",
            "| policy_loss        | 0.0005090714 |\n",
            "| serial_timesteps   | 59392        |\n",
            "| time_elapsed       | 167          |\n",
            "| total_timesteps    | 950272       |\n",
            "| value_loss         | 0.003047104  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0025581333  |\n",
            "| clipfrac           | 0.025146484   |\n",
            "| ep_len_mean        | 68.6          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 9072          |\n",
            "| n_updates          | 233           |\n",
            "| policy_entropy     | 0.9378085     |\n",
            "| policy_loss        | -0.0020752253 |\n",
            "| serial_timesteps   | 59648         |\n",
            "| time_elapsed       | 167           |\n",
            "| total_timesteps    | 954368        |\n",
            "| value_loss         | 0.0019291929  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00021695526 |\n",
            "| clipfrac           | 0.00030517578 |\n",
            "| ep_len_mean        | 69.3          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.982         |\n",
            "| fps                | 8386          |\n",
            "| n_updates          | 234           |\n",
            "| policy_entropy     | 0.9394726     |\n",
            "| policy_loss        | 0.00015688455 |\n",
            "| serial_timesteps   | 59904         |\n",
            "| time_elapsed       | 168           |\n",
            "| total_timesteps    | 958464        |\n",
            "| value_loss         | 0.019921744   |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=93.44 +/- 0.06\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010163028  |\n",
            "| clipfrac           | 0.0045776367  |\n",
            "| ep_len_mean        | 68.9          |\n",
            "| ep_reward_mean     | 93.4          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 5033          |\n",
            "| n_updates          | 235           |\n",
            "| policy_entropy     | 0.937932      |\n",
            "| policy_loss        | -0.0015034092 |\n",
            "| serial_timesteps   | 60160         |\n",
            "| time_elapsed       | 168           |\n",
            "| total_timesteps    | 962560        |\n",
            "| value_loss         | 0.001668208   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0014735422  |\n",
            "| clipfrac           | 0.01171875    |\n",
            "| ep_len_mean        | 68.2          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.998         |\n",
            "| fps                | 8811          |\n",
            "| n_updates          | 236           |\n",
            "| policy_entropy     | 0.93435633    |\n",
            "| policy_loss        | -0.0021876078 |\n",
            "| serial_timesteps   | 60416         |\n",
            "| time_elapsed       | 169           |\n",
            "| total_timesteps    | 966656        |\n",
            "| value_loss         | 0.0015150595  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=93.43 +/- 0.04\n",
            "Episode length: 68.20 +/- 0.40\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0005156223   |\n",
            "| clipfrac           | 0.0018310547   |\n",
            "| ep_len_mean        | 69.2           |\n",
            "| ep_reward_mean     | 93.4           |\n",
            "| explained_variance | 0.981          |\n",
            "| fps                | 5171           |\n",
            "| n_updates          | 237            |\n",
            "| policy_entropy     | 0.9325632      |\n",
            "| policy_loss        | -1.8896098e-05 |\n",
            "| serial_timesteps   | 60672          |\n",
            "| time_elapsed       | 170            |\n",
            "| total_timesteps    | 970752         |\n",
            "| value_loss         | 0.019236406    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0026995216   |\n",
            "| clipfrac           | 0.030883789    |\n",
            "| ep_len_mean        | 69.6           |\n",
            "| ep_reward_mean     | 93.4           |\n",
            "| explained_variance | 0.984          |\n",
            "| fps                | 8694           |\n",
            "| n_updates          | 238            |\n",
            "| policy_entropy     | 0.93181        |\n",
            "| policy_loss        | -0.00050350634 |\n",
            "| serial_timesteps   | 60928          |\n",
            "| time_elapsed       | 170            |\n",
            "| total_timesteps    | 974848         |\n",
            "| value_loss         | 0.01767097     |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.001814493  |\n",
            "| clipfrac           | 0.012207031  |\n",
            "| ep_len_mean        | 68.8         |\n",
            "| ep_reward_mean     | 93.4         |\n",
            "| explained_variance | 0.998        |\n",
            "| fps                | 8698         |\n",
            "| n_updates          | 239          |\n",
            "| policy_entropy     | 0.9295953    |\n",
            "| policy_loss        | -0.001416875 |\n",
            "| serial_timesteps   | 61184        |\n",
            "| time_elapsed       | 171          |\n",
            "| total_timesteps    | 978944       |\n",
            "| value_loss         | 0.0014367546 |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=93.44 +/- 0.05\n",
            "Episode length: 68.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| approxkl           | 0.002081736   |\n",
            "| clipfrac           | 0.018493652   |\n",
            "| ep_len_mean        | 68.1          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.996         |\n",
            "| fps                | 5055          |\n",
            "| n_updates          | 240           |\n",
            "| policy_entropy     | 0.9240358     |\n",
            "| policy_loss        | -0.0010181455 |\n",
            "| serial_timesteps   | 61440         |\n",
            "| time_elapsed       | 171           |\n",
            "| total_timesteps    | 983040        |\n",
            "| value_loss         | 0.0038031547  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0032422943  |\n",
            "| clipfrac           | 0.034484863   |\n",
            "| ep_len_mean        | 68            |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 8972          |\n",
            "| n_updates          | 241           |\n",
            "| policy_entropy     | 0.92075574    |\n",
            "| policy_loss        | -0.0028878702 |\n",
            "| serial_timesteps   | 61696         |\n",
            "| time_elapsed       | 172           |\n",
            "| total_timesteps    | 987136        |\n",
            "| value_loss         | 0.0008664542  |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=93.46 +/- 0.01\n",
            "Episode length: 68.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0038118686 |\n",
            "| clipfrac           | 0.04260254   |\n",
            "| ep_len_mean        | 67.8         |\n",
            "| ep_reward_mean     | 93.5         |\n",
            "| explained_variance | 0.999        |\n",
            "| fps                | 5134         |\n",
            "| n_updates          | 242          |\n",
            "| policy_entropy     | 0.9194388    |\n",
            "| policy_loss        | -0.002865188 |\n",
            "| serial_timesteps   | 61952        |\n",
            "| time_elapsed       | 173          |\n",
            "| total_timesteps    | 991232       |\n",
            "| value_loss         | 0.000955056  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0035013508  |\n",
            "| clipfrac           | 0.035888672   |\n",
            "| ep_len_mean        | 67.7          |\n",
            "| ep_reward_mean     | 93.5          |\n",
            "| explained_variance | 0.999         |\n",
            "| fps                | 8918          |\n",
            "| n_updates          | 243           |\n",
            "| policy_entropy     | 0.9188707     |\n",
            "| policy_loss        | -0.0024517637 |\n",
            "| serial_timesteps   | 62208         |\n",
            "| time_elapsed       | 173           |\n",
            "| total_timesteps    | 995328        |\n",
            "| value_loss         | 0.00085769873 |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0021224113 |\n",
            "| clipfrac           | 0.020080566  |\n",
            "| ep_len_mean        | 67.9         |\n",
            "| ep_reward_mean     | 93.5         |\n",
            "| explained_variance | 0.999        |\n",
            "| fps                | 8873         |\n",
            "| n_updates          | 244          |\n",
            "| policy_entropy     | 0.9146245    |\n",
            "| policy_loss        | -0.002179942 |\n",
            "| serial_timesteps   | 62464        |\n",
            "| time_elapsed       | 174          |\n",
            "| total_timesteps    | 999424       |\n",
            "| value_loss         | 0.0010709271 |\n",
            "-------------------------------------\n",
            "Saving to logs/ppo2/MountainCarContinuous-v0_1\n",
            "[77e9fbce07ac:03032] *** Process received signal ***\n",
            "[77e9fbce07ac:03032] Signal: Segmentation fault (11)\n",
            "[77e9fbce07ac:03032] Signal code: Address not mapped (1)\n",
            "[77e9fbce07ac:03032] Failing at address: 0x7f1d1e5a820d\n",
            "[77e9fbce07ac:03032] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f1d21253980]\n",
            "[77e9fbce07ac:03032] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f1d20e928a5]\n",
            "[77e9fbce07ac:03032] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f1d216fde44]\n",
            "[77e9fbce07ac:03032] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f1d20e93735]\n",
            "[77e9fbce07ac:03032] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f1d216fbcb3]\n",
            "[77e9fbce07ac:03032] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3OYGGy_QxBM",
        "outputId": "0866dc1c-0988-4a16-ea84-62ce66e915e1"
      },
      "source": [
        "# Overwritting the baseline timesteps model\n",
        "\n",
        "!python train.py --algo ppo2 --env MountainCarContinuous-v0 --n-timesteps 10000 #--save-freq 1000\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MountainCarContinuous-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.94),\n",
            "             ('n_envs', 16),\n",
            "             ('n_steps', 256),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 8),\n",
            "             ('noptepochs', 4),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=10000\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MountainCarContinuous-v0_2\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00072787184 |\n",
            "| clipfrac           | 0.0022583008  |\n",
            "| explained_variance | -0.0333       |\n",
            "| fps                | 6006          |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 1.4160135     |\n",
            "| policy_loss        | -0.0010549161 |\n",
            "| serial_timesteps   | 256           |\n",
            "| time_elapsed       | 2.22e-05      |\n",
            "| total_timesteps    | 4096          |\n",
            "| value_loss         | 0.5902689     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.000113212736 |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | -0.104         |\n",
            "| fps                | 9022           |\n",
            "| n_updates          | 2              |\n",
            "| policy_entropy     | 1.4079745      |\n",
            "| policy_loss        | -0.0010972274  |\n",
            "| serial_timesteps   | 512            |\n",
            "| time_elapsed       | 0.682          |\n",
            "| total_timesteps    | 8192           |\n",
            "| value_loss         | 0.1042344      |\n",
            "---------------------------------------\n",
            "Saving to logs/ppo2/MountainCarContinuous-v0_2\n",
            "[77e9fbce07ac:03075] *** Process received signal ***\n",
            "[77e9fbce07ac:03075] Signal: Segmentation fault (11)\n",
            "[77e9fbce07ac:03075] Signal code: Address not mapped (1)\n",
            "[77e9fbce07ac:03075] Failing at address: 0x7f21ab9b420d\n",
            "[77e9fbce07ac:03075] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f21ae65f980]\n",
            "[77e9fbce07ac:03075] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f21ae29e8a5]\n",
            "[77e9fbce07ac:03075] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f21aeb09e44]\n",
            "[77e9fbce07ac:03075] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f21ae29f735]\n",
            "[77e9fbce07ac:03075] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f21aeb07cb3]\n",
            "[77e9fbce07ac:03075] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5om6tse4YPBB"
      },
      "source": [
        "## Optimizing search of parameters with Optuna\n",
        "\n",
        "  - link of optuna: Used for changing hyperparameter (looking for the best so it optimizes a search)\n",
        "  - https://colab.research.google.com/github/optuna/optuna/blob/master/examples/quickstart.ipynb\n",
        "  - Research related to the algorithm Optuna (https://arxiv.org/pdf/1907.10902.pdf)\n",
        "  - That is why we add an hyperparameter optimizer search (up to the search of 10 parameters based on a n (=300, in our exercise) sample trial\n",
        "  - There are many selection of hyperparameters that can be chosen, but in terms of cost-effectiveness some could be skipped (or prunned)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_9Qyyalz-Xm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319e126c-2a7b-49da-e686-5a222eb12189"
      },
      "source": [
        "#%tensorboard --logdir=tensorboard\n",
        "!python train.py --algo ppo2 --env MountainCarContinuous-v0 -n 50000 -optimize --n-trials 300 --n-jobs 2 --sampler tpe --pruner median\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MountainCarContinuous-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.94),\n",
            "             ('n_envs', 16),\n",
            "             ('n_steps', 256),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 8),\n",
            "             ('noptepochs', 4),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=50000\n",
            "Normalizing input and reward\n",
            "Optimizing hyperparameters\n",
            "Sampler: tpe - Pruner: median\n",
            "\u001b[32m[I 2021-03-19 14:44:52,481]\u001b[0m A new study created in memory with name: no-name-35872e91-0846-4263-8b8e-ea341e796f03\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalizing input and reward\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:47:53,176]\u001b[0m Trial 0 finished with value: 2.479185968695674e-05 and parameters: {'batch_size': 64, 'n_steps': 2048, 'gamma': 0.99, 'lr': 3.9593049302166474e-05, 'ent_coef': 7.683809505828654e-06, 'cliprange': 0.1, 'noptepochs': 50, 'lambda': 0.9}. Best is trial 0 with value: 2.479185968695674e-05.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:48:46,101]\u001b[0m Trial 1 finished with value: 0.03835057467222214 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.0001957132697680036, 'ent_coef': 0.00014792911172195836, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 0 with value: 2.479185968695674e-05.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:51:36,591]\u001b[0m Trial 3 finished with value: 1.0547242709435523e-05 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.9, 'lr': 0.0017630805649951789, 'ent_coef': 0.00011755942144170166, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 0.98}. Best is trial 3 with value: 1.0547242709435523e-05.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:51:46,629]\u001b[0m Trial 2 finished with value: -98.11212158203125 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00040439804087355876, 'ent_coef': 5.3070689857745075e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:55:07,731]\u001b[0m Trial 4 finished with value: 99.8990478515625 and parameters: {'batch_size': 64, 'n_steps': 16, 'gamma': 0.9, 'lr': 0.19676640808550347, 'ent_coef': 6.78231367357659e-05, 'cliprange': 0.4, 'noptepochs': 50, 'lambda': 0.98}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:55:36,303]\u001b[0m Trial 5 finished with value: -97.49061584472656 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00019978716190919521, 'ent_coef': 0.029631005344805793, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:58:07,474]\u001b[0m Trial 6 finished with value: 9.76723458734341e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.999, 'lr': 0.36373600104557074, 'ent_coef': 0.0004260848102998029, 'cliprange': 0.1, 'noptepochs': 50, 'lambda': 0.95}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 14:58:20,786]\u001b[0m Trial 7 finished with value: 95.83981323242188 and parameters: {'batch_size': 256, 'n_steps': 32, 'gamma': 0.98, 'lr': 0.008036823471884707, 'ent_coef': 0.0026906441138624237, 'cliprange': 0.1, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:02:16,722]\u001b[0m Trial 8 finished with value: 0.12172162532806396 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.002701664157747044, 'ent_coef': 1.668574645875183e-08, 'cliprange': 0.1, 'noptepochs': 10, 'lambda': 0.9}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:02:25,865]\u001b[0m Trial 9 finished with value: 0.03436021879315376 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.01962178667848197, 'ent_coef': 1.1704551145623017e-07, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:03:36,763]\u001b[0m Trial 10 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:06:30,830]\u001b[0m Trial 11 finished with value: 0.0007682001451030374 and parameters: {'batch_size': 256, 'n_steps': 512, 'gamma': 0.995, 'lr': 1.2208229413789481e-05, 'ent_coef': 2.6808620874762475e-06, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 0.8}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:07:50,940]\u001b[0m Trial 12 finished with value: 0.0005766440299339592 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'lr': 1.8024964641995533e-05, 'ent_coef': 0.08869037731058829, 'cliprange': 0.3, 'noptepochs': 30, 'lambda': 0.95}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:10:35,897]\u001b[0m Trial 13 finished with value: 0.04034343734383583 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00012726599674660344, 'ent_coef': 0.08421016257647383, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:11:55,841]\u001b[0m Trial 14 finished with value: 0.744507908821106 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.995, 'lr': 0.0001671345227695574, 'ent_coef': 0.09503632408427945, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:13:52,888]\u001b[0m Trial 16 finished with value: -95.39118957519531 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.0005993930218643873, 'ent_coef': 1.2218505694336263e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:14:39,345]\u001b[0m Trial 15 finished with value: 1.2783794403076172 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.995, 'lr': 0.000998076351492054, 'ent_coef': 9.212797069935619e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:15:59,469]\u001b[0m Trial 18 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:17:20,656]\u001b[0m Trial 19 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:17:56,576]\u001b[0m Trial 17 finished with value: 0.0012737723300233483 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.995, 'lr': 4.1827517947982434e-05, 'ent_coef': 0.007927275972781407, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:21:00,404]\u001b[0m Trial 21 finished with value: -95.18550109863281 and parameters: {'batch_size': 256, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.005519403617255943, 'ent_coef': 1.8533561828192526e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:21:23,465]\u001b[0m Trial 20 finished with value: 0.002464982680976391 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.999, 'lr': 5.558849471198219e-05, 'ent_coef': 0.009850370080090572, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:22:17,384]\u001b[0m Trial 22 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:23:24,994]\u001b[0m Trial 23 finished with value: -95.29508972167969 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.0006347455247162761, 'ent_coef': 1.182866789350006e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:24:44,591]\u001b[0m Trial 24 finished with value: -96.8970947265625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004564986881048647, 'ent_coef': 1.812458385102629e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:25:31,867]\u001b[0m Trial 25 finished with value: -94.97367858886719 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003328959613032049, 'ent_coef': 1.720807405967486e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:27:44,401]\u001b[0m Trial 26 finished with value: -95.3174057006836 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00022276346517513326, 'ent_coef': 2.2254358636035424e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:28:26,264]\u001b[0m Trial 27 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:30:34,326]\u001b[0m Trial 28 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:32:04,037]\u001b[0m Trial 29 finished with value: -95.28102111816406 and parameters: {'batch_size': 256, 'n_steps': 512, 'gamma': 0.99, 'lr': 0.001425974887417801, 'ent_coef': 4.309040123877741e-06, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 0.9}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:34:39,203]\u001b[0m Trial 30 finished with value: 0.2726535201072693 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.99, 'lr': 0.0016392861681431266, 'ent_coef': 4.896998059245534e-06, 'cliprange': 0.4, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:35:57,868]\u001b[0m Trial 32 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:36:03,726]\u001b[0m Trial 31 finished with value: 2.4860361008904874e-05 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.995, 'lr': 1.6381904707237502e-05, 'ent_coef': 6.771572731064265e-06, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 2 with value: -98.11212158203125.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:39:58,591]\u001b[0m Trial 34 finished with value: -98.6810073852539 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00025941891930756966, 'ent_coef': 2.101018470568001e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:40:02,722]\u001b[0m Trial 33 finished with value: 0.0030926072504371405 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 2.075612061841899e-05, 'ent_coef': 4.7386229235527475e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:41:55,089]\u001b[0m Trial 36 finished with value: -94.9090805053711 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.00023463422145841254, 'ent_coef': 0.0001704666238849975, 'cliprange': 0.4, 'noptepochs': 50, 'lambda': 0.98}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:44:16,286]\u001b[0m Trial 35 finished with value: 0.044005561619997025 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00025388071016347, 'ent_coef': 6.120748242000219e-05, 'cliprange': 0.2, 'noptepochs': 50, 'lambda': 0.98}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:46:02,561]\u001b[0m Trial 37 finished with value: 0.02585427835583687 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0001490068142386659, 'ent_coef': 6.627490347551006e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:47:03,325]\u001b[0m Trial 38 finished with value: 8.69665473146597e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.98, 'lr': 0.0033691979723057775, 'ent_coef': 2.3396231896736218e-06, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:50:09,723]\u001b[0m Trial 39 finished with value: 0.005883403588086367 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.004564839493630004, 'ent_coef': 2.5071691404528494e-06, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 0.8}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:51:11,292]\u001b[0m Trial 40 finished with value: 0.0005618958966806531 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.002987347206360759, 'ent_coef': 3.337041490327049e-07, 'cliprange': 0.1, 'noptepochs': 10, 'lambda': 0.8}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:51:29,770]\u001b[0m Trial 41 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:55:16,942]\u001b[0m Trial 43 finished with value: 0.016636978834867477 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.0008391796405408698, 'ent_coef': 1.2775984345870495e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:55:24,077]\u001b[0m Trial 42 finished with value: 0.05889545753598213 and parameters: {'batch_size': 32, 'n_steps': 16, 'gamma': 0.95, 'lr': 0.0010358302799514864, 'ent_coef': 1.0664688005798827e-08, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:57:58,962]\u001b[0m Trial 44 finished with value: 8.851862730807625e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.00039362196219130686, 'ent_coef': 2.820888376344373e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 15:58:12,264]\u001b[0m Trial 45 finished with value: 7.952169653435703e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.0003999640915821973, 'ent_coef': 0.0002191103329495375, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:02:03,697]\u001b[0m Trial 46 finished with value: 1.3915449380874634 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00012595725820717325, 'ent_coef': 1.5030744355756743e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:02:15,276]\u001b[0m Trial 47 finished with value: 0.014185783453285694 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.95, 'lr': 8.520663815158874e-05, 'ent_coef': 4.8461906732719754e-08, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:06:01,885]\u001b[0m Trial 48 finished with value: 0.0038488120771944523 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 8.471115653572753e-05, 'ent_coef': 3.573331899134388e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:06:19,808]\u001b[0m Trial 49 finished with value: 0.007713118102401495 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.995, 'lr': 3.2778023307709526e-05, 'ent_coef': 4.917060413551364e-07, 'cliprange': 0.2, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:07:22,060]\u001b[0m Trial 50 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:07:38,819]\u001b[0m Trial 51 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:11:24,269]\u001b[0m Trial 52 finished with value: 0.02836737595498562 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00018023916274965065, 'ent_coef': 2.753774667804948e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "\u001b[32m[I 2021-03-19 16:11:26,225]\u001b[0m Trial 53 finished with value: -97.8765869140625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00022734878212204266, 'ent_coef': 2.613796803270577e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:12:45,195]\u001b[0m Trial 55 finished with value: -92.1409912109375 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.00043136104621912147, 'ent_coef': 3.2603797929573756e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:14:26,138]\u001b[0m Trial 54 finished with value: -95.23594665527344 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00028952818654938087, 'ent_coef': 1.6902119404432205e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:16:51,152]\u001b[0m Trial 56 finished with value: 0.04701885208487511 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0009626647519757059, 'ent_coef': 1.5481176011066292e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:17:21,083]\u001b[0m Trial 57 finished with value: -78.0025863647461 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.0009297391821852721, 'ent_coef': 0.03438936511680032, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:18:10,912]\u001b[0m Trial 58 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:18:43,128]\u001b[0m Trial 59 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:20:06,044]\u001b[0m Trial 60 finished with value: -45.24247360229492 and parameters: {'batch_size': 256, 'n_steps': 128, 'gamma': 0.9999, 'lr': 0.00012443600940553948, 'ent_coef': 7.617544914943972e-07, 'cliprange': 0.4, 'noptepochs': 50, 'lambda': 0.9}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:22:50,991]\u001b[0m Trial 61 finished with value: 99.69564056396484 and parameters: {'batch_size': 256, 'n_steps': 1024, 'gamma': 0.995, 'lr': 0.8005665430532578, 'ent_coef': 1.3598447335814073e-06, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 0.99}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:24:08,602]\u001b[0m Trial 62 finished with value: 0.09300033748149872 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00020499845516410328, 'ent_coef': 2.515117728050503e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:26:26,913]\u001b[0m Trial 63 finished with value: -96.49452209472656 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00020506935512446304, 'ent_coef': 3.5808102600885877e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:28:14,811]\u001b[0m Trial 64 finished with value: 0.0912131518125534 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00023405275406194835, 'ent_coef': 0.0003275424173782244, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 34 with value: -98.6810073852539.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:30:26,742]\u001b[0m Trial 65 finished with value: -98.85881042480469 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00031837946717356387, 'ent_coef': 0.0019182523173234608, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 65 with value: -98.85881042480469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:32:18,128]\u001b[0m Trial 66 finished with value: 0.00020616501569747925 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 5.701832773943125e-05, 'ent_coef': 4.265423110886822e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 65 with value: -98.85881042480469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:34:31,183]\u001b[0m Trial 67 finished with value: 0.0013460059417411685 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 5.3024378165932406e-05, 'ent_coef': 0.002106806743224379, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 65 with value: -98.85881042480469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:36:17,269]\u001b[0m Trial 68 finished with value: -98.95207214355469 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00011089599119554573, 'ent_coef': 0.026229328836409407, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:38:38,824]\u001b[0m Trial 69 finished with value: 0.08180613815784454 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.00011074606759529507, 'ent_coef': 0.014325438686161611, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:40:23,965]\u001b[0m Trial 70 finished with value: 0.011047503910958767 and parameters: {'batch_size': 128, 'n_steps': 512, 'gamma': 0.99, 'lr': 0.00010596227879046445, 'ent_coef': 0.02263139206756122, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:42:44,570]\u001b[0m Trial 71 finished with value: 0.0015987928491085768 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.99, 'lr': 3.229859936361967e-05, 'ent_coef': 0.05361968729130006, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:44:31,324]\u001b[0m Trial 72 finished with value: 0.1176014319062233 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 3.6728581507019164e-05, 'ent_coef': 0.06412908987718782, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:44:43,591]\u001b[0m Trial 73 finished with value: -95.61804962158203 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00032462206358342444, 'ent_coef': 0.00540190931401892, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:48:34,349]\u001b[0m Trial 74 finished with value: 0.15825067460536957 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.00016505119025037214, 'ent_coef': 0.004325363124766509, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:48:46,523]\u001b[0m Trial 75 finished with value: 0.0070901187136769295 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.0001575559969698096, 'ent_coef': 0.0023734833925877336, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:52:34,620]\u001b[0m Trial 76 finished with value: 0.31205815076828003 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004957594480836681, 'ent_coef': 4.385408614042954e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "\u001b[32m[I 2021-03-19 16:52:36,437]\u001b[0m Trial 77 finished with value: -98.65316009521484 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0005034808111289083, 'ent_coef': 0.000944050131903079, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:54:49,647]\u001b[0m Trial 79 finished with value: 29.662200927734375 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.995, 'lr': 0.0013047004456248623, 'ent_coef': 0.0006622121630153109, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 0.92}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:56:38,796]\u001b[0m Trial 78 finished with value: 0.005808723159134388 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.995, 'lr': 6.604901529055241e-05, 'ent_coef': 8.736997813751239e-05, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 16:58:53,419]\u001b[0m Trial 80 finished with value: 0.019164210185408592 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00031061152333297236, 'ent_coef': 0.010746314558399568, 'cliprange': 0.3, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:00:13,408]\u001b[0m Trial 82 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:00:41,343]\u001b[0m Trial 81 finished with value: 0.0012116010766476393 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0003290036650880054, 'ent_coef': 0.0013161896757492549, 'cliprange': 0.3, 'noptepochs': 1, 'lambda': 0.95}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:02:07,476]\u001b[0m Trial 83 finished with value: -94.64328002929688 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00044736254595484755, 'ent_coef': 0.0011442538972463287, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:03:27,015]\u001b[0m Trial 85 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:03:44,294]\u001b[0m Trial 84 finished with value: -96.91104888916016 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004884728399410808, 'ent_coef': 1.1305356824496258e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:07:33,431]\u001b[0m Trial 86 finished with value: 0.024003325030207634 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000740813344048939, 'ent_coef': 0.00027114825912358183, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 0.99}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:07:51,473]\u001b[0m Trial 87 finished with value: 0.03893955796957016 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0007627105822844693, 'ent_coef': 0.022720034757575627, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.99}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:10:37,027]\u001b[0m Trial 88 finished with value: -96.97540283203125 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005240525195269763, 'ent_coef': 1.7958154610311793e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:11:54,569]\u001b[0m Trial 89 finished with value: 0.014172373339533806 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00047460076517143196, 'ent_coef': 6.947072109224456e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:13:12,982]\u001b[0m Trial 91 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:14:18,455]\u001b[0m Trial 90 finished with value: -98.36399841308594 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0011702084405374696, 'ent_coef': 1.9116191744138204e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:17:16,545]\u001b[0m Trial 92 finished with value: 0.0005861685494892299 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0011736232770393309, 'ent_coef': 2.191941662821961e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:18:20,639]\u001b[0m Trial 93 finished with value: 0.027003884315490723 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0005706058479969515, 'ent_coef': 2.3574909628477325e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:21:17,805]\u001b[0m Trial 95 finished with value: -96.81341552734375 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0014212863676067438, 'ent_coef': 4.068789794100275e-06, 'cliprange': 0.3, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "\u001b[32m[I 2021-03-19 17:21:19,293]\u001b[0m Trial 94 finished with value: 0.12542104721069336 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0005840385973188033, 'ent_coef': 8.441060649500968e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:23:57,902]\u001b[0m Trial 96 finished with value: 9.36886681301985e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.98, 'lr': 0.0023366124388174508, 'ent_coef': 1.0070348663611673e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:24:06,763]\u001b[0m Trial 97 finished with value: 7.351063686655834e-06 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.995, 'lr': 0.0021734645329031686, 'ent_coef': 1.5585092800434085e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:27:57,864]\u001b[0m Trial 98 finished with value: 0.6697169542312622 and parameters: {'batch_size': 128, 'n_steps': 1024, 'gamma': 0.995, 'lr': 0.0003510165004648907, 'ent_coef': 5.315015172431423e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:28:05,909]\u001b[0m Trial 99 finished with value: 0.0416058786213398 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.95, 'lr': 0.00037401123284118285, 'ent_coef': 5.893179466781584e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:29:48,312]\u001b[0m Trial 101 finished with value: -93.72032165527344 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00025539665438072324, 'ent_coef': 6.081252153332487e-07, 'cliprange': 0.4, 'noptepochs': 10, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:32:00,055]\u001b[0m Trial 100 finished with value: 1.2740747928619385 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.00024092089334396073, 'ent_coef': 5.785412840826238e-07, 'cliprange': 0.3, 'noptepochs': 10, 'lambda': 0.8}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:33:32,633]\u001b[0m Trial 102 finished with value: -98.33882904052734 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00013788519081619764, 'ent_coef': 8.607464393694706e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:36:03,741]\u001b[0m Trial 103 finished with value: 0.02444997802376747 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0008692032564472196, 'ent_coef': 1.8460910488802472e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:37:36,174]\u001b[0m Trial 104 finished with value: 0.003340085269883275 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00010365262155953773, 'ent_coef': 5.6167253478797115e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:40:05,955]\u001b[0m Trial 105 finished with value: 0.01742219179868698 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0001385022877617096, 'ent_coef': 8.313161204377711e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:41:36,463]\u001b[0m Trial 106 finished with value: 0.019930610433220863 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000122504236725259, 'ent_coef': 9.635756005054481e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:44:00,509]\u001b[0m Trial 107 finished with value: 0.604324460029602 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002075692726143082, 'ent_coef': 0.0069741003924048035, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:45:28,972]\u001b[0m Trial 109 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:46:16,801]\u001b[0m Trial 108 finished with value: 0.016669128090143204 and parameters: {'batch_size': 256, 'n_steps': 16, 'gamma': 0.95, 'lr': 0.0001894484733580404, 'ent_coef': 1.372374204582051e-05, 'cliprange': 0.3, 'noptepochs': 50, 'lambda': 0.9}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:46:45,727]\u001b[0m Trial 110 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:50:12,768]\u001b[0m Trial 111 finished with value: 0.011747448705136776 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.9999, 'lr': 6.885063351356414e-05, 'ent_coef': 0.034276301913491874, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:50:46,220]\u001b[0m Trial 112 finished with value: -78.62692260742188 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003985116132021036, 'ent_coef': 3.2992715882681073e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:54:10,417]\u001b[0m Trial 113 finished with value: 0.03707515820860863 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002827915796190683, 'ent_coef': 3.1281315862828304e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:54:40,376]\u001b[0m Trial 114 finished with value: 0.00780072808265686 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003120158881785625, 'ent_coef': 2.2002760593106614e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:55:49,629]\u001b[0m Trial 116 finished with value: -92.72662353515625 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.99, 'lr': 0.001001119463229993, 'ent_coef': 1.8018831826010542e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 68 with value: -98.95207214355469.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:58:06,726]\u001b[0m Trial 115 finished with value: -99.14582824707031 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006213010340576837, 'ent_coef': 0.0004927687872681538, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 17:59:50,528]\u001b[0m Trial 117 finished with value: 0.0030353600159287453 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005292176396237569, 'ent_coef': 0.0008958214593089361, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:02:08,797]\u001b[0m Trial 118 finished with value: 0.011217966675758362 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0016820701650059643, 'ent_coef': 0.00042347006673380317, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:02:49,607]\u001b[0m Trial 119 finished with value: -95.38313293457031 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006906538481612326, 'ent_coef': 0.0005081844772325735, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:06:11,980]\u001b[0m Trial 120 finished with value: 0.4013548493385315 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.00014814799219546848, 'ent_coef': 0.001833963717826568, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:06:54,506]\u001b[0m Trial 121 finished with value: 0.011465411633253098 and parameters: {'batch_size': 128, 'n_steps': 512, 'gamma': 0.9, 'lr': 0.0001571526316764838, 'ent_coef': 0.0038409554569637954, 'cliprange': 0.3, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:09:33,222]\u001b[0m Trial 122 finished with value: -97.9125747680664 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00040362267740253583, 'ent_coef': 1.3528773359994301e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:10:30,264]\u001b[0m Trial 123 finished with value: -98.53755950927734 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0007963618241188463, 'ent_coef': 9.94182835571783e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:11:23,511]\u001b[0m Trial 124 finished with value: -95.31433868408203 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003828225095518836, 'ent_coef': 1.0908666215116572e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:14:36,321]\u001b[0m Trial 125 finished with value: 0.01329109352082014 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.001197013744654082, 'ent_coef': 0.0008672998342964575, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:15:23,928]\u001b[0m Trial 126 finished with value: 0.02515149675309658 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.001202134448905802, 'ent_coef': 3.386110153240021e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:18:30,198]\u001b[0m Trial 127 finished with value: -98.9217529296875 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0008075976537026159, 'ent_coef': 2.989250025661834e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:18:55,614]\u001b[0m Trial 128 finished with value: -96.64897155761719 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0007996223418834765, 'ent_coef': 8.469874668684193e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:19:48,697]\u001b[0m Trial 129 finished with value: -93.34644317626953 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.995, 'lr': 0.003638857587582949, 'ent_coef': 0.0028377620273928477, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:21:10,315]\u001b[0m Trial 130 finished with value: -88.45052337646484 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.98, 'lr': 0.003820058787496653, 'ent_coef': 2.3458613170890472e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:22:32,442]\u001b[0m Trial 131 finished with value: -96.51373291015625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.00022862659328757147, 'ent_coef': 0.01493524381041262, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:24:26,203]\u001b[0m Trial 132 finished with value: -97.82203674316406 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006947475868921188, 'ent_coef': 5.358174017367642e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:25:17,978]\u001b[0m Trial 133 finished with value: -97.3680191040039 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006178437341391515, 'ent_coef': 1.4569245823312265e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:28:28,865]\u001b[0m Trial 134 finished with value: 0.033848777413368225 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006304583019429888, 'ent_coef': 0.00014578773199622853, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:29:19,186]\u001b[0m Trial 135 finished with value: 0.027132604271173477 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0008571252040553644, 'ent_coef': 4.93073710916008e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:32:28,123]\u001b[0m Trial 136 finished with value: 0.010343619622290134 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 9.104223898726212e-05, 'ent_coef': 4.9396842053332606e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:33:17,070]\u001b[0m Trial 137 finished with value: 0.0010548775317147374 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.999, 'lr': 9.263833095669304e-05, 'ent_coef': 2.6946058744460684e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:36:26,666]\u001b[0m Trial 138 finished with value: 0.008498079143464565 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.0003336641913281529, 'ent_coef': 6.67516899140063e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:37:15,738]\u001b[0m Trial 139 finished with value: 0.1534481793642044 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0015927440977004776, 'ent_coef': 7.746051400155031e-05, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:40:22,390]\u001b[0m Trial 140 finished with value: 0.20778241753578186 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0002622844228614189, 'ent_coef': 0.03662797290429042, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:40:57,506]\u001b[0m Trial 141 finished with value: -97.52226257324219 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002954944556834571, 'ent_coef': 0.0002268078388407226, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:44:21,868]\u001b[0m Trial 142 finished with value: 0.6043336987495422 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006083126159168913, 'ent_coef': 0.00027049297860154946, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:44:55,407]\u001b[0m Trial 143 finished with value: 0.11193034797906876 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004174336680023114, 'ent_coef': 0.00031665995267819177, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:47:36,124]\u001b[0m Trial 144 finished with value: -96.7803726196289 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00019372113165969204, 'ent_coef': 0.00041601580839405064, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.99}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:48:54,589]\u001b[0m Trial 145 finished with value: 0.22662410140037537 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00021076744823714553, 'ent_coef': 0.00011395861615088362, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:51:25,179]\u001b[0m Trial 146 finished with value: 0.8570221662521362 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00029579259096354824, 'ent_coef': 0.00011252928622650562, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:52:43,994]\u001b[0m Trial 147 finished with value: 0.586434543132782 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.0003111346227379503, 'ent_coef': 0.0005535157957510076, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:53:53,512]\u001b[0m Trial 148 finished with value: 99.8990478515625 and parameters: {'batch_size': 128, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.001065776941747656, 'ent_coef': 0.07793578052966883, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:55:10,180]\u001b[0m Trial 150 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:56:06,243]\u001b[0m Trial 149 finished with value: -83.8092269897461 and parameters: {'batch_size': 128, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0009864313265425559, 'ent_coef': 0.0002272245587885101, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:56:25,224]\u001b[0m Trial 151 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:59:30,429]\u001b[0m Trial 153 finished with value: -96.99665832519531 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006889647339907287, 'ent_coef': 3.3996836992392113e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 18:59:59,196]\u001b[0m Trial 152 finished with value: 0.0317843034863472 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0007776934046272647, 'ent_coef': 3.096894555112112e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:01:29,509]\u001b[0m Trial 154 finished with value: -96.03812408447266 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004858344056162611, 'ent_coef': 7.957070623525859e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:03:02,617]\u001b[0m Trial 155 finished with value: -98.10968017578125 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005083916094375768, 'ent_coef': 1.299290598777351e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:05:27,940]\u001b[0m Trial 156 finished with value: 0.001196318306028843 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.006604240872834396, 'ent_coef': 1.2146687830371255e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:05:46,528]\u001b[0m Trial 157 finished with value: -96.79540252685547 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00013423427686430462, 'ent_coef': 4.108016326355803e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:09:28,686]\u001b[0m Trial 158 finished with value: 0.019250649958848953 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003641224510731867, 'ent_coef': 1.975922393093177e-06, 'cliprange': 0.2, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:09:46,498]\u001b[0m Trial 159 finished with value: 0.0020615230314433575 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00041290793654571995, 'ent_coef': 2.0492112101256396e-06, 'cliprange': 0.2, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:12:08,245]\u001b[0m Trial 160 finished with value: 9.005111678561661e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.0002600240672812059, 'ent_coef': 2.2480872470760607e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:12:32,978]\u001b[0m Trial 161 finished with value: 8.392371455556713e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.00023922070247691082, 'ent_coef': 0.0010760593971258568, 'cliprange': 0.4, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:15:36,474]\u001b[0m Trial 163 finished with value: -96.5377197265625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000494367274231595, 'ent_coef': 1.4778449783056615e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:15:49,011]\u001b[0m Trial 162 finished with value: -98.4604263305664 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000579857887413539, 'ent_coef': 1.4857667662537572e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:18:48,243]\u001b[0m Trial 164 finished with value: -98.325927734375 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005172863686063753, 'ent_coef': 1.2178511902204818e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:19:38,820]\u001b[0m Trial 165 finished with value: 0.0007719395798631012 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005744926657357153, 'ent_coef': 9.440206948670885e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:22:42,371]\u001b[0m Trial 166 finished with value: 0.0009641318465583026 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005847541640340018, 'ent_coef': 1.1330332681645236e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:23:32,882]\u001b[0m Trial 167 finished with value: 0.007876078598201275 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0007450472806324615, 'ent_coef': 1.1603937885004847e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:26:34,574]\u001b[0m Trial 168 finished with value: 1.320749044418335 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0007153012424935536, 'ent_coef': 7.307605592292245e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:27:11,689]\u001b[0m Trial 169 finished with value: -98.58610534667969 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0014035679650430445, 'ent_coef': 2.891319781005806e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:30:23,328]\u001b[0m Trial 170 finished with value: 1.3421612977981567 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.00036400610964504597, 'ent_coef': 1.421787447185454e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:31:03,127]\u001b[0m Trial 171 finished with value: 0.01871425285935402 and parameters: {'batch_size': 128, 'n_steps': 512, 'gamma': 0.995, 'lr': 0.0017768917278253155, 'ent_coef': 5.2659024657063024e-06, 'cliprange': 0.2, 'noptepochs': 10, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:32:11,960]\u001b[0m Trial 172 finished with value: -95.25566101074219 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00144359928139906, 'ent_coef': 5.511817465428344e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:34:17,477]\u001b[0m Trial 174 finished with value: -95.44471740722656 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00108584024498652, 'ent_coef': 2.7939537179577804e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:34:53,259]\u001b[0m Trial 173 finished with value: 0.008315620943903923 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00130589938914484, 'ent_coef': 2.84966176032843e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:38:09,846]\u001b[0m Trial 175 finished with value: 0.015005646273493767 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0005369625541553908, 'ent_coef': 2.2358436615699695e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:38:47,863]\u001b[0m Trial 176 finished with value: 0.005355847999453545 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004882641961714151, 'ent_coef': 3.944483721339503e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:42:01,355]\u001b[0m Trial 177 finished with value: 0.03212428465485573 and parameters: {'batch_size': 256, 'n_steps': 1024, 'gamma': 0.95, 'lr': 0.0004543091673856789, 'ent_coef': 0.0006993869386026801, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:42:32,810]\u001b[0m Trial 178 finished with value: -98.4843978881836 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0009449605149443152, 'ent_coef': 3.492955724117443e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:44:19,476]\u001b[0m Trial 179 finished with value: -93.8016357421875 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0009028995144007762, 'ent_coef': 4.500455962442913e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:44:55,231]\u001b[0m Trial 180 finished with value: -95.75627136230469 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.002464745553469906, 'ent_coef': 1.3843580739994574e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:46:46,993]\u001b[0m Trial 181 finished with value: -96.01301574707031 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0007656616910553673, 'ent_coef': 7.601034440598044e-06, 'cliprange': 0.3, 'noptepochs': 50, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:48:26,569]\u001b[0m Trial 182 finished with value: -97.6046371459961 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0007566623950043742, 'ent_coef': 1.497471560911045e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:50:37,570]\u001b[0m Trial 183 finished with value: 0.07731831818819046 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0003017562488582733, 'ent_coef': 2.8137549852855313e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:52:12,429]\u001b[0m Trial 184 finished with value: 0.43261557817459106 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0010857013619621388, 'ent_coef': 1.58998439849309e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:54:27,271]\u001b[0m Trial 185 finished with value: -77.44641876220703 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0009517930796304999, 'ent_coef': 9.029950963184635e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:55:26,417]\u001b[0m Trial 186 finished with value: -97.4489517211914 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0006505421151642288, 'ent_coef': 8.148565546151035e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:58:16,297]\u001b[0m Trial 187 finished with value: 0.05917160585522652 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0012950736716433732, 'ent_coef': 6.647567692308778e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:59:13,941]\u001b[0m Trial 188 finished with value: 0.11283795535564423 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0013782154736386098, 'ent_coef': 3.5984017788804954e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 19:59:46,465]\u001b[0m Trial 189 finished with value: -94.7684097290039 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.000823246434872857, 'ent_coef': 2.0897233148451175e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:00:58,137]\u001b[0m Trial 190 finished with value: -94.19020080566406 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0008023444654877947, 'ent_coef': 1.0221409912008767e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:02:51,943]\u001b[0m Trial 191 finished with value: 99.8990478515625 and parameters: {'batch_size': 128, 'n_steps': 16, 'gamma': 0.95, 'lr': 0.0005804180665543353, 'ent_coef': 3.921236664013852e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.9}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:04:43,116]\u001b[0m Trial 192 finished with value: 0.04740958660840988 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000365541189145047, 'ent_coef': 4.1788055094343666e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.9}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:05:43,472]\u001b[0m Trial 193 finished with value: -97.18479919433594 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00041482010401417996, 'ent_coef': 6.296816869355058e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:08:33,816]\u001b[0m Trial 194 finished with value: 0.00025652878684923053 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00040144640303220263, 'ent_coef': 1.5456359047440048e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:08:51,756]\u001b[0m Trial 195 finished with value: -97.3554916381836 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00018486317947139034, 'ent_coef': 1.353097716614248e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:12:22,235]\u001b[0m Trial 196 finished with value: 0.4339753985404968 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0001735876106604233, 'ent_coef': 2.6835761321748723e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:12:40,733]\u001b[0m Trial 197 finished with value: 0.008797399699687958 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005794756205119045, 'ent_coef': 3.0066730279381136e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:14:57,182]\u001b[0m Trial 199 finished with value: -97.02223205566406 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.00026294616852319877, 'ent_coef': 1.838027168561182e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:14:59,997]\u001b[0m Trial 198 finished with value: -97.30162048339844 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0006013774293913152, 'ent_coef': 2.3717955910257166e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:18:43,762]\u001b[0m Trial 200 finished with value: 0.0006996382726356387 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0019459052968261218, 'ent_coef': 1.1107150858402729e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:18:45,883]\u001b[0m Trial 201 finished with value: 0.02606309950351715 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.0003303250299449272, 'ent_coef': 0.00038779938466322795, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:19:56,933]\u001b[0m Trial 202 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:21:11,430]\u001b[0m Trial 204 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:22:34,592]\u001b[0m Trial 203 finished with value: 0.05973546952009201 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00012500205582749351, 'ent_coef': 0.019181387214913567, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:23:19,608]\u001b[0m Trial 205 finished with value: -95.22431945800781 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 0.00047465592759495347, 'ent_coef': 0.0018501056684563206, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:26:21,772]\u001b[0m Trial 206 finished with value: 0.032976340502500534 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004783601540629085, 'ent_coef': 0.05374451913323028, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:26:59,148]\u001b[0m Trial 207 finished with value: 0.08030055463314056 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00030341450653133744, 'ent_coef': 5.56482970525841e-05, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:30:06,595]\u001b[0m Trial 208 finished with value: 0.0707663968205452 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0010032960606463056, 'ent_coef': 1.0707049553000092e-06, 'cliprange': 0.4, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:30:48,547]\u001b[0m Trial 209 finished with value: 2.2674670219421387 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0006960576390085161, 'ent_coef': 1.8154026756785226e-06, 'cliprange': 0.4, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:33:49,119]\u001b[0m Trial 210 finished with value: 0.0172454621642828 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.0007207984924814335, 'ent_coef': 1.9897870426857403e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:34:26,365]\u001b[0m Trial 211 finished with value: 0.7714712619781494 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.00015394036460900627, 'ent_coef': 2.9620897366709865e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:36:50,364]\u001b[0m Trial 212 finished with value: -96.22531127929688 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0006216007260775406, 'ent_coef': 8.909697454240612e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:37:43,229]\u001b[0m Trial 213 finished with value: -98.23383331298828 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0006883502037264459, 'ent_coef': 6.588873123979948e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:40:32,550]\u001b[0m Trial 214 finished with value: 0.12074773013591766 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0009079402325391335, 'ent_coef': 4.80233056815152e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:41:26,818]\u001b[0m Trial 215 finished with value: 0.29172390699386597 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.000977700285114588, 'ent_coef': 6.160732447813877e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:44:01,885]\u001b[0m Trial 216 finished with value: -97.9637451171875 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0002374569708444901, 'ent_coef': 2.377698693959595e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:45:08,330]\u001b[0m Trial 217 finished with value: 0.004431598819792271 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00020806557956491343, 'ent_coef': 2.1146850785065334e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:47:39,383]\u001b[0m Trial 219 finished with value: -95.18641662597656 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00035785603955918794, 'ent_coef': 3.6736071407479425e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "\u001b[32m[I 2021-03-19 20:47:39,396]\u001b[0m Trial 218 finished with value: 0.1266222596168518 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0004028150152014701, 'ent_coef': 2.3102428102817716e-07, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:50:10,362]\u001b[0m Trial 221 finished with value: -97.1302719116211 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0005129492437622455, 'ent_coef': 4.849285398874316e-07, 'cliprange': 0.3, 'noptepochs': 10, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:51:22,753]\u001b[0m Trial 220 finished with value: 0.1500674933195114 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0004989453263231713, 'ent_coef': 1.4406966786532506e-05, 'cliprange': 0.3, 'noptepochs': 1, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:53:53,402]\u001b[0m Trial 222 finished with value: 1.0587091445922852 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0002445087632946309, 'ent_coef': 0.0013852257787733719, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:55:04,123]\u001b[0m Trial 223 finished with value: 0.014873094856739044 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.99, 'lr': 0.00029277289181215815, 'ent_coef': 0.0008666348431086978, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:57:28,885]\u001b[0m Trial 224 finished with value: 0.23293793201446533 and parameters: {'batch_size': 32, 'n_steps': 1024, 'gamma': 0.98, 'lr': 0.00018426716068368592, 'ent_coef': 1.7393323724706901e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 20:58:39,998]\u001b[0m Trial 225 finished with value: -98.83293914794922 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.00018253509461077118, 'ent_coef': 1.314534174053324e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:01:11,057]\u001b[0m Trial 226 finished with value: 0.15421132743358612 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.00010262956310853824, 'ent_coef': 3.2218968454044087e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:02:17,254]\u001b[0m Trial 227 finished with value: 0.009396667592227459 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0011793304774964568, 'ent_coef': 1.2974450159969796e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:03:19,408]\u001b[0m Trial 228 finished with value: -96.47765350341797 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0008126057257023089, 'ent_coef': 1.2458286011749596e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:06:03,443]\u001b[0m Trial 229 finished with value: 0.07231636345386505 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0007555686752279776, 'ent_coef': 1.0271939553038374e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:06:58,602]\u001b[0m Trial 230 finished with value: 0.050098717212677 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0006949125111719768, 'ent_coef': 8.595383591747861e-07, 'cliprange': 0.2, 'noptepochs': 50, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:09:40,516]\u001b[0m Trial 231 finished with value: 1.0154650211334229 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0002501178806064551, 'ent_coef': 6.966846750518051e-07, 'cliprange': 0.2, 'noptepochs': 50, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:10:44,555]\u001b[0m Trial 232 finished with value: 0.03316015377640724 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00014664674084665253, 'ent_coef': 3.6926855847090204e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:13:21,431]\u001b[0m Trial 233 finished with value: 0.038130756467580795 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0001422706217594977, 'ent_coef': 3.637795477200995e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:13:44,329]\u001b[0m Trial 234 finished with value: -98.5754165649414 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0001861987821846229, 'ent_coef': 0.0005690026025407652, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:16:53,857]\u001b[0m Trial 235 finished with value: 0.0022153411991894245 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00020993120807804222, 'ent_coef': 0.0005544254415982179, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:17:30,228]\u001b[0m Trial 236 finished with value: 0.02018238976597786 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003253390268474696, 'ent_coef': 0.0005393139793698305, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:20:31,084]\u001b[0m Trial 237 finished with value: 0.0015482923481613398 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003667268086818086, 'ent_coef': 0.0007680585570436277, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:21:07,445]\u001b[0m Trial 238 finished with value: 0.008122202008962631 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00017261545073048753, 'ent_coef': 0.0006862853578069671, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:24:01,460]\u001b[0m Trial 239 finished with value: 0.0011005664709955454 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00018125823087745139, 'ent_coef': 2.1602414461420257e-06, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:24:37,594]\u001b[0m Trial 240 finished with value: 0.004145464859902859 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005685981570982916, 'ent_coef': 2.358269320655955e-06, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:27:28,182]\u001b[0m Trial 241 finished with value: 0.013390375301241875 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0005909742376072864, 'ent_coef': 6.831377655632671e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:28:13,285]\u001b[0m Trial 242 finished with value: 0.015396269969642162 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00010973852900812692, 'ent_coef': 0.00034618819733067625, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:31:00,725]\u001b[0m Trial 243 finished with value: -98.95348358154297 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00025139424134592306, 'ent_coef': 0.0003572984302688505, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:31:41,908]\u001b[0m Trial 244 finished with value: 0.9295241236686707 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0002608857119352308, 'ent_coef': 1.4211922508154743e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.95}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:32:09,857]\u001b[0m Trial 245 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:34:11,605]\u001b[0m Trial 246 finished with value: -96.5904769897461 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004484880473139683, 'ent_coef': 0.0003849977724530602, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:34:13,829]\u001b[0m Trial 247 finished with value: -96.15493774414062 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0004145520085619157, 'ent_coef': 1.7621909325104038e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:35:18,516]\u001b[0m Trial 248 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:36:43,280]\u001b[0m Trial 249 finished with value: -88.78785705566406 and parameters: {'batch_size': 32, 'n_steps': 16, 'gamma': 0.95, 'lr': 0.00020840386425325456, 'ent_coef': 0.0002814005035370942, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:38:46,513]\u001b[0m Trial 250 finished with value: 0.12094694375991821 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.95, 'lr': 0.0002973872722495401, 'ent_coef': 1.6489482014104733e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:40:12,905]\u001b[0m Trial 251 finished with value: 0.03279619663953781 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00032518837747449585, 'ent_coef': 9.748020351334367e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:42:11,610]\u001b[0m Trial 252 finished with value: 0.1145012229681015 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0010878026349764418, 'ent_coef': 0.00014417579280603607, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:43:52,197]\u001b[0m Trial 253 finished with value: 1.0760496854782104 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.0011139319273250374, 'ent_coef': 0.00017614243527371338, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:45:39,016]\u001b[0m Trial 254 finished with value: 1.4189367294311523 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.9999, 'lr': 0.0008367803858663228, 'ent_coef': 3.040348947168964e-05, 'cliprange': 0.3, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:46:05,426]\u001b[0m Trial 255 finished with value: -96.26423645019531 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0005072384105102286, 'ent_coef': 2.492492522066414e-05, 'cliprange': 0.2, 'noptepochs': 20, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:49:11,430]\u001b[0m Trial 256 finished with value: 0.07558340579271317 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0004897714484281822, 'ent_coef': 4.355711308730333e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:49:39,641]\u001b[0m Trial 257 finished with value: 0.04781468212604523 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.9, 'lr': 0.000633131611410767, 'ent_coef': 4.1765393247444764e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:50:48,217]\u001b[0m Trial 259 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:51:35,917]\u001b[0m Trial 258 finished with value: 9.675153705757111e-06 and parameters: {'batch_size': 64, 'n_steps': 2048, 'gamma': 0.9, 'lr': 0.0006770302939066272, 'ent_coef': 0.0010784868871602617, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:53:30,730]\u001b[0m Trial 260 finished with value: -97.47564697265625 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00015545945168523047, 'ent_coef': 0.00023626523076105674, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:55:03,931]\u001b[0m Trial 261 finished with value: 0.09715677052736282 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 8.550156596862408e-05, 'ent_coef': 0.00025623001721833015, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "\u001b[32m[I 2021-03-19 21:55:04,442]\u001b[0m Trial 262 finished with value: -95.82355499267578 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0003700799169625218, 'ent_coef': 5.418364479286328e-07, 'cliprange': 0.3, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:57:40,487]\u001b[0m Trial 264 finished with value: -97.4984359741211 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.99, 'lr': 0.0002262577533780622, 'ent_coef': 2.741685064511299e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 21:58:40,211]\u001b[0m Trial 263 finished with value: 0.03648483008146286 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0004096490196044685, 'ent_coef': 0.0005714572238751784, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:01:11,604]\u001b[0m Trial 265 finished with value: 0.00859985314309597 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0009034842363742551, 'ent_coef': 1.218051496562463e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:02:10,607]\u001b[0m Trial 266 finished with value: 0.022247646003961563 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.000263456456617833, 'ent_coef': 1.487150332223328e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:04:34,514]\u001b[0m Trial 267 finished with value: -38.395713806152344 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00030616457549940447, 'ent_coef': 9.708674565334444e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:05:45,245]\u001b[0m Trial 268 finished with value: 0.05108203738927841 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00011634288248681735, 'ent_coef': 1.089093565061322e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:07:04,917]\u001b[0m Trial 269 finished with value: -95.65583038330078 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.00012555194569110437, 'ent_coef': 5.11313092794225e-06, 'cliprange': 0.3, 'noptepochs': 10, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:09:23,766]\u001b[0m Trial 270 finished with value: 0.007828998379409313 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0006690758083873863, 'ent_coef': 2.7099272112281e-06, 'cliprange': 0.3, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:10:24,215]\u001b[0m Trial 271 finished with value: 0.005667927674949169 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.0006265895569571004, 'ent_coef': 1.6631288897371523e-06, 'cliprange': 0.2, 'noptepochs': 1, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:12:47,692]\u001b[0m Trial 272 finished with value: 0.08671535551548004 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.999, 'lr': 0.000516335355194013, 'ent_coef': 2.10521800870748e-05, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:13:49,362]\u001b[0m Trial 273 finished with value: 0.006850909441709518 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0008791635725731562, 'ent_coef': 9.788755702407455e-08, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:13:59,296]\u001b[0m Trial 274 pruned. \u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:16:32,408]\u001b[0m Trial 276 finished with value: -97.23234558105469 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00017171201319687103, 'ent_coef': 0.0004061614789638241, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:17:23,876]\u001b[0m Trial 275 finished with value: 0.001351920422166586 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.0014332573745241385, 'ent_coef': 0.006585022172863231, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.92}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:19:56,578]\u001b[0m Trial 277 finished with value: 0.3343733251094818 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00036730715949794695, 'ent_coef': 7.134131790328571e-07, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:20:49,874]\u001b[0m Trial 278 finished with value: 0.233188658952713 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0003908907627432872, 'ent_coef': 6.290800848255761e-05, 'cliprange': 0.1, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:23:24,305]\u001b[0m Trial 279 finished with value: 0.031327854841947556 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.00023356247244136165, 'ent_coef': 3.6534453091974155e-07, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:24:20,774]\u001b[0m Trial 280 finished with value: 0.04041465371847153 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.00022219751817185453, 'ent_coef': 2.5063934722167827e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:26:05,494]\u001b[0m Trial 281 finished with value: -97.45672607421875 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.0008233002965231975, 'ent_coef': 2.495588457016806e-06, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:27:50,711]\u001b[0m Trial 282 finished with value: 4.464471817016602 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.995, 'lr': 0.000773860282454967, 'ent_coef': 0.00010346671065833028, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:28:12,341]\u001b[0m Trial 283 finished with value: -95.00799560546875 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.98, 'lr': 0.0005458244192705071, 'ent_coef': 8.426322722237431e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:29:35,819]\u001b[0m Trial 284 finished with value: -88.20379638671875 and parameters: {'batch_size': 128, 'n_steps': 32, 'gamma': 0.98, 'lr': 0.0005347448894502415, 'ent_coef': 1.6283376913197238e-06, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 0.98}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:31:27,361]\u001b[0m Trial 285 finished with value: -98.54085540771484 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002989132824760789, 'ent_coef': 0.000675357549384381, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:32:56,534]\u001b[0m Trial 286 finished with value: 1.5000797510147095 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0002891401261267621, 'ent_coef': 0.0008006421093886877, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 0.8}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:34:53,508]\u001b[0m Trial 287 finished with value: 0.010536936111748219 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.001080000962813331, 'ent_coef': 0.001354978191712451, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:35:57,695]\u001b[0m Trial 288 finished with value: -97.80298614501953 and parameters: {'batch_size': 32, 'n_steps': 256, 'gamma': 0.99, 'lr': 0.0028788070876784124, 'ent_coef': 0.0027803307415303646, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:37:12,057]\u001b[0m Trial 289 finished with value: 8.332468496519141e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.99, 'lr': 0.00017957864059594172, 'ent_coef': 0.0005399624255387583, 'cliprange': 0.3, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:38:21,883]\u001b[0m Trial 290 finished with value: 9.872881491901353e-06 and parameters: {'batch_size': 32, 'n_steps': 2048, 'gamma': 0.95, 'lr': 0.002903027946669068, 'ent_coef': 0.001036177935761897, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:38:39,056]\u001b[0m Trial 291 finished with value: -96.2354965209961 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0025256144358446154, 'ent_coef': 0.002977923604901399, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:40:16,135]\u001b[0m Trial 292 finished with value: -96.12925720214844 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.95, 'lr': 0.0018343653632945313, 'ent_coef': 0.0021598243002033043, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:41:36,785]\u001b[0m Trial 293 finished with value: -98.22249603271484 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.00014883155135890933, 'ent_coef': 0.009320462557354149, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:43:36,700]\u001b[0m Trial 294 finished with value: 0.010051682591438293 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.0035311166812060134, 'ent_coef': 0.0006651757401705804, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:45:17,201]\u001b[0m Trial 295 finished with value: 0.6132508516311646 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.95, 'lr': 6.330491968910393e-05, 'ent_coef': 2.3980873509110256e-05, 'cliprange': 0.2, 'noptepochs': 30, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:46:56,889]\u001b[0m Trial 296 finished with value: 0.0023177810944616795 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.00014503301675561982, 'ent_coef': 0.009346359540349925, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:48:49,342]\u001b[0m Trial 297 finished with value: 0.04880461096763611 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.00010876088213124106, 'ent_coef': 0.01130449527514483, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Normalizing input and reward\n",
            "Normalization activated: {'norm_reward': False}\n",
            "\u001b[32m[I 2021-03-19 22:50:26,792]\u001b[0m Trial 298 finished with value: 0.24413642287254333 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.95, 'lr': 0.00010226462523519806, 'ent_coef': 0.0015466853761390807, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "\u001b[32m[I 2021-03-19 22:51:13,064]\u001b[0m Trial 299 finished with value: 0.001321527175605297 and parameters: {'batch_size': 128, 'n_steps': 512, 'gamma': 0.95, 'lr': 0.0001654371584857235, 'ent_coef': 0.022620294406348367, 'cliprange': 0.2, 'noptepochs': 5, 'lambda': 1.0}. Best is trial 115 with value: -99.14582824707031.\u001b[0m\n",
            "Number of finished trials:  300\n",
            "Best trial:\n",
            "Value:  -99.14582824707031\n",
            "Params: \n",
            "    batch_size: 32\n",
            "    n_steps: 256\n",
            "    gamma: 0.95\n",
            "    lr: 0.0006213010340576837\n",
            "    ent_coef: 0.0004927687872681538\n",
            "    cliprange: 0.2\n",
            "    noptepochs: 5\n",
            "    lambda: 1.0\n",
            "Writing report to logs/ppo2/report_MountainCarContinuous-v0_300-trials-50000-tpe-median_1616194273.csv\n",
            "[77e9fbce07ac:03110] *** Process received signal ***\n",
            "[77e9fbce07ac:03110] Signal: Segmentation fault (11)\n",
            "[77e9fbce07ac:03110] Signal code: Address not mapped (1)\n",
            "[77e9fbce07ac:03110] Failing at address: 0x7fafcb5b420d\n",
            "[77e9fbce07ac:03110] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fafce25f980]\n",
            "[77e9fbce07ac:03110] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fafcde9e8a5]\n",
            "[77e9fbce07ac:03110] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fafce709e44]\n",
            "[77e9fbce07ac:03110] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fafcde9f735]\n",
            "[77e9fbce07ac:03110] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fafce707cb3]\n",
            "[77e9fbce07ac:03110] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwG4delwEeL7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRfdbb1YEfHt"
      },
      "source": [
        "### Optimal combination of hyperparameters for 300 trials\n",
        "  - We run a search model with 1000 trials, up to the 376 model the algorithm stop due to runtime out. It suggested then that the best combination was given by the trial 115\n",
        "  - To avoid runtime probles we ran the model with 300 trials. The best model chosen was the one of 115 trials (see description below)\n",
        "  - We cannot see the model running, but we asume the code (!python train.py --algo ppo2 --env MountainCarContinuous-v0 -n 50000 -optimize --n-trials 300 --n-jobs 2 --sampler tpe --pruner median) does not only search the optimal combination of hyperparameters, but it also trains the adapted model (as the !python suggests)\n",
        "  - The only addition to the !python... order that worked, including the optimised set of hyperparameters) was: !python train.py --algo ppo2 --env MountainCarContinuous-v0 --n-timesteps 256 #--n_envs 16 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uehzE7ds92ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c7c35c-059a-4e97-ccad-4ecf6f846532"
      },
      "source": [
        "\n",
        "'''\n",
        "Optuna search results, 300 trials\n",
        "Params: \n",
        "    batch_size: 32\n",
        "    n_steps: 256\n",
        "    gamma: 0.95\n",
        "    lr: 0.0006213010340576837\n",
        "    ent_coef: 0.0004927687872681538\n",
        "    cliprange: 0.2\n",
        "    noptepochs: 5\n",
        "    lambda: 1.0\n",
        "'''\n",
        "!python train.py --algo ppo2 --env MountainCarContinuous-v0 --n-timesteps 256 #--n_envs 16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "========== MountainCarContinuous-v0 ==========\n",
            "Seed: 0\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gamma', 0.99),\n",
            "             ('lam', 0.94),\n",
            "             ('n_envs', 16),\n",
            "             ('n_steps', 256),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('nminibatches', 8),\n",
            "             ('noptepochs', 4),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=256\n",
            "Normalizing input and reward\n",
            "Creating test environment\n",
            "Normalization activated: {'norm_reward': False}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "Log path: logs/ppo2/MountainCarContinuous-v0_8\n",
            "Saving to logs/ppo2/MountainCarContinuous-v0_8\n",
            "[77e9fbce07ac:08720] *** Process received signal ***\n",
            "[77e9fbce07ac:08720] Signal: Segmentation fault (11)\n",
            "[77e9fbce07ac:08720] Signal code: Address not mapped (1)\n",
            "[77e9fbce07ac:08720] Failing at address: 0x7f08edadf20d\n",
            "[77e9fbce07ac:08720] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7f08f078a980]\n",
            "[77e9fbce07ac:08720] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7f08f03c98a5]\n",
            "[77e9fbce07ac:08720] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7f08f0c34e44]\n",
            "[77e9fbce07ac:08720] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7f08f03ca735]\n",
            "[77e9fbce07ac:08720] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7f08f0c32cb3]\n",
            "[77e9fbce07ac:08720] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9XuzTLdRgL8"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2zxBgNNqU2o"
      },
      "source": [
        "'''\n",
        "# Tuned\n",
        "MountainCarContinuous-v0:\n",
        "  normalize: true\n",
        "  n_envs: 1\n",
        "  n_timesteps: !!float 20000\n",
        "  policy: 'MlpPolicy'\n",
        "  batch_size: 256\n",
        "  n_steps: 8\n",
        "  gamma: 0.9999\n",
        "  learning_rate: !!float 7.77e-05\n",
        "  ent_coef: 0.00429\n",
        "  clip_range: 0.1\n",
        "  n_epochs: 10\n",
        "  gae_lambda: 0.9\n",
        "  max_grad_norm: 5\n",
        "  vf_coef: 0.19\n",
        "  use_sde: True\n",
        "  policy_kwargs: \"dict(log_std_init=-3.29, ortho_init=False)\"\n",
        "'''\n",
        "#!python train.py --algo trpo --env HotterColder-v0 # it is not in training format yet\n",
        "# algorithms a2c,acer,acktr,dqn,ddpg,her,sac,ppo2,trpo,td3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}